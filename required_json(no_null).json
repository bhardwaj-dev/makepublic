[{"document": "                                   Akash Raj Data Scientist  2 years experienced in Data Science. I have done 3 Industrial IoT Machine Learning projects where I was able to Optimize the Powerplant by 7.1%. Looking forward to work in challenging and impactful AI/ML and Cloud projects.  akashraj120@gmail.com  7003482660/7585094425  Chennai, India  linkedin.com/in/akashraj001  github.com/akashraj001  SKILLS  C DBMS Python  SageMaker OOPs  Hypothesis Testing  Machine Learning  Excel Macro NLP  SQL Deep Learning  Statistics Azure  Data Visualization  AWS Docker  LANGUAGES English Full Professional Proficiency  Hindi Native or Bilingual Proficiency  Bengali Full Professional Proficiency  INTERESTS  Music Badminton  Volunteering  Travelling YouTuber  EDUCATION  Graduation Institute of Engineering and Management, Kolkata 04/2014 - 04/2018, 7.93 CGPA  B.Tech in Computer Science Engineering  DATA SCIENCE EXPERIENCE                                                            (TATA CONSULTANCY SERVICES - 11/2018  - PRESENT) Optimizing the NOx emission & Cost of Japanese Thermal Power Plant(4 team members and 10 months) (01/2019 - 10/2019)  Cleaned and merged the IoT sensors(3000) data of a Japanese Thermal Power Plant.  Proposed and Developed Framework for extracting Steady State data from raw(noisy) Sensor Data.  Performed EDA and Descriptive Statistics on the raw data as well as steady data.  Performed Feature Engineering to generate many useful features.  Developed Feature Selection framework.  Developed Model building framework for trying various Models like Linear Regression, Rigde, Lasso, KNNRegressor, RandomForestRegressor, GradientBoostingRegressor, XgboostRegressor.  Built 80 predictive models for predicting various parameters of using IoT sensor data of the Power Plant. Best model was selected based on Adjusted R-squared and RMSE.  Built Optimization framework using PSO(Particle Swarm Optimization).  Finally was able to minimize the NOx emission(7%) and Cost(0.1%) suggesting changes in powerplant settings.  Coal Properties Prediction from IOT Sensors of a Japanese Thermal Power Plant(3 team members and 5 months) (11/2019 - 03/2020)  Predicting Coal properties based on Data from IOT Sensors of a Japanese Thermal Power Plant  Optimized the Steady State Extraction, Feature Selection frameworks  Built 6 predictive models with Sensor data as Input Features and Lab Data of Coal properties as output  Coal Analysis- Developed own Algorithm for Coal clustering(as KMeans didn't give desired results)  Coal Properties includes: Ash Softening Point, Volatile Matter, Moisture, Ash Content, Calorific Value and Fixed Carbon  Deploy Machine Learning Architecture on AWS SageMaker(1 team member and 7 months) (04/2020 - 10/2020)  Deploy the full machine learning architecture on AWS Sagemaker for model building, retraining, optimization and deployment  Dockerized the Steady State Extraction Framework, Modelling building Framework, Feature Selection Framework, Optimization Framework and Deploy in ECR(Elastic Container Registry)  Create Lambda(serverless) functions to invoke these algorithms from SageMaker  Create Step Functions to different steps to create a complete Workflow  Deploy all the frameworks as REST API  CERTIFICATIONS & HACKATHONS Kaggle- ASHRAE - Great Energy Predictor III (ranked 554 among 3614 teams worlwide)  Hacker Earth- Identify Dance forms using Deep Learning( I scored 70% f1_score)  Cousera:- Convolutional Neural Networks in TensorFlow  Udemy: AWS Certified Cloud Practitioner Ultimate Exam Training 2020  Udemy:- Data Science: Natural Language Processing (NLP) in Python  Cousera:- Machine Learning by Andrew Ng  KhanAcademy:- Statistics & Probability  Udemy:- SQL for Data Science With Google Big Query  Courses  mailto:akashraj120@gmail.com https://www.linkedin.com/in/akashraj001/ https://github.com/akashraj001  ", "tokens": [{"text": "TATA CONSULTANCY SERVICES", "start": 979, "end": 1004, "token_start": 149, "token_end": 151, "entityLabel": "company"}, {"text": "11/2018  - PRESENT", "start": 1007, "end": 1025, "token_start": 153, "token_end": 156, "entityLabel": "period"}], "relations": [{"child": 153, "head": 149, "relationLabel": "duration"}]}, {"document": "                                             Elegant Resume   Abhishek Dokania  480.599.9475  abhidokania@outlook.com      HIGHLIGHTS   Hands on technical leader with proven expertise in conceptualizing, designing, prototyping and leading end to end delivery of On-  Prem and Hybrid Cloud hosted Machine Learning solutions across services and product development teams.     Inquisitive Innovator - co-filed two high value patents around use of Machine Learning in AIOps systems    Currently working as technical product manager for IBMs leading Conversational(chatbot based) AIOps solution - responsible for  embedding machine learning and conversational features into operational systems and business processes.     As key member People Analytics CoE at IBM Corporate Headquarters, I interact with IBMs Talent, Compensation and Business  Unit leaders to evangelize and co-create cutting edge, advanced analytics, solutions for hiring, developing and retaining IBM s top   Talent. Co-led development of Next-Gen talent platform by leveraging statistical and machine learning models to drive business   value while ensuring measurable return on investment.    Hands on experience with designing and developing prototypes demonstrating benefits from proposed changes to decision systems  Responsible for iteratively scaling and operationalizing high value prototypes through from MVP to a fully deployable production   system.    Bring cross industry cross domain perspective to drive client pressing business problems in the areas of Customer Experience  measurement and enhancement, operations optimization and research initiatives. Advised clients from Health   Care/Pharmaceutical, Telecommunication & Entertainment and Financial     Developed innovative advanced analytics-based solutions for Health Care/Pharmaceutical industry using Real World Evidence  datasets such as APLD, EMR, Claims and others    Leverage multiple NLP, Machine Learning, Data Analyses, Modeling, and Visualization tools such as SPSS, R and/or Python,  Netezza, Oracle SQL, MySQL, Hadoop based components, Watson suite of tools, Spark and others.      EDUCATIONAL BACKGROUND       MBA    Indian Institute of Technology, Madras    Major -  Operations Research        B.E   I.E.T, DAVV, Indore         Major -  Computer Engineering      WORK EXPERIENCE  IBM                6/15  Present   Senior Data Scientist   Identify opportunities to inject value enhancing advanced analytics-based solutions for multiple Fortune 500   companies    Define MVP/PoC roadmap with vision to scale the minimum viable products/solutions to fully functional  production deployments.    Lead development, production deployment and maintenance of analytical models/systems providing actionable  insights and recognizable RoI    Help generalize/standardize products/offerings to be taken to market. Responsible for analyzing product usage  data to ensure high levels of user engagement.    Build and lead data science teams by hiring and developing top talent.     Sample Client Engagement #1:   Project Description: Design and Deliver Data Platform and operationalize insights pertaining to operational  effectiveness   Client: Leading Pharmaceuticals client   Roles and Responsibilities:    Led multiple data science work streams engaged in delivering actionable insights clients commercial and  research organizations and presented outcomes to client leadership teams    Designed and implemented data platform for collecting and combining data from multiple type of sources -  APLD data, EMR, EHR, Claims data, Drug Sales, Prescriptions data, Real World Evidence data such as   Symphony, IMS, Explorys, Truven etc.    Led insight generation pertaining to Patient and HCP behavior along Procedures, Diagnoses, Prescriptions,  Tests/Observations and Demographic dimensions   Sample Client Engagement #2:     Project Description: Design and Deliver Data Platform and operationalize insights pertaining to operational  effectiveness   Client: Leading Energy and Utilities client   Roles and Responsibilities:    Led team engaged in developing scalable platform for sourcing, aggregating, cleaning and analyzing data  from enterprise-wide CRM, Utilization and Call Center databases.    Collaborated closely with client executive sponsors and other stakeholders in defining project scope,  objectives, tasks, milestones, budgets and success criterion       JOHNSON CONTROLS, INC.                03/15- 06/15   Enterprise Data-lake Consultant   Roles and Responsibilities:   Helped implement Hadoop 2.0(Hortonworks Distribution) based Enterprise Data Lake with the goal to develop   market leading technical differentiator for JCI.      LATENTVIEW ANALYTICS                05/13- 03/15   Analytics Delivery Manager/Senior Data Scientist   Built, developed and lead data science delivery teams.     Advised clients on use cases leveraging internal and external data assets for value generations in the areas of  sales forecasting, social media analytics and customer & marketing analytics.    Generated and presented actionable insights to client teams      Sample Engagement #1  Project Description: Data Analysis and Predictive modeling   Client: Leading Music and Entertainment Companies in US   Roles and Responsibilities:    Steered team of analysts and senior analyst to ensure the quality of delivery and customer satisfaction.    Built a scalable platform for sourcing, aggregating, cleaning and analyzing large volumes of social  media data (structured and semi-structured), sales data and customer data from diverse set of in-house and   external sources.    Perform analysis in R using the techniques such as Classification and Regression Trees (CART), k-means  clustering and decision trees to appropriately classify/cluster artists/bands.   Sample Engagement #2   Project Description: To build scalable platform for sourcing, aggregating, cleaning and analyzing large volumes of  social media data, sales data and customer data. Analyze the ingested data for insight generation - pertaining to Brand   Perception and Product Perception.   Client: One of the leading fast-food chains in US   Roles and Responsibilities:   Sourced, explored, crunched data sets from Social Media (Facebook, Twitter, Blogs, Forums etc.) using tools   such as Radian6, Scikit Learn and NLTK -Python, R and MS Excel    Delivered actionable insights pertaining to Brand Perception and Product Perception to business users    Led a six-member team of analysts and senior analyst and ensured quality of deliverables to be aligned with  business needs and provided actionable insights.   IBM (INDIA-US)                    11/06  08/11    Data Specialist    Worked on multiple data analysis and DW/BI Projects in various capacities     Syntel (INDIA)                    10/05  11/06    Programmer Analyst   Client: Major US based Payer   Roles and Responsibilities:    Development, testing and maintenance of software components for one of Fortune 100 Healthcare clients  billing and open enrollment systems.       ", "tokens": [{"text": "IBM", "start": 2329, "end": 2332, "token_start": 371, "token_end": 371, "entityLabel": "company"}, {"text": "6/15  Present", "start": 2348, "end": 2361, "token_start": 373, "token_end": 375, "entityLabel": "period"}, {"text": "JOHNSON CONTROLS, INC.", "start": 4407, "end": 4429, "token_start": 712, "token_end": 716, "entityLabel": "company"}, {"text": "03/15- 06/15", "start": 4445, "end": 4457, "token_start": 718, "token_end": 719, "entityLabel": "period"}, {"text": "JCI.      LATENTVIEW ANALYTICS", "start": 4676, "end": 4706, "token_start": 753, "token_end": 757, "entityLabel": "company"}, {"text": "05/13- 03/15", "start": 4722, "end": 4734, "token_start": 759, "token_end": 760, "entityLabel": "period"}, {"text": "IBM (INDIA-US)", "start": 6623, "end": 6637, "token_start": 1101, "token_end": 1106, "entityLabel": "company"}, {"text": "11/06  08/11", "start": 6657, "end": 6669, "token_start": 1108, "token_end": 1110, "entityLabel": "period"}, {"text": "Syntel (INDIA)", "start": 6770, "end": 6784, "token_start": 1129, "token_end": 1132, "entityLabel": "company"}, {"text": "10/05  11/06", "start": 6804, "end": 6816, "token_start": 1134, "token_end": 1136, "entityLabel": "period"}], "relations": [{"child": 373, "head": 371, "relationLabel": "duration"}, {"child": 718, "head": 712, "relationLabel": "duration"}, {"child": 759, "head": 753, "relationLabel": "duration"}, {"child": 1108, "head": 1101, "relationLabel": "duration"}, {"child": 1134, "head": 1129, "relationLabel": "duration"}]}, {"document": "                                                Abhay Bhargava  Email: abbharga1@gmail.com  Mobile: +91-9818124582    Diligent Delivery Project Executive having more than 18 years of experience offering a proven record of successfully leading all  phases of projects from design to delivery, handling large teams of up to 80 people. Managed large scale business transformation  projects focused on SAP package enablement. Work with prospective clients in sales activities responding to RFPs, conducting client  workshops and building demonstrations to highlight the firms skills and capabilities.      Areas of Expertise                                                                                                                                                                                    1    SAP R/3   Account Management   Business Process Management   System Migration/Integration   Budgeting and Costing     Stakeholder Management   Process Improvements   Agile/Scrum   Team Building & Mentoring   Client Relations &Presentations    Operations Management   Governance   Communication and Negotiations   Strategic Planning   Resource Management     Professional Experience and Accomplishments                                                                                                                                 1    IBM India  Account Manager                                                                     Mar 16  Until Date    Client: Ineos    The project is using AGILE methodology.    As a SAFe methodology Manager, I am a member of the Program review Team that is responsible for prioritizing the epics   and managing the backlog for teams. Working closely with Uber Scrum Master.   Responsible for teams effective burn down charts and velocity for each sprint. Ensuring that grooming of epics, user stories   is taken up by the teams as per cadence and retrospective happens for continuous improvement.    The sprint duration is of 1 week, a program comprises of 3 sprints. The last one being the HIP   (Hardening/Innovation/Planning) sprint to ensure robust iterative delivery of enhancements.    All project tracking is done using JIRA. Epics, Stories, sub-tasks are tracked here. Kanban board is used for tracking the   program progress.    Handling Offshore Responsibility of Managing globally distributed team ( Technical as well as Functional teams comprising  of modules like SD , MM , PM/QM etc .) for customizing the software.    Managing the Team involved in Designing, Building and Configuring the software from End to End.    Creating solutions for identified process improvement areas and work with management team to implement them.    Identifying continuous improvement processes that will improve unit costs and service provided to the client    Successfully migrated the project to cloud in May 2018    Done detailed HIA(HANA Impact assessment) and submitted the RFP response from IBM side.     Single Point of Contact for Multi-Center coordination, Status reporting, tracking, staffing, forecasting and managing project  and expense management.       IBM India  Account Manager /Senior Project Manager                                              May 13 -Feb 16      Clients: HESS/ Nokia    As a senior Project Manager for delivery and implementation led teams across broad technical, financial and business  disciplines.     Anticipated and managed change effectively in rapidly evolving global business environments.    Handling Technical as well as Functional comprising of modules like SD , MM , PM/QM etc    Defined processes and tools best suited to each project. Creating detailed project road maps, plans, schedules and work  breakdown structures.     Leveraged technical, business and financial acumen to communicate effectively with client executives and their respective  teams    mailto:abbharga1@gmail.com mailto:abbharga1@gmail.com    Provided direction and support to the formulation, evaluation, strategy setting and presentation of product and process  solution improvements.         IBM India  Senior Project Manager / Project Manager                   Feb 10- Apr 13   Clients: Medtronic / NSN    Promoted to lead role to manage a team of project analysts and support portfolio level governance.    Project Management/ People management for the above projects at offshore.     End to end responsibility for Design and development teams.     Ensured adherence to software quality processes and responsible for planning and implementation at all stages and sites    Ownership of delivery in terms of timeline and estimate, compliance with project quality goals and organization goals for  the entire project, contributing towards generation, harvesting and implementation of assets in project.     Build out an operational quality control function and support the department in responding to compliance and audit    During the tenure of the Project, received huge Appreciation from the Customer and the Geo DPE for finishing each  planned activity before time and with utmost quality. This was a Parallel assignment for me along with the NSN TM project     IBM India  Project Manager / Team Lead                     Mar 06-Jan 10      Clients: Medtronic / Campbell    Promoted to Team Lead with exceptional performance shown in previous projects    Managed small to medium teams of 10-15 people    Played the part of Technical Lead for the project and guided the team for implementation    Prepared project plans and drove client workshops to work on detailed requirements    Responsible for compliance audits and software processes implementation throughout the lifecycle of the project      IBM India Team Lead /  Senior Developer                  Jan 05  Feb 06      Clients: Heinz /Wolters Kluwer    Worked as a Senior Developer and took the role of Technical Team Lead helping the team to deliver quality objects on time.    In the capacity of Senior Developer delivered all high quality Objects that earned client praise and IBM recognition awards     Worked on complex CRM Solutions and was awarded the BRAVO award back to back for both the projects     HCL Technologies  Software Engineer                     Jan 02  Dec 04      Clients: Conexant /Mindspeed    Worked on two projects when in HCL    As a software Engineer was involved in developing ABAP/4 programs for classical and interactive reports using ALV, BDCs,  etc     Worked on LSMW for data migration, applied OSS notes, modified SAP Scripts, worked on user-exits    Extended IDOC by adding a custom segment and all other related assignments for various SAP modules.      Academic & other details                                                                                                                                                                                11        Bachelor of Technology  Electrical  (I.E.T. Lucknow- Year 2001)    ", "tokens": [{"text": "IBM", "start": 1340, "end": 1343, "token_start": 158, "token_end": 158, "entityLabel": "company"}, {"text": "Mar 16  Until Date", "start": 1435, "end": 1453, "token_start": 164, "token_end": 168, "entityLabel": "period"}, {"text": "IBM", "start": 3103, "end": 3106, "token_start": 469, "token_end": 469, "entityLabel": "company"}, {"text": "May 13 -Feb 16", "start": 3199, "end": 3213, "token_start": 478, "token_end": 481, "entityLabel": "period"}, {"text": "IBM", "start": 4053, "end": 4056, "token_start": 613, "token_end": 613, "entityLabel": "company"}, {"text": "Feb 10- Apr 13", "start": 4123, "end": 4137, "token_start": 623, "token_end": 626, "entityLabel": "period"}, {"text": "IBM", "start": 5126, "end": 5129, "token_start": 792, "token_end": 792, "entityLabel": "company"}, {"text": "Mar 06-Jan 10", "start": 5185, "end": 5198, "token_start": 801, "token_end": 803, "entityLabel": "period"}, {"text": "IBM", "start": 5662, "end": 5665, "token_start": 878, "token_end": 878, "entityLabel": "company"}, {"text": "Jan 05  Feb 06", "start": 5719, "end": 5733, "token_start": 887, "token_end": 891, "entityLabel": "period"}, {"text": "HCL Technologies", "start": 6130, "end": 6146, "token_start": 962, "token_end": 963, "entityLabel": "company"}, {"text": "Jan 02  Dec 04", "start": 6186, "end": 6200, "token_start": 968, "token_end": 972, "entityLabel": "period"}], "relations": [{"child": 164, "head": 158, "relationLabel": "duration"}, {"child": 478, "head": 469, "relationLabel": "duration"}, {"child": 623, "head": 613, "relationLabel": "duration"}, {"child": 801, "head": 792, "relationLabel": "duration"}, {"child": 887, "head": 878, "relationLabel": "duration"}, {"child": 968, "head": 962, "relationLabel": "duration"}]}, {"document": "                                                   Aditya  Trivedi   Bengaluru, India    +91 8840999318    iadi7ya@gmail.com    Linkedin.com/in/iadi7ya    Github.com/iadi7ya      Result oriented Data Scientist with 5+ years of experience and a successful track record of meeting  Organizations goals in conjunction with the increase in my learning curve. Areas of expertise include  Artificial Intelligence, Data Science, Big Data, Python, Linux and Internet of Things. Prior to this I am a  quick learner, committed towards work and my problem-solving technique helps me execute tasks  based upon my proficiency with minimal errors.   Skills    Machine Learning    Deep Learning    Big Data Analytics    Natural Language Processing    Data Visualization    Statistics and Probability    Digital Signal Processing    Internet of Things    Cyber Security    Python, TensorFlow, Scikit-learn    Apache Spark, Hive, Impala, Kafka    NumPy, Pandas, SciPy, NLTK    Tableau, Matplotlib, Plotly, Seaborn    Linux, SQL, NoSQL, Neo4J, Matlab    Git, JIRA, Confluence, Bitbucket    GCP, Azure, Java and Scala    ROS, MQTT, BLE, Sensors, Raspberry Pi    Wireshark, Aircrack-ng, Metasploit   Experience  DEC, 2019  PRESENT   Data Scientist, Consultant / Equifax Inc (Equifax Analytics)  Working in Satellite Labs of Data Science Team at Equifax Australia, I engineered solutions for the  following:    Developed ML models and end-to-end data pipelines using Spark and Hive for Predictive Modelling,  Risk Analytics, Customer Behavior Analytics, Retrospective Analysis, Credit Worthiness Assessment,  Customer Segmentation, Pattern Recognition.    Designed and developed Credit Scoring Product, Bureau Evaluation Tool using comprehensive data  of the organization. Handled adhoc requests for Analytics Business as Usual (BAUs).    Created Visualization Dashboards for the monitoring of the Machine Learning models and Attributes  for the assessment and improvement. Developed Insights Dashboards and Reports.    Worked on algorithms like Gradient Boosting Machines (GBM), XGBoost, K-means clustering,  Logistic Regression, Recurrent Neural Networks and Convolution Neural Networks. Preparation &  preprocessing of massive datasets for both the Consumer and Commercial products.    Collaborating with globally distributed different teams, requirement gathering, creating tasks, QC,  execution of the projects till the final delivery.        SEP, 2015  DEC, 2019   Data Scientist / Tata Consultancy Services  Being an active member of Innovation & Transformation Group in TCS which works upon Artificial  Intelligence and Internet of Things (IoT), I engineered solutions for the following:    Performed Predictive Analytics, Predictive Maintenance, Anomaly Detection, Edge Analytics, Natural  Language Processing, Topic Modelling. Worked on algorithms like Auto-encoders, RNNs, Support  Vector Machine (SVM), Principal Component Analysis, Logistic Regression.    Smart Sensor: Machine parts failure prediction using Autoencoders and Supervised Machine  Learning. Designed the Data pipeline using Apache Spark to analyze and process the data.    Predict and Self Heal: Developed ML model which analyzes ServiceNow Ticket data and successfully  predict the Incidents that are going to occur in future and resolve them before time by applying NLP.    Virtual Assistant: Designed and developed Conversational Chatbot using Deep Learning and NLP to  solve L3 technical queries automatically for an American multinational conglomerate.   Volunteer Experience  FEB, 2017  PRESENT   Data Science and IoT Coach / Internity Foundation  As a stream head the key programs I executed, conducting training sessions on Data Science and IoT.  Taking interviews, impart training and project development. Also, Conducted workshop at IIT-KGP.   JULY, 2018  JAN, 2019   Machine Learning Instructor / Google Developers  Held a Pro bono position as a MLCC Study Jams Facilitator. I have conducted in-person hands-on session  about Machine Learning using TensorFlow and Scikit-learn and weekly meetups for problem solving.   Education  JUNE 2015   Bachelor of Technology / Uttar Pradesh Technical University  Data Structures, Algorithms, Cyber Security, Operating Systems, Computer Networks   Certifications   Advanced Data Science with IBM Specialization  Coursera  License: BVJBP7TX7RFK   IBM Data Science Professional Certificate  Coursera  License: T3AEKWFQJUL   Data Visualization with Tableau Specialization  Coursera  License: NKH4HPJVFAEY   Neo4j Certified Professional  Neo4j  License: 16717925   Machine Learning by Stanford University  Coursera  License: KB2YG3EE7ZQU     Skills  Experience  DEC, 2019  Present  Data Scientist, Consultant / Equifax Inc (Equifax Analytics)  Sep, 2015  DEC, 2019   Data Scientist / Tata Consultancy Services   Volunteer Experience  Feb, 2017  Present  Data Science and IoT Coach / Internity Foundation  July, 2018  JAN, 2019   Machine Learning Instructor / Google Developers   Education  june 2015  Bachelor of Technology / Uttar Pradesh Technical University   Certifications  ", "tokens": [{"text": "Equifax Inc", "start": 1242, "end": 1253, "token_start": 228, "token_end": 229, "entityLabel": "company"}, {"text": "DEC, 2019  PRESENT", "start": 1192, "end": 1210, "token_start": 217, "token_end": 221, "entityLabel": "period"}, {"text": "TCS", "start": 2557, "end": 2560, "token_start": 453, "token_end": 453, "entityLabel": "company"}, {"text": "SEP, 2015  DEC, 2019", "start": 2427, "end": 2447, "token_start": 428, "token_end": 434, "entityLabel": "period"}], "relations": [{"child": 217, "head": 228, "relationLabel": "duration"}, {"child": 428, "head": 453, "relationLabel": "duration"}]}, {"document": "                                                        Akshay_Tyagi_CSMe                 AKSHAY TYAGI   OBJECTIVE      To apply 14 years of acquired knowledge and skills of software programming and  development to add value to the organization. It includes 6 years of leading Design and  Development work on SAS / Hadoop / Big Data projects, Project Management, Managing  Team, Talent Development, Technical Training and Performance Appraisals.     EXPERIENCE    Currently working with Wipro  Alight as a Sr. Practice Manager (SAS).    Promoted as: Sr. Benefits Manager on Nov 2017  till Sep 2018 with Alight Solutions.  Promoted as: Sr. Client Service Manager on Nov 2012  till Nov 2017 with AON Hewitt.  Promoted as: Sr. Developer Analyst on Dec 2010  Oct 2012 with AON Hewitt.  Promoted as: Developer Analyst on June 2009  Dec 2010 with Hewitt Associates.  Joined Hewitt Associates as Sr. Software Developer on: 12th March 2008    Worked with Infosys Technologies Limited. as Software Engineer   From: 13th June 2005  Till: 07th March 2008     Worked with Pawan Hans Helicopters Ltd.   From: 27th Jan 2005  Till: 13th June 2005      ENVIRONMENT    Windows, Linux, Unix, MVS (Mainframes).   Hadoop EcoSystem: HDFS, MapReduce, Sqoop, Hive, Flume, Pig, Spark Scala.     TECHNICAL SKILLS                        Methodology: Agile    Key Skills: Business Analytics, Business Data Mining, Spreadsheet Modeling, Time-Series  Modeling, Economics and Mathematics for Business Analytics.    Programming Languages:  R, BASE SAS, SAS Macros, DB2  SQL, PL/SQL  Programming, JCL, Control-M.    Data Base:  Teradata, DB2, HBase.   5 4 - S A V I T A  V I H A R ,   D E L H I - 1 1 0 0 9 2   M O B I L E :  0 9 6 5 0 9 3 6 9 7 2   E-mail: akshay.tyagi.infosys@gmail.com            Scripting Languages: Unix Shell Script, Qlikview scripting.    Tools: Qlikview, Advance Query Tool(AQT), Cygwin, Toad,  MS Office, Teradata SQL  Assistant .     CERTIFICATION DETAILS    Certified Scrum Master    Successfully completed Executive Program in Business Analytics from IIM-Calcutta   (1 year)    SAS ETL Studio: SAS ETL Studio training completed in Infosys.     PROJECTS UNDERTAKEN      Role: Senior Benefits Manager / CSM-Exp (Certified Scrum Master)    Responsibility:    Lead Robotics Process Automation (RPA) for Production Support and developing  ADX (Advance Data Exchange) tool, resulting a total cost saving for $500 million to  the organization for FY 2017.     Responsible for end-to-end delivery of Batch projects for 40+ Define Benefits clients  thereby enabling Alight to be a continued success in the field of HWS solutions and  consultancy.     The role demanded in-depth involvement in the Software Development Life Cycle  from discussing the customers requirements, chalking out the project plan.     Involved in Requirement gathering, Project Planning and Code reviews and  discussing strategies to solve various technical problems that arose during the  development cycle.    Initiate ideas and lead discussions for client improvements with leadership team to  impact higher quality or more efficient delivery within technical domain (SAS-ETL,  Mainframes, DB2).    Performs Defect Management for the aligned client change orders.     Posting and reviewing defect reports.    Following up to understand root cause and status of individual defects with assignee  as needed.    Multiple projects completed on SAS-ETL.     People Management    Lead the performance management activities including performance and  development planning, performance review and appraisals, recognition, and coaching.    Leverage the talent/resources on the team to generate results.    Succession planning within the team.    Client / Stakeholder Management    Act as a SPOC between onsite and offshore team for all given projects.    Identifying areas of process improvement and escalating them to the Directors and  other key service stakeholders (BDMs, BSMs).    Communicate status of various projects to onsite team and to the reporting head.      Guide offshore team about right contacts for the given subject.    Act as a point of escalation for both onsite and offshore team.    Develop and maintain relationship with clients and resolve issues, if any.    Requirements Management    Perform high-level assessment of the business, functional, technical, and non- technical requirements of the project.    Perform an Initial Project Feasibility (ROI, Business Case) analysis.     Coaching    Identify cross training strategy within units.    Act as an expert resource on project management tools, processes, and techniques.    Coach team members on how to manage projects and understand the technical  aspects of the system.    Build team members skills on more advanced skills using formal and informal training  methods.     Post Project Analysis/ Review    Collate post project comments documented by onsite and offshore teams.    Review and analyze post project comments.    Lead the conversation and present the findings, project success, and failure factors to  both the onsite and offshore team.    Identify the process improvement areas and coordinate with Project Leads in  introducing new processes/standards or improving the existing ones     Awards/Achievements:    Unmatched Team Award (Q4 FY13):  Annual Enrollment is each years critical and biggest project which requires both  reactive and proactive approach and team work around the clock and extended hours  with dedication. AE work has zero defect tolerance and strict due dates and timelines.    Platinum Team Award (Year 2014):  Lead EB-HWe team for major Annual Enrollment project. Generated capacity  through Abend Reduction project.       Role:  Sr. Developer Analyst / Developer Analyst  Responsibility:    The Senior Developer Analyst role is responsible for leading the development of the  technical detailed design (Low-Level Design / High-Level Design) and is responsible  for overall quality and timeliness of the deliverables related to the project.       Requirements Elicitation, Understanding, Analysis, & Management  - Understand the project's Vision and requirements, and contribute to the creation of the  supplemental requirements, building the low-level technical specifications for a particular  platform and/or service solution.  - Review client requirements and perform gap and/or impact analysis.       Project Planning, Tracking, & Reporting  - Estimate the tasks and resources required to design, build, and test the code for smaller  projects.  - Provide inputs in creating the detailed schedule for the project.  - Support the team in project planning activities, in evaluating risks, and shuffle priorities     based on unresolved issues.  - During development and testing, ensure that assigned parts of the project/modules are  on track with respect to schedules and quality.  - Note scope changes and work with team to shuffle priorities accordingly and/or make  appropriate business decisions.  - Communicate team schedule and status information to the team.  Participate in project review meetings.      Design  - Provide inputs to the creation of High-Level Design (HLD).  - Create the Low-Level Design (LLD)/detailed design for the project with possible  alternate solutions. Ensure that LLD design meets business requirements.  - Review and validate the LLD of individual modules created by other analysts before the  start of code development.  - Act as the primary reviewer to review the application code, unit test plan created by  software engineers to ensure compliance to defined standards. Recommend changes to  the code as required.       Coaching  - Act as a technical subject matter expert for the internal team on areas such as SAS-ETL,  JCL, and Control-M. Leverage existing knowledge and expertise in multiple ways.  - Build team skills using formal and/or informal training sessions.  - Create and maintain knowledge repositories for lessons learnt and developments in the  respective domains.    Multiple projects completed on SAS-ETL.     Awards/Achievements:    Champions Award  For defect free delivery and exceptional quality (April 2009)    Promoted to Developer Analyst in June 2009    Identified as a member to bring knowledge from US Lincolnshire team to India (Aug- Oct 2009).    Project Completion Award  Received for high quality of work at onsite for period Oct  2009.    Xtra-Miler Award  For leading Dev projects, meeting timelines, and delivering quality  work. (Aug 2010)     Training Completed:  Provided by SAS India Institute    SAS Programming 3: Advanced Techniques and Efficiencies    SAS Macro Language 2: Developing Macro Applications    SAS SQL     Project Name:   SBCSAS   Client:    SBC (AT&T)     Role:  Software Engineer / Developer, Offshore Coordinator for the NEMO Application     Responsibility:  Bottom-line responsibility for coding and research work in SAS, Teradata (FastExport),  Unix Shell scripting. Also responsible for completing on any Major/Minor work request.        Awards/Achievements:    Brain Wave (R.I.S.E.) award won for exceptional performance in an quarter.    KM Super Kruiser award won for knowledge sharing efforts at organizational level.    Best Module award won at portfolio level.    Duration:   2.9 Years  Software:   SAS, Oracle, Unix, Teradata.      EDUCATION    Completed Masters of Computer Application (MCA) from AMITY School of Computer  Science (NOIDA), affiliated to U.P Technical University (Lucknow).    Overall Aggregate : 71.95% [2005]     Completed Bachelors in Computer Application (BCA) from Ajay Kumar Garg Engg.  College (Ghaziabad).     Overall Aggregate : 71.4% [2002]    Passed A.I.S.S.C.E. of CBSE from Bal Bharti Public School.    12th  :  72 % (Best Four)  [1999]    10th  :  70.6 %   [1997]      PERSONAL PROFILE    Date of Birth  : 6th January, 1982  Father's Name  : Mr. Ashok Chander Tyagi  Marital Status  : Married  Nationality  : Indian  Hobbies & Interests : Playing Cricket, Listening to Music, Sketching.  Visa Status  : US B1 visa  LinkedIn Profile : https://www.linkedin.com/in/akshay-tyagi-csm            ", "tokens": [{"text": "Wipro  Alight", "start": 487, "end": 500, "token_start": 72, "token_end": 74, "entityLabel": "company"}, {"text": "Nov 2017  till Sep 2018", "start": 574, "end": 597, "token_start": 94, "token_end": 99, "entityLabel": "period"}, {"text": "AON Hewitt", "start": 769, "end": 779, "token_start": 139, "token_end": 140, "entityLabel": "company"}, {"text": "Dec 2010  Oct 2012", "start": 745, "end": 763, "token_start": 133, "token_end": 137, "entityLabel": "period"}, {"text": "AON Hewitt", "start": 694, "end": 704, "token_start": 121, "token_end": 122, "entityLabel": "company"}, {"text": "Nov 2012  till Nov 2017", "start": 665, "end": 688, "token_start": 114, "token_end": 119, "entityLabel": "period"}, {"text": "Hewitt Associates", "start": 841, "end": 858, "token_start": 155, "token_end": 156, "entityLabel": "company"}, {"text": "June 2009  Dec 2010", "start": 816, "end": 835, "token_start": 149, "token_end": 153, "entityLabel": "period"}, {"text": "Infosys Technologies Limited", "start": 947, "end": 975, "token_start": 175, "token_end": 177, "entityLabel": "company"}, {"text": "13th June 2005  Till: 07th March 2008", "start": 1006, "end": 1043, "token_start": 185, "token_end": 193, "entityLabel": "period"}, {"text": "Pawan Hans Helicopters Ltd.", "start": 1060, "end": 1087, "token_start": 197, "token_end": 200, "entityLabel": "company"}, {"text": "27th Jan 2005  Till: 13th June 2005", "start": 1096, "end": 1131, "token_start": 204, "token_end": 212, "entityLabel": "period"}], "relations": [{"child": 94, "head": 72, "relationLabel": "duration"}, {"child": 114, "head": 121, "relationLabel": "duration"}, {"child": 133, "head": 139, "relationLabel": "duration"}, {"child": 149, "head": 155, "relationLabel": "duration"}, {"child": 185, "head": 175, "relationLabel": "duration"}, {"child": 204, "head": 197, "relationLabel": "duration"}]}, {"document": "                                                              Aman Gupta Noida, UP | +91 9599388969 | 1992aman.gupta@gmail.com https://www.linkedin.com/in/amangupta2  Professional Summary 6.5+ Years of experience in IT industry comprising of Service Delivery, Solution design and Cloud implementation. Providing Cognitive approach about designing, deploying and operating highly available, scalable and fault tolerant solutions. Adept of working with management to prioritize tasks in order to achieve defined project objectives and experienced in coordinating with global teams to deal with technical problems and requirements by working as a teammember or as a Team Leader.  Skills & Abilities  Presales Consultation  Cloud Migration Planning  Infra Solution Designing  AWS Infra Implementation  Project management  Azure Infra Implementation   Disaster Recovery Planning  Gathering Projects Requirements  System Administration  Change Management  Third Party WAF & Firewall integration on  AWS & Azure  Technical Competencies  Amazon Web Services: EC2, S3, RDS, IAM, Cloud Formation, VPC, Cloud front, ECS, Glacier,  Cloudwatch, CloudTrail, ASG, EBS, EFS, Load Balancers, Route53, Redshift, Amplify, CI/CD Pipeline, Code Deploy, AWSWorkspaces, Directory Service, Client VPN, Site to Site VPN,API Gateway, Lambda, Route53, DMS etc. ,   Azure : VMs, Blob Storage, SQL, DMS, VNet, AD, Load Balancer, Container Service, Functions, Auto Scaling, Backup, Site Recovery, VPN Gateway, ExpressRoute, Monitoring, Billing, Key Vault, etc , GCP, OCI   Other Skills: Cloud Cost optimization technique, Migrations, HA Architecture Design, DR, Pre- Sales Consultation, HA setup, Failover plan   Disaster Recovery & Migration Tools: CloudEndure, CloudBerry  Operating Systems: Linux, Windows  Programming languages: CSS, HTML, .Net  Database: MS-SQL, MySQL  Web Server: Tomcat, HTTP Server, IIS, Apache, Apache2  Experience  January 2020 to Current  Sr. Cloud Architect  Sify Technologies Ltd. - Gurgaon, HR   Holding key role in Migrating On-prem datacenter servers to Cloud.  Designing solutions for the Cloud Infra with respect to Applications.  Design the overall Virtual Private Cloud VPC environment including server instance, storage  instances, subnets, availability zones, etc  Understand clients' business and functional needs for creating customized cloud enabled  solutions.  Support the Service Delivery Team to initiate pilot projects, demonstrations, Proof of-concepts  (POC), evaluations, managed services projects and consulting projects.  Implementing and designing new services of Azure for the customers which reduces infra cost &  less time to manage.  Managing the Azure Infra for higher scaleability and availability.  Implemented CI/CD pipelines for deploying the codes at the ease without having any downtime.     Directly worked with the CTO & VPs of the organization for designing new approach for migration traditional application to cloud.   Experience in designing solutions for Disaster Recovery over different availability zones for critical Apps.   Validate the environment to meets all security and compliance controls  Implementing new services of AWS for the customers which reduces infra cost & less time to  manage.  Clix Capital Finance Pvt. Ltd. - Gurgaon, HR (On-Site)   Designing the solutions for implementing new applications in AWS.  Migrating whole infra from On-Prem datacenter i.e. DXC &Wipro to AWS.  Designing methodologies for Automating the Infra through Terraform & Cloud Formation.  Designing solutions for Disaster Recovery hosting on AWS with different availability zones.  Directly worked with the CTO & VPs of the organization for designing new approach for  migration traditional application to cloud.   August 2018 to Dec 2019 Cloud Architect  Rapyder Cloud Solutions Pvt. Ltd. - Delhi, DL   Holding key role in designing strategy for Migrating On-Prem servers to Cloud.  Designing and deploying dynamically scalable, available, fault-tolerant, and reliable applications  on the Cloud.  Understand clients' business and functional needs for creating customized cloud enabled  solutions.  Create and deliver Proposals/Presentations/SOW/WBS docs on time to address all client  requirements.  Managing the Azure Infra for higher scaleability and availability.  Giving best in anchoring client RFPs, ensure superior quality and content of RFP responses.  Support the Service Delivery Team to initiate pilot projects, demonstrations, Proof of-concepts  (POC), evaluations, managed services projects and consulting projects.  Experience in designing solutions for Disaster Recovery over different availability zones.  Owning the delivery of implementation or managed services projects from beginning to end.  Worked with CXOs to resolve Critical problems which all are facing due to traditional  datacenter practices.  Design the overall Virtual Private Cloud VPC environment including server instance, storage  instances, subnets, availability zones, etc  Validate the environment to meets all security and compliance controls.  Implementing and designing new services of Azure for the customers which reduces infra cost &  less time to manage.  Implemented CI/CD pipelines for deploying the codes at the ease without having any downtime.   January 2014 to August 2018  Project Manager & Creative Head  RannLab Technologies Pvt. Ltd.  Greater Noida, UP   Look after Technical Aspects of the Project and Guiding Team towards Technology,  Manages the Projects and leading the team of 15 people.  Closely collaborated with project members to identify and quickly address problems.  Developed implementation methodologies to rein in project costs while meeting key milestones.  Increased customer satisfaction through adherence to all quality standards and customer  requirements.  Headed 100+ project teams specializing in .Net & PHP  Identified plans and resources required to meet project goals and objectives by setting realistic  timelines and checkpoints.  Determined coding requirements for site creation, including e-commerce capability and specialized  scripts.     Transitioned from concepts and layout to deployment by experimenting with themes and genres.  Translated established creative direction into conceptual ideas.  Gathered specifications and other key details to meet requirements of site development.  Gathered and synthesized business requirements and input from customers, stakeholders, business  architects and engineers.   October 2017 to February 2018  Senior Website Developer (Remote Position)  Girlpact, LLC  Greenville, US  Education Master of Technology: Information Technology, 2016 Jaypee Institute of Information Technology - Noida, UP  Bachelor of Technology: Computer Science, 2014 Sharda University - Greater Noida, UP  Certifications  Amazon Solution Architect Certified Associate.  Oracle Cloud Foundation Associate.  CORE JAVA by HP GURUKUL (HP/CTP/N/11-12/00419)  PL/SQL by NIIT Technology  Oracle Database Administration by Oracle University in association with NIIT Technology  Accomplishments  Promoted to Cloud Architect from Senior System Engineer within 11 months of Employment.  Promoted to Project Manager within 12 months of employment.  Promoted to Team Leader within 6 months of employment.  Strength  Ability to handle high-performant as well as novice teams.  Ability to adjust as per the working environment and culture.  Ready to accept challenges.  Self-motivated & confident.  Ability to set goals and priorities.  Personal Details Date of Birth 11thMar 1992 Contact Address 411, 4th Floor, Wonder Homes, Sector 45 , Noida  201303 Languages Known English, Hindi Mobile 09599388969 Email 1992aman.gupta@gmail.com Passport No. K6611325  Declaration I hereby declare that the above information is true and correct to the best of my knowledge.  Aman Gupta   ", "tokens": [{"text": "January 2020 to Current", "start": 1912, "end": 1935, "token_start": 350, "token_end": 353, "entityLabel": "period"}, {"text": "Sify Technologies Ltd.", "start": 1958, "end": 1980, "token_start": 360, "token_end": 362, "entityLabel": "company"}, {"text": "Rapyder Cloud Solutions Pvt. Ltd.", "start": 3791, "end": 3824, "token_start": 685, "token_end": 690, "entityLabel": "company"}, {"text": "August 2018 to Dec 2019", "start": 3750, "end": 3773, "token_start": 677, "token_end": 681, "entityLabel": "period"}, {"text": "RannLab Technologies Pvt. Ltd.", "start": 5345, "end": 5375, "token_start": 963, "token_end": 967, "entityLabel": "company"}, {"text": "January 2014 to August 2018", "start": 5283, "end": 5310, "token_start": 951, "token_end": 955, "entityLabel": "period"}], "relations": [{"child": 350, "head": 360, "relationLabel": "duration"}, {"child": 677, "head": 685, "relationLabel": "duration"}, {"child": 951, "head": 963, "relationLabel": "duration"}]}, {"document": "                                                TECHNICAL SKILLS     Programming Language    Python    Postgre SQL     Libraries       NumPy    Pandas    SciPy    Scikit-Learn    Matplotlib,    Seaborn     Statistical Tools      SPSS    R         Visualization Tools      Power BI    Tableau      Amit Chawla     : +91-9161363536 |    |  | |           Ambitious Data Scientist having 5+ years of experience with a demonstrated ability in delivering hypothesis for business decisions and   data modelling thereby transforming the client business for efficient and enhanced outputs.        AREA OF EXPERTISE                                                                                               AREA OF EXPERTISE      Data Analysis    Descriptive Analytics    Statistical Analysis     Big/Data modelling    Data Visualization    Business Intelligence    Predictive Modelling    Data and Quantitative Analysis    Decision Analytics    Data-Driven Personalization    Data Mining and Visualization Tools    Business Intelligence (BI)    Research, Reports and Forecasts    Machine Learning Algorithms    Supervised Learning (Classification   Model, Logistic Regression, Linear   Regression Decision Tree & Random   Forest)    Exploratory Data Analysis    Statistics    Data Visualization     Machine Learning Algorithms    Classification     Unsupervised Learning   (Clustering, Anomaly   detection, Dimensionality   reduction)    Time Series Forecasting    Recommendation System    ANOVA    Hypothesis Testing              BUSINESS SKILLS      Value Driven Development    Adaptive Planning    Team Leadership    Solution Optimization              PROFESSIONAL EXPERIENCE      Technology Analyst: TATA Consultancy Services, Gurgaon March 19- Till date   Projects:  Banking, Telecom and Healthcare        Leading a team of Data Scientists with experience in formulating and solving complex problems related to Data Science and ML.    Project Management experience in Agile Methodologies with prior experience in Presales for Data Science.     Demonstrated successful track record in managing high performing, highly technical & analytical teams and developing individuals    Business Analyst responsible for sharing analytical insights, recommendation and business proposals for banking project.    Collaborate and build relationships with leaders, business partners and peers to offer direction on analytical approach &   methodologies, insight development and best practices as the global customer insights & analytics Center of Excellence    Performed Exploratory Data Analysis, Dimensionality Reduction for sentiment analysis of the customers to study the purchase trend   and derive hypothesis for the same.    Strong analytical capability including the ability to interpret data and transform into meaningful conclusions and business   recommendations using the visualization tools like Power BI    Excellent analytical skills with a strong ability to translate data and analytics into stories, insight, and actionable recommendations    Exceptional presentation and communication skills - particularly strong in visualising and presenting complex data and analytical   findings to non-technical audiences, and the ability to communicate effectively to a diverse range of teams    Business acumen and understanding of how analytics and insights support a large organisation including being able to successfully   articulate the linkage between business decisions, business objectives, and analytical approaches & findings, turning analytics into   compelling stories for non-technical audience.   mailto:amitchawla9161@gmail.com https://www.linkedin.com/in/amitchawla9161/ mailto:amitchawla9161@gmail.com https://www.linkedin.com/in/amitchawla9161/ https://www.kaggle.com/amitchawla9161     Technology Analyst:  Accenture Services Ltd, Gurgaon                                                                July 2017 till March 2019   Project:  Pharmaceutical, Telecom and Banking    Building and maintaining the predictive models in support of the Customer Intelligence and Personalization strategies for the clients.    Strong analytical capability to interpret data and transform into meaningful conclusions and business recommendations.    Strong ability to translate data and analytics into stories, insight, and actionable recommendations with the help of analytical tools like   Python, R, SPSS and visualizations tools like Power BI.    Project Management experience in Agile Methodologies with prior experience in Presales for Data Science.     Strong analytic experience in advanced modeling techniques and tools such as Python, R and Tableau.     Expertise in Statistics including first order descriptive statistics, types of distributions, hypothesis testing, statistical significance,   probability distributions, Decision Trees, ANOVA and Random Forest.    Familiarity with SQL and the ability to analyses and combine large data sets using tools such as R, SPSS    Exceptional presentation and communication skills - particularly strong in visualizing and presenting complex data and analytical   findings to non-technical audiences, and the ability to communicate effectively to a diverse range of teams    Senior Analyst:  HCL Technologies, Noida                                                                                 September 2015 till July 2017    Project: Shared Service       Responsible for getting insights from data by performing various Hypothesis testing, Time Series Forecasting, Chi- Square Analysis,   ANOVA Techniques and Probability Distribution    Application of Outlier Imputation, Data mining techniques such as Classification, Clustering, Decision Trees, Random Forest, etc.    Responsible for getting insights from data by performing various Hypothesis testing, Chi- Square Analysis, ANOVA Techniques and   Probability Distribution    Anomaly Detection, Outliers Handling, Processing complex data sets through Descriptive Statistics and Inferential Statistics.    Analyzed and processed complex data sets using advanced querying, visualization and analytics tools like Python, R and SPSS.   Skills:      Statistical Analysis: Hypothesis Testing, Inferential Analysis, ANOVA, Predictive Modelling: Multivariate Linear Regression, Clustering,   Decision Trees, Random Forest, Business Analytics and Intelligence     CERTIFICATIONS    Introduction to Python that focuses on python for data science and powerful ways to store and manipulate data.    Python for Data Science from Udemy that visualizes real data with Matplotlib's functions and get acquainted with data structures.    Python Data Science Toolbox from Udemy that focuses on the art of writing python functions to solve problems that are dictated   by data.    Machine Learning with Tree-Based Models in Python from Udemy that focuses on using python to train decision trees and tree-   based models with the scikit- learn library.    Tableau and Power BI for Business Intelligence from LinkedIn Learning.    R Programming: Advanced Analytics in R for Data Science from Udemy.    SQL - MySQL for Data Analytics and Business Intelligence from Udemy.    Google Analytics Certified.      EDUCATIONAL CREDENTIAL       Executive MBA candidate at Department of Management Studies, Indian Institute of Technology Delhi in Technology:   (2018- 2021)    Bachelor of Technology (B.Tech) from Uttar Pradesh Technical University: (2011-2015)  74.20%    Intermediate: 2010-2011 (C.B.S.E Board)  72%    Matriculation: 2008-2009 (C.B.S.E Board) - 83.16 %    ", "tokens": [{"text": "March 19- Till date", "start": 1733, "end": 1752, "token_start": 249, "token_end": 252, "entityLabel": "period"}, {"text": "TATA Consultancy Services", "start": 1698, "end": 1723, "token_start": 244, "token_end": 246, "entityLabel": "company"}, {"text": "July 2017 till March 2019", "start": 3913, "end": 3938, "token_start": 548, "token_end": 552, "entityLabel": "period"}, {"text": "Accenture Services Ltd,", "start": 3818, "end": 3841, "token_start": 542, "token_end": 545, "entityLabel": "company"}, {"text": "September 2015 till July 2017", "start": 5349, "end": 5378, "token_start": 768, "token_end": 772, "entityLabel": "period"}, {"text": "HCL Technologies", "start": 5245, "end": 5261, "token_start": 763, "token_end": 764, "entityLabel": "company"}], "relations": [{"child": 249, "head": 244, "relationLabel": "duration"}, {"child": 548, "head": 542, "relationLabel": "duration"}, {"child": 768, "head": 763, "relationLabel": "duration"}]}, {"document": "                                                      Dr.V.Vignaraj Ananth   102/1, Abirami Nagar, Cheranmahadevi Tirunelveli 627006 | +91 9486088422| vignaraj254@gmail.com |      Profile   To achieve high career growth through a continuous learning process, keep myself dynamic, visionary and competitive   with the changing scenario of the world. Friendly and engaging team player, multi-tasker, leadership, people skills and   able to inspire staff to perform their best. Detail oriented and experienced researcher with in-depth knowledge of Virtual   Reality, Machine Learning and Software Project Management.      Work Experience   ASSISTANT PROFESSOR-CSE | THIAGARAJAR COLLEGE OF ENGINEERING | OCTOBER   2013 - PRESENT    Expert in doing Society Relevant projects.   Placement Officer for Recruitment process with the major IT sectors like Amazon, Microsoft, VMware and holds 95%   job offers.    Published 10 Research articles in reputed journals.   Member in Finance Team managing budgets and estimation.      ASSISTANT SYSTEM ENGINEER | TCS | JUNE 2011  SEPTEMBER 2011    Created a cross-training program using programming languages like Java, Blender, Unity 3D.   Developed web based and centralized banking solution covering all the functions of a bank.      Education    Doctor of Philosophy- Faculty of Information and Communication Engineering |February  2020|Anna University, Chennai, Tamil Nadu.   Thesis title: Design and Analysis of Machine Learning Models for Software Effort Estimation.   First Class with Distinction -CGPA:10/10    Master of Business Administration  Human Resource | December 2018| Manonmaniam Sundaranar  University, Tirunelveli, Tamil Nadu   First Class with Distinction -CGPA:8.0/10    Master of Engineering-Computer Science and Engineering| May 2013| Thiagarajar College of  Engineering|, Madurai, Tamil Nadu   First Class with Distinction -CGPA:9.57/10   Best Outgoing Student, ME CSE, TCE   Topper of the batch, ME CSE, TCE    Bachelor of Technology-Information Technology| May 2011| Thiagarajar College of Engineering|, Madurai,  Tamil Nadu   First Class with Distinction -CGPA:9.00/10      Areas of Interest    Software Engineering   Data Mining   Machine Learning      Certifications and Awards   Certified. \"Blockchain Essentials\", an online course on Cognitive Class, IBM.   Blockchain Basics, an Online course offered by The State University of New York and secured 94.5%.   Machine Learning: Classification, an Online course offered by University of Washington by secured 98%.   Software Development Processes and Methodologies, an Online course offered by The State University of New   York and secured 97%.   Fellow of Computer Science Research Council (FCSR), Global Journals, Open Association of Research Society.   mailto:vignaraj254@gmail.com      Publications  International Journals      1. V.Vignaraj Ananth, Dr.S.Muthuramalingam, A Transmission Range Based Topology Maintenance Algorithm  for Manets, Australian Journal of Basic and Applied Sciences 8(18) December 2014, Pages: 361-366.      2. V.Vignaraj Ananth, Dr.N.Shivakumar, \"Software cost estimation using function point with non-algorithmic  approach\", Global Journal of Computer Science and Technology, ISSN 0975-4172.      3. V.Vignaraj Ananth, Dr.S.Srinivasan, AGS : A precise and efficient AI based hybrid Software Estimation Model,  International Journal of Business Intelligence and Data Mining, Inderscience 2019.      4. V.Vignaraj Ananth, Dr.Srinivasan, Empirical based fuzzy effort estimation on story points, Journal of Electrical  Engineering, 2019.      5. V.Vignaraj Ananth, Dr.Srinivasan, Judgement Based Effort Estimation for Software Management: A Survey,  Journal of Computational and theoretical Nanoscience,2018.      6. V.Vignaraj Ananth, \"A Field Test to Estimate Efficiency of Rewound Induction Motor\", Journal of Electrical and  Electronics Engineering,2013.      7. V.Vignaraj Ananth, Dr.Srinivasan, Estimating the Accuracy of Software Cost Using Machine Learning  Techniques, Tierarztliche, 2020.      8. V.Vignaraj Ananth, Dr.Srinivasan,Cluster Based Regression Method For Software Effort Estimation, Solid  State Technology, 2020.      9. V.Vignaraj Ananth, S.Aditya, Regression Model to Estimate Effort for Software-Oriented Projects, Advances in  Intelligent Systems and Computing, Springer, 2021.      10. V.Vignaraj Ananth, Dr.Srinivasan, A Survey on Effort Estimation Techniques in Agile Software Development,  International Journal of Interdisciplinary Global Studies (IJIGS),2020.     Conference Proceedings   1. Effort based software estimation using regression methodology, Artificial Intelligence And Computer Vision,  Anna University Chennai,2018.   2. \"A survey on software effort estimation\", International Conference On Electrical, Electronics, And Optimization  Techniques, Chennai, DOI: 10.1109/iceeot.2016.7755665, publisher: IEEE.   3. \"An innovative mobility based self-stabilizing clustering algorithm for manet\" International Conference On  Computing, Communication, Electrical, Electronics, Devices And Signal Processing (CCEEDS), Andhra Pradesh.   4. \"An energy-conversing topology maintenance algorithm in Manets, International Conference On Eco-Friendly  Computing And Communication Systems, NITK Surathkal.   5. \"Fuzzy based software cost estimation for non-algorithmic approach\", National Conference On Information And  Network Management.     Patents & Books   1. \"Novel Algorithm for Software Development Effort Estimation(Filed), No. 201941017231,Dated: 16-01-2019  2. V.Vignaraj Ananth, N.Shivakumar, \"Analysis and Design of Software Development Effort Estimation\", LAP   Lambert Academic Publishing (2018-08-10 ).           Page 2    ", "tokens": [{"text": "THIAGARAJAR COLLEGE OF ENGINEERING", "start": 663, "end": 697, "token_start": 105, "token_end": 108, "entityLabel": "company"}, {"text": "OCTOBER   2013 - PRESENT", "start": 700, "end": 724, "token_start": 110, "token_end": 114, "entityLabel": "period"}, {"text": "TCS", "start": 1046, "end": 1049, "token_start": 172, "token_end": 172, "entityLabel": "company"}, {"text": "JUNE 2011  SEPTEMBER 2011", "start": 1052, "end": 1077, "token_start": 174, "token_end": 178, "entityLabel": "period"}], "relations": [{"child": 110, "head": 105, "relationLabel": "duration"}, {"child": 174, "head": 172, "relationLabel": "duration"}]}, {"document": "Anusha R                                                                                                   Ph: +91-9600031470  Email : anushark92@gmail.com  Company Designation: Sr. Infra developer (associate level)  Project Designation: Current role -> Bigdata Engineer (Developer)      Job Objective   To secure a challenging position in a reputable organization to expand my learnings, knowledge, and skills.    Learnings & Certifications  Certification done on 2019 in AWS solution architect- Associate from Amazon  IBM Tivoli Netcool Omnibus 7.4 certified.  Completed PG Diploma course in Data Science from  IIITB completion Date March 2020    Employment Details  Working in Cognizant technology solutions from March 25th 2014 to till date.    Profile Summary  Having 7, yrs. of overall experience in cognizant technology solutions(cts), whereas 4 yrs. of experience in Fault Management Tool (Tivoli Netcool suit) technology and 3 yrs of experience in BigData technology.    Having a countable hands on experience in Hive.    Involved in creation of tables by understanding the business and the inputs from the client    Techniques like partitioning and bucketing are applied on the tables    Have hands on exp with Hadoop command line.    Built pipelines for data ingestion using oozie framework    Involved in data ingestion tasks in hive.    Created shell/python scripts for automation purpose    Having knowledge on performance tuning and optimization techniques  Having Onsite experience.  Worked as a Netcool administrator for processing alarms in to IBM Netcool suit omnibus/impact  Worked on Rundeck to automate regression test cases.  Having experience in creating buckets/storing/retrieving objects in aws s3 using command line.  Worked exp in working on Aws Athena database for retrieving from it using boto3 utility.       TECHNICAL SKILLS    Operating Systems  Red Hat Linux, Solaris  Programming languages  Pyspark,hql,core java(basics)  Scripting  ksh, Perl,Python  Versioning tool  Tortoise SVN,GIT  Project tracking tool  Jira  Database  Oracle, Sybase and SQL Developer. Athena DB,hive  Cloud  AWS components like s3, ec2  Automation tool  Rundeck,oozie  Distributed Filesystem  HDFS,S3      PROESSIONAL EXPERIENCE IN BIGDATA  Company: Cognizant technology solutions  Project 1: US Mainframe scoring   Client: D&B (Dun & Bradstreet)  Duration: April 2018  2019 Jan  Team size: 3 Members  Technologies: hql, AWS, oozie, shell script,python     Description:   US mainframe Scoring project is an ETL requirement from different table and transforming the data as needed and sending the data as mainframe fixed file to downstream.     Roles and Responsibilities.  Requirement analysis,   Written complex hql logic to transform data from the required sources and sending the data as a mainframe file to the downstream.  Written python/shell script to transform data and partitioning the data in S3.  Created oozie workflows to scheduling the job.    Company: Cognizant technology solutions  Project 2: Business trust score  Client: D&B (Dun & Bradstreet)  Duration: Jan 2019  Feb 2020  Team size: 3 Members  Technologies: python/hql, AWS     Description:   Scoring project identifies the companies which are at risk, can layoff employees, also identifies the companies that are bankrupt and started with different name.     Roles and Responsibilities.  Requirement analysis,   Worked on python, implemented score logic with hqls.      Project 3: US & UK scoring   Client: D&B (Dun & Bradstreet)   Duration: Feb 2020 - Till Date  Team size: 9 Members    Client Description: - The Dun & Bradstreet Corporation is a company that provides commercial data, analytics, and insights for businesses. The big    Project Description:  In D&B, we receive huge chucks of customer data to be ingested in to AWS bigdata. The data has to be cleansed, check for quality and then ingested in to bigdata environment.    Roles & Responsibilities   Creation of oozie workflow to automate the ingestion process for various sources.  Automating ingestion process using shell/python script.  Have hands on exp in working on Athena DB, fetching data from it by writing python script and storing the data in s3 using boto client.  Worked on Python Libraries-Boto client,Pandas to fetch data/compare the data for future processing.  Written python code for converting a json file to csv.  Written complex hql for transforming data from tables and storing the output in s3.  Have hands on exp with Hadoop command line.  Written few pyspark code to transform data needed for data analytics and ran them in cluster mode.  Involved in creating partitioning and bucketing of tables wherever required.  Sending audit reports for the scores.      PROESSIONAL EXPERIENCE IN NETCOOL  Company: Cognizant technology solutions  Project: Event Management Applications (EMMA)  Duration: March 2014  April 2018  Team size: 9 Members  Client Description: - DTAG (Deutsche Telekom) is a leading German Telecommunication company. Cognizant handles the Event Management Application for DTAG.  Project Description:  EMMA is the Event Management Systems of DTAG- Germany. The system EMMA (Event Management Application) that processes Alarm is based on a Netcool/Omnibus System. Primary task of the system is to promptly provide a processed alarm list to the end users to work on. System EMMA handles the Mobile and Fixed networks of German Telekom.  Events sent by different Network-Management-Systems (NMS) can be displayed with Netcool. Different triggers and procedures are set in specific Object servers. These automations are processed at regular intervals and they process the alarms received further. Other products such as WebSphere MQ and Impact are used, so that more complex correlations are enabled.    Roles & Responsibilities:    Installation and configuration of Netcool Omnibus from scratch.  Integration of Omnibus with different probes, ODBC/JDBC gateway and unidirectional-bidirectional gateways.  Providing solutions and automations using triggers.  Handling SQL commands in Netcool Omnibus.  Developed customized rules file based on client requirement.  Dealing with Process Agent (PA)  Building new automations in Omnibus using shell script, procedures and triggers.  Worked on IPL policy/webservice(GetByXpath) for enrichment.  Worked on correlation enrichment concepts.  Worked on starting and stopping of Queue manager, checking of the queue length.  Worked on policies that connects to various interfaces and HPSM ticketing tools.    EDUCATION  Bachelor of Information Technology, Easwari Engineering College affiliated to Anna University: completed in the year of 2013    PERSONAL PROFILE  Permanent Address  Mango block b2, Orchards Apts arcot road vadaplani Chennai-26  Date of Birth  02/07/1992  Marital Status  Single  Gender  Female  Nationality  Indian", "tokens": [{"text": "March 25th 2014 to till date", "start": 716, "end": 744, "token_start": 109, "token_end": 114, "entityLabel": "period"}, {"text": "Cognizant technology solutions", "start": 680, "end": 710, "token_start": 105, "token_end": 107, "entityLabel": "company"}], "relations": [{"child": 109, "head": 105, "relationLabel": "duration"}]}, {"document": "AnandDev Singh  ananddevsingh.aiet@gmail.com  Contact No. +91 9599-663363 / +91 9555-813321      Experience : 7.8 Years  Position : Senior Technical Consultant  Technologies : JavaScript, React JS, Redux, TypeScript and Apollo Client  UI  : HTML/5, CSS/3, Material UI, Styled Components, Flex-box      Experience Summary    Having 7+ Years of Experience in design, development and Implementation of software applications.  More than 5 years of Experience in Development of responsive websites.  Strong in developing applications using JavaScript, React JS, Redux, React-Redux, Redux-Thunk, Redux-Saga, Apollo Client, HTML5 & CSS3 along with Material UI.  Good knowledge of TypeScript, Node JS and Alexa.  Good exposure on the Unit Testing frameworks like Jest along with enzyme and react testing library.  Good exposure on application usability and accessibility.  Good understanding of the styled-components for the Reactive applications.  Good understanding of JIRA tool.  Good in managing the code repositories using Git Hub & Git Lab.  Team Member with a strong work ethic, committed to work hard and smartly.      Qualification     B.Tech (IT) from Azad Institute of Engineering & Technology, Lucknow.      Summary of Work Experience    IRIS Software.   Sep19Till now, Senior Technical Consultant.  GlobalLogic.    Aug16  Sep19, Associate consultant.  Aon Hewitt.    May14Jun16, Sr. Software Engineer.  Pyramid IT.   Nov13May14, Software Engineer (On Contract with Aon Hewitt).  CPM India.   Jun13Nov13, Software Engineer.      Skill Sets     Operating Systems : Window 7/10 and MacOS.  Data Base   : SQL Server 2008/2008 R2/2012.  Tools & Technology : Kentico CMS 7/8/9, .NET 3.5/4.0, C#, ADO.NET.  IDE    : Microsoft Visual Studio, VS code, Brackets etc.  Programming Languages : JavaScript, ES6, ReactJS, TypeScript, Alexa & Apollo Client.  Designing   : HTML/5, CSS/3, Bootstrap, Flex-box, styled components.  Reporting Tools                      : High charts.  Version Management Tools     : TFS, GitHub, GitLab.  Agile Related Tools               : TFS, JIRA.        Project Experience      Project 1    Project  : Products on Platforms  Period   : Sep 2019  Till date  Client   : Lord Abbett  Team size : 6  Role                  : Front-end Developer (Developing reusable components and unit testing)  Environment  : JavaScript, React JS, Apollo Client (with GraphQL backend), Material UI.  Methodology : Agile  Unit Testing : Jest with React test renderer and Apollo client testing.    Description:  The financial advisors of Lord Abbett were managing the firms, platforms and products data or excel. We have developed an application that gives then interface to create the firms, platforms and available products data that can be easily available to those who need quick answers to the questions regarding which products are available on which platforms and which of those are recommended i.e. Regional Managers.     Responsibility:  Initial setup of the framework from scratch.  Designing and developing the components.  Unit testing of modules using Jest with React test renderer and Apollo client testing library.  Leading the project & two junior resources.      Project 2    Project  : ON!Track  Period   : Sep 2017  Sep2019  Client   : Hilti  Team size : 14  Role                  : Front-end Developer (Developing reusable components and unit testing)  Environment  : TypeScript, React JS, Material UI, Styled Components, Redux, Redux-Saga.  Methodology : Agile  Unit Testing : Jest with Enzyme    Description:  It is a Liechtenstein multinational company that develops, manufactures, and markets products for the construction, building maintenance, energy and manufacturing industries, mainly to the professional end-user.    Responsibility:  Designing the framework to support styled components.  Developing the components.  Did unit testing for modules using Jest with Enzyme.            Project 3    Project  : Future Focus  Period   : Oct 2017  Aug 2018  Client   : The Economist Group  Team size : 4  Role                  : Developer (Developed Alexa skill, documentations and unit testing)  Environment  : NodeJS, Alexa, Lambda.  Methodology : Agile  Unit Testing : Jest    Description:  Machine learning and AI is a sub-module developed for Future Focus. It focuses on social media integrations of the Mobile Apps with Facebook and Google Newsstand. Also its main focus area is to create the AI enabled apps and Skills which will run on Amazon Alexa/Echo devices.    Responsibility:  Writing & deploying the Lambda functions, consuming & parsing the GraphQL end-points.  Creating the Alexa skills, Intents and consuming the lambda functions.  Did unit testing for modules using Jest.    Project 4    Project  : Online Assets  Period  : May 2014  Jun 2016  Client   : Aon Hewitt Australia, New Zealand  Team size : 4  Role                  : Developer (Kentico webparts, widgets, SQL stored procedures, unit testing)  Environment  : C#, .Net 4.5,VS 2013, JQuery, AngularJS, KenticoCMS 7/8/9, Sql Server 2012  Methodology : Agile  Live URL : https://aonhewitt.com.au    Description:  Online Assets, an informative website built for Aon Hewitt Australia, focusing on Employers and Employees. The focus is on human resource management which helps employers to maximize their employees potentials. It is based on Kentico CMS, which helps content authors to manage it.   Responsibility:  Designed and developed Layout, Page Templates, and Kentico Web Parts for different pages.  Used JQuery, AngularJS to bind data with these web parts.  Did unit testing for their modules like hot topics, report and survey, training and events etc.      Personal details     Name : Anand Dev Singh   Fathers Name : Mr. Gulab   Marital status : Single   Date of birth : 04th Jan1987   Native place : Allahabad   Hobbies : Writing, Running & Solving Rubik's cube      (Anand Dev Singh)", "tokens": [{"text": "IRIS Software", "start": 1242, "end": 1255, "token_start": 239, "token_end": 240, "entityLabel": "company"}, {"text": "Sep19Till now", "start": 1259, "end": 1272, "token_start": 243, "token_end": 244, "entityLabel": "period"}, {"text": "GlobalLogic", "start": 1304, "end": 1315, "token_start": 251, "token_end": 251, "entityLabel": "company"}, {"text": "Aug16  Sep19", "start": 1320, "end": 1332, "token_start": 254, "token_end": 256, "entityLabel": "period"}, {"text": "Aon Hewitt", "start": 1357, "end": 1367, "token_start": 262, "token_end": 263, "entityLabel": "company"}, {"text": "May14Jun16", "start": 1372, "end": 1382, "token_start": 266, "token_end": 266, "entityLabel": "period"}, {"text": "Pyramid IT", "start": 1408, "end": 1418, "token_start": 274, "token_end": 275, "entityLabel": "company"}, {"text": "Nov13May14", "start": 1422, "end": 1432, "token_start": 278, "token_end": 278, "entityLabel": "period"}, {"text": "CPM India", "start": 1484, "end": 1493, "token_start": 291, "token_end": 292, "entityLabel": "company"}, {"text": "Jun13Nov13", "start": 1497, "end": 1507, "token_start": 295, "token_end": 295, "entityLabel": "period"}], "relations": [{"child": 243, "head": 239, "relationLabel": "duration"}, {"child": 254, "head": 251, "relationLabel": "duration"}, {"child": 266, "head": 262, "relationLabel": "duration"}, {"child": 278, "head": 274, "relationLabel": "duration"}, {"child": 295, "head": 291, "relationLabel": "duration"}]}, {"document": "                                                      Ashish Mishra                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Contact No: 91 7893295396                                                                                   Sayantan Chowdhury                                                                                                                                                                                                                                                                                                                     Email id:  sayantanam1900@gmail.com                                                                            Experience Summary:    I have 8.7 years of experience as a Business/Data analyst. Including 4 years of experience as lead analyst. I am employed  with TCS for the full tenure of my experience and have worked in Insurance and Retail domains, I had joined TCS after  completing my MBA in Marketing and Finance.     Skills:   Domain knowledge: Property and Casualty insurance, Pricing/Payment, licensing workflow, Retail Supply Chain.   Technical Skills: SQL, HADOOP, Microsoft Azur, Teradata, Excel & Macros, Python,   Tools used: Microsoft Visio (for UML), Adobe XD (for prototyping), Google Analytics JIRA, Salesforce   Work Experience:   Client:  Walmart (current project)   Role: Data Analyst/ Lead Analyst         Project Model: Agile        Duration  Jan 2020 to present   Project Description - iCan is a Supplier Negotiation tool which helps to create Data Analytics solutions that would aid  business to gauge vendor negotiations across various geographies and to plan the optimal supply chain strategy for  different markets.   Responsibilities    Have analyzed data received from markets in different geographies and identified the best possible way for   implementation of business logic.    Carried out Requirement Analysis and Requirement Elicitation for different Data Analytics tools    Have created requirement documents for developing different metrices necessary for generating supplier   score cards (Analytics Reports)    Have been using Microsoft Azur, Teradata and Hadoop to validate data and to analyze requirements     Have used Excel and Macros to validate data and to generate reports.    Have investigated and rectified issues related to data or business logic    Have created the plan for supplier scorecards     Have done comprehensive validation/QA of all subject area metrices after development and have done data   deep dive wherever necessary to identify the root cause for data inconsistencies or code /logic issues.    Have worked closely with the dev team to clarify any gaps in requirement understanding.    Have worked with market representatives to clarify and remove any possible gaps in requirement/business   logic.    Have worked to aid defect free end to end integration of all the subject area matrices and also the validation   of final reports/table.    Have acted as backup scrum master and have managed sprint planning and backlog grooming.   Client: John Wiley & Sons   Role: Business/Data Analyst / Lead         Project Model: Agile         Duration  Sep 2015 to Dec 2019     Project Description - Custom Web Solutions develop and deliver various websites for books and journals printed in  Wiley.    Responsibilities:     Implemented Salesforce (Service Cloud) as an incident management system and worked on all the   requirements for creating different queues.    Collected requirements for standalone websites related to different business assets.    Used Google Analytics to setup different metrics in order to track traffic on different websites and to identify   user trends.    Worked to analyze data from different sources to implement pricing logic to project product valuation.    Worked on the requirements for both pricing and payment tools related to various segments of products.    Have worked on requirements related to Licensing tools, which can process submission, review and   typewriting writing workflow of manuscripts and can determine relevant royalties.    Have created UML diagrams and wireframes to support implementation of requirements    Have set up BDD sessions (Business Driven Development) to enrich requirements and create acceptance   criteria.     Have performed validation of requirements before UAT testing   Client: State farm USA    Role: Business Analyst                Project model  Waterfall              Duration  Aug 2012 to July 2015   Project description  State-firm Account is a web-based application which incorporates and automates all the process  of an insurance lifecycle from policy creation and premium collection to final disbursement/closure, giving end users a  seamless experience and allows business to have a consolidated portal to monitor the performance of their products.   Responsibilities:     Have worked on BRD documents and have interacted with the client representatives to clarify the   requirements, Business Rules, Data Elements etc.    Have created Use Cases and UML diagrams for every Property & Casualty insurance workflow.    Have used excel and macros to analyze backend data.    Used prototyping to identify issues/gaps in provided business logic.    Have worked on requirements related to policy claim and disbursement, policy initiation channels, different   transaction processes (cards, ACH etc.)    Also worked with the testing team to create test cases for QA.    Also worked in collaboration with third party analyst teams for requirement elicitation.     Academic Qualifications:   Post - Graduation  MBA in Marketing and Finance (from Institute of Public Enterprise, Hyderabad)     Graduation  Blech in Biotechnology (West Bengal University of Technology)    Personal Details:        Fathers Name         :    Abhyuday Chowdhury        Mothers Name                            :    Chitra Chowdhury        Date of Birth                 :    17/02/1987        Gender                            :    Male        Nationality                :    India              ", "tokens": [{"text": "Jan 2020 to present", "start": 1887, "end": 1906, "token_start": 175, "token_end": 178, "entityLabel": "period"}, {"text": "Walmart", "start": 1780, "end": 1787, "token_start": 155, "token_end": 155, "entityLabel": "company"}, {"text": "John Wiley & Sons", "start": 3530, "end": 3547, "token_start": 449, "token_end": 452, "entityLabel": "company"}, {"text": "Sep 2015 to Dec 2019", "start": 3632, "end": 3652, "token_start": 470, "token_end": 474, "entityLabel": "period"}, {"text": "Aug 2012 to July 2015", "start": 4939, "end": 4960, "token_start": 682, "token_end": 686, "entityLabel": "period"}, {"text": "State farm USA", "start": 4835, "end": 4849, "token_start": 666, "token_end": 668, "entityLabel": "company"}], "relations": [{"child": 175, "head": 155, "relationLabel": "duration"}, {"child": 470, "head": 449, "relationLabel": "duration"}, {"child": 682, "head": 666, "relationLabel": "duration"}]}, {"document": "SINCHAN DATTA    Phone no: 9874878551  Email ID: sinchandatta90@gmail.com            EXECUTIVE SUMMARY      An IT professional with total 6 years and 4 months of working experience in various technologies which includes Hadoop Development, Data Science, Machine Learning, Python, AWS, Teradata, SQL and Unix.    Quickly Adapts to New Professional Settings    Fast Learner; Acquires Skills/Knowledge Easily    Persuasive, Influential, Effective Communicator    Skilled in Research/Analysis/Statistical Functions    Extensive Technical and Analytical Skills    Strong Organizational and Time Management Skills  Skills/Technical Expertise  6 years 4 months of Industry Experience in Big Data, Data Science, Python, AWS, SQL, Unix.  Proficient in Apache Hadoop, HDFS, Hive, Pig, Oozie, Kafka, Oozie, Sqoop.  Experience in data ingestion and data transformation from relational databases to Data Lake.  Experience Spark with Scala, Java and Python.  Experience in Python Numpy, Matplotlib and Pandas.  Experience in AWS IAM, EC2, S3, EBS, ELB, RDS, DynamoDB, EMR, RedShift.  Experience in HBase NoSQL Database.  Experience in Machine Learning Algorithms K nearest Neighbour, Naive Bayes, Logistic Regression, Linear Regression, Support Vector Machines, Decision Trees, Ensemble Models.  Experience in Natural Language Processing.  Experience in Performance measurement of Machine Learning Models.  Experience in Dimensionality Reduction and Visualization using PCA, t-SNE.  Experience in Exploratory Data Analysis.  Understanding of Linear Algebra, Probability and Statistics, Feature Engineering.  Experience in Data Ingestion from Relational Databases to Hadoop in Data Lake.  Exposure to Performance tuning in Teradata and Hadoop.  Experience in HTML, CSS and JavaScript.  Highly adaptable, quick learner, planning skills, honest and hardworking.  A good team player with strong communication & interpersonal skills.     Area of Expertise  Big Data Ecosystems: Hadoop, Hive, HDFS, Sqoop, Oozie, Pig, Spark.  Programming Languages: Scala, Java, Python  Database Programming: Teradata, Oracle.  Web Technologies: HTML, CSS, JavaScript.  Scripting Languages: UNIX.  Job Scheduling Tool: Autosys.  Platforms:  Mac OS X.  Education  Post Graduate Diploma in Business Analytics (Executive) from IMT Ghaziabad Center of Distance Learning (2017-2018)  Bachelor of Technology (Computer Science &Engineering) from Guru Nanak Institute of Technology (2008-2012)          Professional Experience   Company Name: Cognizant Technology Solutions India Private Limited (Oct 2016 - Present)  Project #1:  Title:   Data Ingestion and Data Transformation  Client:     Leading Bank  Role:    Big Data Developer  Project Summary:     This project is to ingest and transform the data sources from multiple relational data sources to Data Lake. This is a development project to implement transformation logic using Spark and Scala. Transformation are written in Scala and applied on data using Spark SQL. Implemented models using machine learning models and natural language processing for the application identifying use case along with Exploratory Data Analysis.    Role and Responsibilities:    Coding   Unit Testing  Ensure Day to Day Seamless Delivery            Tools & Technology:  HDFS, Hadoop, Hive, Shell Scripting, Scala, Spark, AWS, Python, Redshift, Data Lake, Python, Machine Learning, Natural Language Processing    Project #2:  Title:   Data Ingestion and Transformation  Client:     A leading Insurance Company  Role:    Big Data Developer  Project Summary:     This project is to ingest and transform the data sources from multiple relational data sources to Data Lake. This is a development project to ingest raw data Ingested into HDFS system and transformation logic implemented on this using Spark. Finally, the processed data consumed by Analytics team for reporting purpose.    Role and Responsibilities:    Coding   Unit Testing  Ensure Day to Day Seamless Delivery            Tools & Technology:  HDFS, Hadoop, Hive, Pig, Shell Scripting, Core Java, Spark, AWS, Python, Redshift, Data Lake, Python    Project #3:    Title:   Data Ingestion  Client:     A leading Insurance Company  Role:    Big Data Developer  Duration:   November 2016 to April 2017  Project Summary:     This is a development project to ingest their sensitive and non-sensitive data into Hadoop ecosystem. The data source comprises of Oracle, DB2 and ADABAS files. Raw data Ingested into HDFS system and transformation logic implemented on this using Spark. Finally, the Processed data consumed by Analytics team for reporting purpose.          Role and Responsibilities:    Coding (involves fetching metadata for various data sources like Oracle, DB2, ADABAS files using Jar. Sqoop the data based on that then basic cleansing with Pig. Put the data into Hive Landing and persistent zone)  Use Autosys scheduler to make it as a Job  Unit Testing  Ensure Day to Day Seamless Delivery           Tools & Technology:  HDFS, Hadoop, Hive, Pig, Shell Scripting, Core Java, Python  Company Name: Infosys Limited (February 2014- September 2016)  Project #1    Title:   Reports and Charts Generation  Client:     A leading product based company  Role:    Big Data Developer  Duration:   From June 2014 to Jan 2015  Project Summary:           Charts Reporting project provides daily sales extracts of music and related contents to external partners based on the data received from data warehouse.    Role and Responsibilities:    Development and enhancement of reports by ETL technologies using Teradata and Hadoop.  Involved in writing Teradata Stored Procedures, Macros and Complex SQL Queries.  Involved in writing Hive queries and job scheduling through Oozie workflow.  Performance Tuning of Hive Queries.  Scheduling of jobs through Autosys Tool.  Involved in implementation of relevant SDLC activities.  Involved in Autosys Job creation and Configuration files.    Tools & Technology:  HDFS, Hadoop, Hive, Teradata, UNIX, Sqoop, OOZIE    Project #2    Title:   Analytics Project  Client:     A leading product based company  Role:    Big Data Developer  Duration:   From Feb 2015 July 2016  Project Summary:   Analytics project provides various insights and thus a thorough analysis of business to      the client which helps the client to take effective strategic and business decisions.     Role and Responsibilities:    Development and enhancement of reports by ETL technologies using Teradata and Hadoop.  Development of adhoc reporting requests using Hadoop and Hive.  Experience in Sqoop to transfer data from RDMS to Hadoop and vice versa.  Performance Tuning of Hive Queries.  Involved in working with Teradata Utilities like Fastload, FastExport and Multiload.  Involved in implementation of relevant SDLC activities.    Tools & Technology:  HDFS, Hadoop, Hive, Teradata, UNIX, Sqoop      Project #3:    Title:   Reports and Charts Generation  Client:     A leading product based company  Role:    Big Data Developer  Duration:   From August 2016 to September 2016  Project Summary:   Charts Reporting project provides daily sales extracts of music and related contents to      external partners based on the  data received from data warehouse.    Role and Responsibilities:    Development and enhancement of reports by ETL technologies using Teradata and Hadoop.  Involved in writing Teradata Stored Procedures, Macros and Complex SQL Queries.  Involved in writing Hive queries and job scheduling through Oozie workflow.  Performance Tuning of Hive Queries.  Scheduling of jobs through Autosys Tool.  Involved in implementation of relevant SDLC activities.  Involved in Autosys Job creation and Config files.    Tools & Technology:  HDFS, Hadoop, Hive, Teradata, Unix, Sqoop, OOZIE", "tokens": [{"text": "Cognizant Technology Solutions", "start": 2498, "end": 2528, "token_start": 471, "token_end": 473, "entityLabel": "company"}, {"text": "Oct 2016 - Present", "start": 2552, "end": 2570, "token_start": 478, "token_end": 481, "entityLabel": "period"}, {"text": "Infosys Limited", "start": 5059, "end": 5074, "token_start": 962, "token_end": 963, "entityLabel": "company"}, {"text": "February 2014- September 2016", "start": 5076, "end": 5105, "token_start": 965, "token_end": 968, "entityLabel": "period"}], "relations": [{"child": 478, "head": 471, "relationLabel": "duration"}, {"child": 965, "head": 962, "relationLabel": "duration"}]}, {"document": "RESUME    Name  Bhanuchandar Kandi  Contact  +91-8143622971  Total Experience  6.2+ years      Operating Systems  Windows  Technologies  Data science, Data analytics  Languages  Python, R  Database   MySQL, Postgresql  Skills Summary   6.2+ years as a Data scientist and data analyst into various domains such as telecom, financial and hospitality and manufacturing.   Strong expertise in Predictive analytics with machine learning, deep learning, statistical modeling    Expertise in regression, classification and segmentation/clustering and good knowledge in nlp.   Tools / technologies learning : AWS, Docker   Educational Qualifications   Masters in ML & AI and PG Diploma in ML & AI from Liverpool John Moores University and IIIT-Bangalore. (In progress: 2020-2021)   MBA from Osmania University, Hyderabad, 2014   B.Tech from JNTU Hyderabad, Hyderabad, 2012  PROFESSIONAL EXPERIENCE     Role  Data Scientist  Responsibilities    Organization: HCL Technologies  Oct 28th 2020 - Till date  Languages : Python  Tools: MS office   Involved in resource reviews and recruitment for the team and few POCs, and Looking forward for a challenging role.          Project Name     Tool price prediction  Client    Applied Materials  Role  Sr. Data scientist  Responsibilities  Building regression model and also implemented rule based model   Organization: Perint Info Systems  Duration : 1 Year  Languages: Python, Postgresql   Tools: Jupyter notebook  Project Description            Applied materials is a material engineering solution provider which is into manufacturing of semiconductor chips and also tools used to manufacture them. The business problem is the difficulty facing by organization to come up with the accurate pricing and we were asked to build a machine learning model to help them in deciding the best pricing and increase the revenue.        Project Name     Material cost prediction  Client    Applied materials  Role   Sr.Data scientist    Responsibilities  Building regression model  Organization: Perint Info Systems  Duration : 1 Year and maintenance  Languages : Python, Postgresql  Tools: Jupyter   Project Description            The business problem is the organization wants to understand the patterns in the material costs incurring by the suppliers and vendors, and to understand and estimate KPIs from the data and build a machine learning model to help them in deciding the best pricing and increase the revenue.      Also involved in maintaining the deployed pipelines and handling the issues by thoroughly monitoring and  debugging the model code in the production environment.      Project Name     Cabin class prediction and Cabin price prediction  Client    RCCL  Role   Sr. Analyst    Organization: Accenture Solutions, Bangalore  Duration : 5 months  Languages : Python, R, Mysql  Tools: Pycharm, PowerBI  Project Description          RCCL (Royal Caribbean Cruise Ltd) is tourism and hospitality giant in US region, the business problem is to come up with best models to help them in predicting the class of cabin a passenger going to book based on the historical data and also deciding the price based on various features.       Project Name     Truck load cancellation  Client    Comcast  Role   Sr. Analyst    Organization: Eclerx Services  Duration :  7 months  Languages :  Python, MySql, mongodb    Tools: Pycharm, jupyter,spyder    Project Description   Comcast is giant telecom, cable tv and internet provider in the US region. The business problem and project goal is to build a classification model to help the business in processing the customers complaints and provide the services as per the problem whether the customer needs a home service or technical support through the other modes of communication.    Also involved in other tasks such as cleaning the noisy text data and summarizing the important content and visualize to the superiors.    Project Name     Various regression, classification, clustering and nlp (chatbot), text analytics projects for the customers of the client into banking and telecom domains.  Client    Global Data Plc   Role   Data scientist   Organization: Grapple Info solutions  Duration : 3 years 5 months  Languages : Python, R, Mysql  Tools: Pycharm, tableau, Jupyter, spyder, MS Excel  Project Description          Global data is a data analytics and consulting company having customers around the globe into multiple lines of businesses such as telecom, banking, healthcare etc.    Actively involved in predictive modeling projects in banking and telecom domain customers and building various regression and classification models using the most popular algorithms available and couple of clustering problems for the same.    And helped the customers by finding the valuable insights from the huge data with extensive Exploratory Data Analysis and summary statistics and also profiling of consumer data to help decision making.    Involved in couple of POCs into telecom and retail banking customers.    Built a chatbot which acts as virtual agent to help the employees of the customer in training and making them learning job responsibilities through Q&A interface.                  Professional Skills:     Statistics: descriptive statistics - measures of central tendency & measures of dispersion, and inferential statistics, hypothesis testing, and statistical testing.   Exploratory data analysis: univariate, bi-variate and multivariate analysis, derived metrics.   Building predictive models using supervised and unsupervised machine learning algorithms: regression, classification with linear and ensembles (bagging & boosting) algorithms, clustering/segmentation, time series analysis and forecasting using AR, ARMA, and ARIMA techniques.   Algorithms: linear regression, ols, glm, logistic regression, knn, decision tree, random forests, adaboost, gradientboosting, and xgboost, svm, kmeans, hierarchical clustering, kmodes, and dbscan.   Deep learning: Neural Networks, RNNs, LSTM, GRUs, and CNNs.   Natural language processing and text analytics:  text cleaning/processing, bag of words & tf-idf model, text classification, clustering, text mining, pos-tagging, word embedding, n-grams, topic modeling, and sentiment analysis, web scraping.   Libraries used: scikit-learn, statsmodels, numpy, pandas, scipy, nltk, tensorflow, keras, spacy, gensim, opencv, re, pickle, flask, imblearn, tesseract, lime, yellowbrick and other python libraries.       HCL Technologies India               Page 3 of 4", "tokens": [{"text": "HCL Technologies", "start": 950, "end": 966, "token_start": 176, "token_end": 177, "entityLabel": "company"}, {"text": "Oct 28th 2020 - Till date", "start": 968, "end": 993, "token_start": 179, "token_end": 184, "entityLabel": "period"}, {"text": "Perint Info Systems", "start": 1352, "end": 1371, "token_start": 250, "token_end": 252, "entityLabel": "company"}, {"text": "1 Year", "start": 1384, "end": 1390, "token_start": 256, "token_end": 257, "entityLabel": "period"}, {"text": "Perint Info Systems", "start": 2019, "end": 2038, "token_start": 363, "token_end": 365, "entityLabel": "company"}, {"text": "1 Year", "start": 2051, "end": 2057, "token_start": 369, "token_end": 370, "entityLabel": "period"}, {"text": "Accenture Solutions", "start": 2736, "end": 2755, "token_start": 487, "token_end": 488, "entityLabel": "company"}, {"text": "5 months", "start": 2779, "end": 2787, "token_start": 494, "token_end": 495, "entityLabel": "period"}, {"text": "Eclerx Services", "start": 3265, "end": 3280, "token_start": 589, "token_end": 590, "entityLabel": "company"}, {"text": "7 months", "start": 3294, "end": 3302, "token_start": 595, "token_end": 596, "entityLabel": "period"}, {"text": "Grapple Info", "start": 4138, "end": 4150, "token_start": 748, "token_end": 749, "entityLabel": "company"}, {"text": "3 years 5 months", "start": 4173, "end": 4189, "token_start": 754, "token_end": 757, "entityLabel": "period"}], "relations": [{"child": 179, "head": 176, "relationLabel": "duration"}, {"child": 256, "head": 250, "relationLabel": "duration"}, {"child": 369, "head": 363, "relationLabel": "duration"}, {"child": 494, "head": 487, "relationLabel": "duration"}, {"child": 595, "head": 589, "relationLabel": "duration"}, {"child": 754, "head": 748, "relationLabel": "duration"}]}, {"document": "                                                    SIMRAN SETHI  Data Scientist and Developer     Phone: +49 (174) 304 3538 (Available on WhatsApp)     Email: simrannsethi@gmail.com  LinkedIn: linkedin.com/in/simran-sethi  Location: Berlin, Germany   Profile     Enthusiastic data science professional with research experience in deep learning.    More than 3+ years of experience working with data science related projects.    Winner of technology hackathons in data science and blockchain domain.    Completed Data Science and Machine Learning Courses from the University of British Columbia, Canada in June 2019.   Technical Skills      Programming  Languages:   Python (Numpy, Scipy, Scikit-learn, Pandas, Pytorch, Tensorflow, Fastai, Keras, OpenCV etc.), R (Tidyverse stack, Shiny,  RMarkdown, etc.), SQL, Scala, Spark, C++, CSS, XML, Bash    Key Concepts:       Data visualization (D3, Shiny), regression, classification, deep learning, Bayesian inference, Advanced Machine Learning,  Monte Carlo Simulation, Gradient Boosting, Feature Engineering, Ensemble methods, Forecasting, A/B Testing     Frameworks:     Git, AWS, Google Cloud Platform (GCP), Flask, Docker   Education   DATA SCIENCE  COURSES  (GRAD LEVEL) ,  University of British Columbia, Vancouver, Canada Sep 17 - June19    Relevant Coursework: Advanced Machine Learning, Algorithms and Data Structures, Computing Platforms for Data Science, Data   Visualization I & II, Databases and Data Retrieval, Descriptive Statistics and Probability for Data Science, Experimentation and Causal   Inference, Feature and Model Selection, Privacy, Ethics and Security, Regression I & II, Spatial and Temporal Models, Statistical   Inference and Computation I & II, Supervised and Unsupervised Learning, Web and Cloud Computing.    BACHELOR OF ENGINEERING, NSIT, University of Delhi, New Delhi, India July 12 - June16    Relevant Coursework: Operating Systems, Software Engineering    Specialization: Electronics and Communication Engineering    AWS Machine Learning Specialty Certified  Certification ID: 96CW2X31BEQQQGCF  Issuer: Amazon Web Services   April 19  April 22   Work Experience   DATA SCIENTIST    Peregrine Technologies, Berlin, Germany  Full-time Oct 19  Sep20   Company summary: Peregrine is a startup offering real time traffic video analytics to help make smarter decisions on the road.       Analyzed the telematics data obtained from phone sensors to provide analytics and driving insights to the customer.    Developed the anonymization pipeline for obfuscating faces and number plates from video data collected through the phones.    Developed algorithms to better predict ride start, end and triggered events.   Key Technologies: Python, AWS, Git, Tensorflow, Bash, Deep Learning, Docker   DATA SCIENCE  CAPSTONE   TRIUMF Lab, Vancouver, Canada  Short Term Project April 19  June 19   Company summary: TRIUMF lab is the particle accelerator center of Canada, affiliated with CERN Switzerland.       Developed a python package for particle identification of highly charged decaying particles - Muons, Pions, and Positrons in the   CERN NA62 experiment.    Improved accuracy of correct classification by a factor of 2 as compared to the currently deployed model. This will help physicists   analyze if the theory behind the decay matches the experimental physics results.   Key Technologies: Python, Git, Pytorch Framework, Snakemake   DATA SCIENCE  RESEARCH SCHOLAR   Asia Pacific Foundation of Canada, Canada  Part-time March 19 - June 19   Company summary: APFC is a think-tank analyzing Canadas relation with Asia Pacific region.   mailto:simrannsethi@gmail.com?subject=Let's%20connect! http://www.linkedin.com/in/simran-sethi http://aws.amazon.com/verification       Automated process of trade data extraction from statistics Canada website thereby reducing errors and saving number of hours in manual  extraction.    Performed unsupervised learning using dynamic topic modelling method on twitter data and analysed of tweets of the Peoples daily of china  for observing emerging economic trends and discussions in Peoples Republic of China.   Key Technologies: Python, Git, Latent Dirichlet Allocation  DEEP LEARNING RESEARCH ASSISTANT   Department of Forestry, UBC, Canada  Part-time Sep 18  April 19       Developed a notebook to automatically detect roads in the aerial TIFF imagery of Alberta, Canada using SegNet architecture and Pytorch  library.    Achieved 99% accuracy with respect to the ground truth imagery which led to the model reliably detect roads.   Key Technologies: Python, Pytorch Framework, Git, SegNet Architecture, ArcGIS    DATA SCIENTIST  -  Innovaccer Analytics, Noida, India  Full-time         June 16  Aug 17   Company summary: Innovaccer is a Silicon Valley-based healthcare data analytics firm building big-data solutions for U.S. healthcare.     Developed proof of concepts for data science project of potential clients using Spark SQL, python and R in teams of 2-3 members.   Built plugins for a demand and supply analytics dashboard. Used Javascript and Flask endpoints to display data to the end user.    Deployed analytics modules into the dashboard of hospitals such as Mercy ACO and Catalyst Healthcare Network which led to improved care  management by care providers.   Key Technologies: Python, R, JavaScript, React, Scala, Spark, XML, MongoDB, SQL    DATA SCIENCE  INTERN   Snapdeal, New Delhi, India  Short Term Project June 15  July 15  Company summary: Indias 3rd largest e-commerce company      Developed a prototype on shiny to score the sentiment expressed in tweets about Snapdeal and its competitors using TF-IDF and Bag of words.     Visualized the data through word cloud and time series graphs which was developed into an internal tool for the data science team.   Key Technologies: R, Rshiny, Sentiment Analysis   Technical Projects    WatsonNLU  R API wrapper package     Developed functions and tests for IBM Watson Natural Language Understanding API Wrapper  package in R.    RegscorePy and RegscoreR  Python and R  Packages   Developed python and R packages that do model comparison between different regression  models using AIC, BIC and mallows Cp score.    Recommendation Engine on Movie Lens  Dataset (Grouplens) and Amazon Product  Dataset (UCSD)   Applied KNearestNeighbors, Collaborative filtering and Neural Network techniques to build a  recommendation engine.     Awards and Achievements    Scholarship Recipient (award amount 5000$) March 19    WiDS Kaggle Datathon  (Highest scorer in Vancouver  awarded 150$ prize by Unbounce Inc.) Feb 19    Verif-IDBlockathon at UBC (Adjudged winner and awarded 2500$ cash prize)     May 18    McKinsey Online Hackathon (ROC score 0.718, winners ROC score was 0.729 Top 6%) July 18    Online Sex-Work Risk Predictor  Hack for Humanity hackathon (Adjudged winner and awarded 1000$ cash prize)     Feb 18    CityFlux  Vanquish Collisions Hackathon by City of Vancouver (Adjudged 2nd place winner and awarded 2000$ cash prize) Sep 18   Publication   Maintaining accurate, current, rural road network data: An extraction and updating routine using RapidEye, participatory GIS and deep  learning.   Sean P. Kearney, Nicholas C. Coops, Simran Sethi, Gordon B. Stenhouse    International Journal of Applied Earth Observation and Geoinformation  Volume 87, May 2020, 102031       Profile  Technical Skills  Education  Work Experience  Technical Projects  Awards and Achievements  Publication  ", "tokens": [{"text": "Peregrine Technologies", "start": 2168, "end": 2190, "token_start": 399, "token_end": 400, "entityLabel": "company"}, {"text": "Oct 19  Sep20", "start": 2219, "end": 2232, "token_start": 409, "token_end": 412, "entityLabel": "period"}, {"text": "TRIUMF Lab", "start": 2795, "end": 2805, "token_start": 512, "token_end": 513, "entityLabel": "company"}, {"text": "April 19  June 19", "start": 2845, "end": 2862, "token_start": 522, "token_end": 526, "entityLabel": "period"}, {"text": "Innovaccer Analytics", "start": 4651, "end": 4671, "token_start": 828, "token_end": 829, "entityLabel": "company"}, {"text": "June 16  Aug 17", "start": 4705, "end": 4720, "token_start": 839, "token_end": 843, "entityLabel": "period"}, {"text": "Snapdeal", "start": 5398, "end": 5406, "token_start": 974, "token_end": 974, "entityLabel": "company"}, {"text": "June 15  July 15", "start": 5445, "end": 5461, "token_start": 984, "token_end": 988, "entityLabel": "period"}, {"text": "Sep 18  April 19", "start": 4269, "end": 4285, "token_start": 756, "token_end": 760, "entityLabel": "period"}, {"text": "Department of Forestry", "start": 4222, "end": 4244, "token_start": 745, "token_end": 747, "entityLabel": "company"}, {"text": "Asia Pacific Foundation of Canada", "start": 3447, "end": 3480, "token_start": 629, "token_end": 633, "entityLabel": "company"}, {"text": "March 19 - June 19", "start": 3500, "end": 3518, "token_start": 640, "token_end": 644, "entityLabel": "period"}], "relations": [{"child": 409, "head": 399, "relationLabel": "duration"}, {"child": 522, "head": 512, "relationLabel": "duration"}, {"child": 756, "head": 745, "relationLabel": "duration"}, {"child": 640, "head": 629, "relationLabel": "duration"}, {"child": 839, "head": 828, "relationLabel": "duration"}, {"child": 984, "head": 974, "relationLabel": "duration"}]}, {"document": "Ganapathy Govindan (51897727)  ERS MMT  Mobile: +91-9940323911  Email: GANAPATHY.GOVINDAN@HCL.COM Passport No: Z462263    Skills:  AWS, Azure and GCP, DevOps (Terraform, Docker and Kubernetes)  Bigdata (ETL, Hadoop, Spark and Machine learning), TOGAF & PMP.    Ganapathy Govindan (51897727)  ERS MMT  Mobile: +91-9940323911  Email: GANAPATHY.GOVINDAN@HCL.COM Passport No: Z462263    Skills:  AWS, Azure and GCP, DevOps (Terraform, Docker and Kubernetes)  Bigdata (ETL, Hadoop, Spark and Machine learning), TOGAF & PMP.     Key Impact Areas  Soft Skills          DevOps Automation  Kubernetes Administration  Containerization & CI/CD  Cloud Migration & Serverless  Big Data Analysis  Solution Design & Architecture  Cyber Security   Development & Deployment  Team Building & Leadership           Communicator      Collaborator      Intuitive      Innovator      Leader      Motivator      Analytical      Team Builder                   Executive Profile    A goal-oriented professional offering nearly 14 years of experience in in all phases of SDLC   Technology experience entails DevOps, Cloud, Big Data Hadoop and Data Science.  Migrated SOA to Micro Services Architectures of existing application through Kubernetes Cluster deployment (EKS & AKS).  Successfully launched high volume data lake, interpreted data & founded machine learning models  Implemented Data Science algorithms with existing production system, automated with DevOps frameworks   Administered big data lifecycle from business strategies, requirements data design, data sourcing/transformation, analytics, development, & implementation  Contributed in architectural design planning, implemented technology initiatives & organizational integrations  Successfully worked in both managerial and architectural aspects, with proficiency in continuously evolving technology, communicating data findings to support organizations meet business challenges   Proven excellence in automating build & deploying activities of applications on various environments using various tools  Team-based Management style with expertise in driving the companys vision into reality.              Technical Skills    Docker, Kubernetes, Terraform  Hadoop Data Lake (Petabyte Data), on-Premises & Cloud   Cloudera/Hortonworks cluster, Cluster setup on Cloud   Micro-Services, DevOps, Data Pipeline, Code Pipeline   Linux, Python, Scala, Java, Spark, Spark Streaming   Cloud Migration, AWS DevOps, Azure & GCP  Big data Hadoop, Hive, Sqoop, Pig, Kafka, Oozie   Jenkins, Redshift, Athena, Glue, EMR, Big Query  HBase, Cassandra, MongoDB, EC2, RDS, S3   Cyber Security, Pen Testing,      Certifications    CKA: Certified Kubernetes Administrator  AWS Certified Solutions Architect - Professional   AWS Certified DevOps Engineer  Professional  TOGAF 9 certification for Enterprise Architects  Project Management Professional (PMP)  Microsoft Azure Architect Technologies-AZ-300  Cloudera Certified Developer for Apache Hadoop   Certified Talend Open Studio for DI Consultant  Certified SAFe 4 Agilest & Exin Agile Scrum Master  Red Hat Certified Engineer            Preparing for CISSP & OSCP  IT Skills   Terraform, Kubernetes (on-premises), Elastic Kubernetes Services, Azure Kubernetes Service  Big Data Systems: Cloudera / Hortonworks Cluster, HDFS, Map Reduce, Hadoop. Hive, Pig, Flume, Sqoop, Scala, Kafka, Spark, Flink.   Cloud:  AWS, Azure & Google Cloud, Cloud Migration, Serverless, AWS Data lake, Glue, Athena  Analytics & ETL: Machine Learning, R, Python, Scala, Spark, Snowflake, Talend, AWS ML, SAS, Redshift  No SQL & RDBMS: HBase, Cassandra and Mango DB, Oracle, BD2, MS-SQL, My SQL  DevOps:  Jenkins, Docker, Docker Swarm, Open shift, Ansible,  Operating Systems: RedHat Linux6, MS DOS, Windows NT/XP/2000, UNIX  Software Languages: UNIX, Unix Shell Scripting, Java, PERL, and TCL  Code Compile: Jira, Remedy, Maven, Ant, SBT, GIT and Gitlab              Career Timeline  June   2006  To  March 2010  Satyam Computer Services (Mahindra Satyam) -  Sep 2006 to Sep 2007. Deployed at Optus Telecommunication. Sydney   April 2010  To  Oct       2014  Cognizant Technology Services   Feb    2015   To  Dec      2015  HCL Technologies  June 2016  To  Dec       2017  Verizon Data Services  April 2018  To  April    2019  Syntel   July   2019  To  March 2020  Societe Gnrale  Oct    2020  To  Till Now  HCL Technologies                      Professional Experience    Common Key Result Areas:   Working as Technical Manager in Roche  Redcap Healthcare Application project.  Acting as Enterprise Integration Architect for DAT and ARS group @ SocGen.  Worked as Enterprise Integration Architect where responsible to migrate compliance applications to cloud  AWS and Azure.   Working on Data lake for various regulatory applications towards one platform and open-source migration principles  Worked as Developed & maintained architectures for various business functional areas; resolved integration and interface issues between various applications or systems with focus on optimizing application performance and scalability.   Ensuring the successful delivery of emerging big data solutions; simulating, designing, developing & deploying computationally complex and practical data; building and delivering comprehensive data strategy roadmaps; ensuring final deliverables are of the highest quality.  Analyzing & reviewing business, functional and high-level technical requirements; designing detailed technical components for complex applications utilizing high-level architecture, design patterns and reusable code  Managing end-to-end of projects from stage of initiation till monitoring & control and closure including planning, estimation & scheduling and contingency planning; implementing projects within present deadlines                Oct 2020  to Till Now with HCL Technologies as Technical Manager    Technologies: AWS, Terraform, Java, Python, DevOps, AWS Code Pipeline, AWS Code Commit, Docker, HPCluster.  Roles & Responsibilities:  Worked on Roche  Casino  Redcap Health care Data analytics platform and application created by Terraform on AWS Cloud. Involved complete Terraform code creation, testing, implementing and automated deployment.              Jul19 to March 20 with Societie Generale (Global Solution Center), Bangalore as Enterprise Integration Architect    Technologies: Data lake, Data pipeline, Spark, Scala, Talend, Architecture, DevOps, Open shift, Automation with Terraform Roles & Responsibilities:  Architecting based on one platform strategy to SocGen regulatory applications. Integrating the compliance applications towards cloud adoption and tuning the existing Hadoop spark jobs. Moving from conventional SOA deployment to Micro service based container deployment on Open shift.       Apr18 - Apr19 with Syntel International, Chennai as Senior Consultant  Technologies: AWS, Azure & Google Cloud Stacks - AWS Data lake, Migrating exiting on-premises Data lake into AWS Data lake with AWS, Glue, Athena, Redshift, Redshift Spectrum and EMR (Spark PySpark). Setup dedicated VPC, Route 53, API gateway and Server less Analytics. GCP DataProc, Big Query and Big Table, Automation with Terraform  Roels & Responsibilities: updating the State Street Big data platform into Data lake. Updating the analytics layer to storage + reporting layer.        Jun16 - Dec17 with Verizon Data Services as Big Data and Data Science Consultant    Project: Verizon Supply Chain Domain - (100% onsite facing product project)  Role:  Consultant (Big Data Hadoop and Data Science)  Technologies: Hadoop Data lake, Micro Services, Docker Container Services, Machine Learning, R, Hadoop, Python, Hive, Sqoop, Oozie, Kerberos, Hortonworks, Spark, AWS Cloud, AWS DevOps. Automated CD/CI in AWS  Responsibilities:   Launched Hadoop Data Lake, Data Cleaning, Finding insight on Data, Come up with Machine learning algorithms, Implement and automate the models in real time applications. Trained / mentored supply chain team, other DevOps teams. Architected Bid Sales Application which run by Micro services and Deploys in Docker Container Services      Feb15 - Dec15 with HCL Technologies Limited, Chennai as Big Data Hadoop Architect    Project: ABB Raise Fabric by ABB   Role:   Big Data Hadoop Architect   Technology: Hive, Sqoop, HBase, Cloudera Impala, Cloudera Manager, Kafka, Spark, Python, Azure, Sentry, Cloudera Navigator, Kerberos Security, Spark Streaming       Apr 2010  Oct 2014 with Cognizant Technology Solutions, Chennai as Senior Associate (Projects)    Project: Cognizant - Capital One   Role: DevOps, ETL and Hadoop Analyst  Technology:  Hive, Pig, HBase, Hadoop Cluster, Data Explorations, Data Cleaning, ETL Jobs, Hadoop Cluster Planning, Linux, Java, Hudson, Sonar, Git, Talend Job, Sqoop Job and Weblogic11g  Description:   Migration of existing j2ee huge data centric application to Hadoop, Setup up Hadoop 1 standalone cluster, Update to Yarn from Hadoop1, Data Injection through Sqoop from DB2, Writing the data into Hive table, Connecting the processed data to Tableau for visualization       Project: Cognizant - S & P  Reporting Applications and S & P Ratings Gateway  Technology:  Informatica Power Center, SQL, Oracle, Linux, Vignette Application Portal, Adobe CQ, 24/7 Production support, WebLogic, Linux Scripting       June 2006  March 2010 with Satyam Computer Services, Chennai as Consultant 1  Project:    McGraw-Hill  Standard and Poors - Rating Gateway Production Support   Nissan Kaizen Support & Satyam Computer Services - Vignette Service Offering  Technology: Vignette Application Portal VAP 7.4, Web Services, Java MVC Spring framework, Java, Mercury Quality Director, Oracle 9i, SCO Unix, Windows    Project: Corporate & Web Systems of Optus. Sydney.  Australia  Role: Onsite Coordinator & Level 3 Support Engineer    Team Size: 12  Technology: Vignette Content Suite 7.3.0.1, Vignette Application Portal 7.3, Vignette Dynamic Module, Vignette Story Server 4.2, Java, TCL, Oracle 8, WebLogic 8, SCO-Unix, Windows       Education & Credentials    2021: Pursuing Cyber Security Master Certificate program by HackerU.  2017: Machine Learning Nano Degree from Udacity   2017: Post Graduate Program in Business Analytics from Great Lakes Institute of Management, Chennai   2012: Executive MBA, University 21 Global and IGNOU, Singapore & New Delhi   1999: Professional Diploma in Network Centered Computing (DNIIT) from NIIT, Madurai   1987: Diploma in Textile Technology from SSMITT. Kumaranayakam", "tokens": [{"text": "Satyam Computer Services", "start": 3960, "end": 3984, "token_start": 754, "token_end": 756, "entityLabel": "company"}, {"text": "June   2006  To  March 2010", "start": 3931, "end": 3958, "token_start": 745, "token_end": 752, "entityLabel": "period"}, {"text": "Optus Telecommunication", "start": 4040, "end": 4063, "token_start": 771, "token_end": 772, "entityLabel": "company"}, {"text": "Sep 2006 to Sep 2007", "start": 4006, "end": 4026, "token_start": 763, "token_end": 767, "entityLabel": "period"}, {"text": "Cognizant Technology Services", "start": 4106, "end": 4135, "token_start": 785, "token_end": 787, "entityLabel": "company"}, {"text": "April 2010  To  Oct       2014", "start": 4074, "end": 4104, "token_start": 776, "token_end": 783, "entityLabel": "period"}, {"text": "HCL Technologies", "start": 4171, "end": 4187, "token_start": 799, "token_end": 800, "entityLabel": "company"}, {"text": "Feb    2015   To  Dec      2015", "start": 4138, "end": 4169, "token_start": 789, "token_end": 797, "entityLabel": "period"}, {"text": "Verizon Data Services", "start": 4220, "end": 4241, "token_start": 811, "token_end": 813, "entityLabel": "company"}, {"text": "June 2016  To  Dec       2017", "start": 4189, "end": 4218, "token_start": 802, "token_end": 809, "entityLabel": "period"}, {"text": "Syntel", "start": 4274, "end": 4280, "token_start": 824, "token_end": 824, "entityLabel": "company"}, {"text": "April 2018  To  April    2019", "start": 4243, "end": 4272, "token_start": 815, "token_end": 822, "entityLabel": "period"}, {"text": "Societe Gnrale", "start": 4312, "end": 4326, "token_start": 835, "token_end": 836, "entityLabel": "company"}, {"text": "July   2019  To  March 2020", "start": 4283, "end": 4310, "token_start": 826, "token_end": 833, "entityLabel": "period"}, {"text": "HCL Technologies", "start": 4355, "end": 4371, "token_start": 847, "token_end": 848, "entityLabel": "company"}, {"text": "Oct    2020  To  Till Now", "start": 4328, "end": 4353, "token_start": 838, "token_end": 845, "entityLabel": "period"}], "relations": [{"child": 745, "head": 754, "relationLabel": "duration"}, {"child": 763, "head": 771, "relationLabel": "duration"}, {"child": 776, "head": 785, "relationLabel": "duration"}, {"child": 789, "head": 799, "relationLabel": "duration"}, {"child": 802, "head": 811, "relationLabel": "duration"}, {"child": 815, "head": 824, "relationLabel": "duration"}, {"child": 826, "head": 835, "relationLabel": "duration"}, {"child": 838, "head": 847, "relationLabel": "duration"}]}, {"document": "Name: Gaurav Midha [Gaurav]  Email: gmidha246@gmail.com  Mobile: +1 860 994 9054(US number)    Professional Summary:  Over 11+ years of experience in IT industry with major focus on Automation and Deployment in Amazon Web Services (AWS) using devops tools through Continuous Integration (CI) & Continuous Deployment/Delivery (CD).  Leading the automation and engineering teams since 2011.   Experienced in big data technologies and event driven architectures. Created multiple Datalake architecture using Databricks, EMR & S3.  Designed and Deployed multiple machine learning models in PROD using Sagemaker, Lambda and API Gateways.  Lead the AWS automation and cloud infrastructure for ADPs Data lake and Data Science department.  Created company level internal terraform module repository.  Build the shared ECS/Fargate for common containerized services.  Designed and Built the automation to generate IAM roles and policies for federated Athena access into Data lake.  Developed CI/CD pipeline for managing common infrastructure across many AWS accounts.  Designed and built terraform module for managing Datalake security via AWS glue, IAM, and s3 policies.  Automate the deployment and configuration of MLFlow.  Developed organization level standards for infrastructure as code.  Implemented spot fleets for EMR automation across development environments to reduce costs.  Designed and implemented CI/CD security automation using IAM and cross account controls.  Developed self-serve automation for EMR clusters using cloudformation and service catalog  Automated security compliance remediation and preventative IAM policies.  Created CICD pipelines for deploying ECS applications, cdk based apps and spark notebooks through Jenkins and CircleCI.  Expert in doing cost cutting by archiving and purging indices from Elasticsearch. Reduced aws cost by 120,000 $ per year.  Hands on experience on AWS cloud service like Compute, Network, Storage, Lambda, Kinesis and Identity & access management.  Wrote Ansible playbooks to manage web configuration files and packages.   Automated and deployed Puppet manifests and modules, the modules for configuring EC2 instances.  Created deployment strategies and the coordinated architecture changes for rapidly evolving projects.  Designed highly available and cost-effective systems using multiple EC2 instances, Auto Scaling, ALB, Elastic Load Balance and AMIs.  Design and build out CI/CD Infrastructure with Jenkins on ECS/Fargate and Terraform.  Deployed applications using zdt and blue green approach.   Extensive experience in developing and maintaining build, deployment scripts for test, Staging and Production environments using Jenkinsfile.  Used container orchestration tools like ECS or Kubernetes for doing container deployments.   Used Python for doing automation.  Extensive experience in administration of Build and Deployment tools: Jenkins, CircleCI, AntHillpro and CruiseControl.  Experience in Release Management, support and environment troubleshooting.  Experience in creating environments, setting up configuration and deployment as a Weblogic admin for 3 years.  Designed cloud solutions for customers leveraging services including VPC, EC2, S3, RDS, Route53, Step functions, SageMaker, Batch and Cloud Formation.  Quick learner, highly motivated team player with excellent organizational and analytical skills.  Performed application monitoring using Prometheus, Dynatrace, Datadog & New Relic.  Provided environment support to testing teams for environment show stopper defects and resolved issues quickly.  Provided development support to developers by helping them resolve build issues quickly.   Technical Skills  Cloud Platform: Amazon AWS, Google Cloud.  Container Orchestration Tools: Kubernetes and ECS.  DevOps Tools: Puppet, Ansible and Docker.  CI/CD Tools: Jenkinsfile, CircleCI.  Infrastructure as code: Terraform.  Monitoring Tools: Dynatrace, New Relic and Datadog.  Build/Deploy Tools: Jenkins, Anthillpro and CruiseControl.  Web/App servers: Apache Tomcat and Weblogic.  Database: SQL, PostgreSQL and MongoDB(basic knowledge)  Scripting Languages: Shell and Python  Operating System: Unix, Linux and Windows  Education  Bachelor of Technology (Computer Science:2009) from ITM University, Gurgaon.  Certification  AWS Solution Architect-Associate   Puppet Certified Professional  Certified Kubernetes Administrator    Work Experience     ADP (Nov 2019 till Now)  Role: Solution Architect/Senior AWS Automation Engineer  Environment:   AWS EMR, Glue, Step functions, Sagemaker, ECS, ALB, Target Groups, VPC, S3, IAM, CloudWatch, Prometheus & CICD using Jenkinsfile.   Responsibilities/Work:  Lead DS Arch team for cloud automation and machine learning deployments.  Setup containerized jenkins on Linux in ECS which deploys to different AWS accounts by assuming role in that account.  Integrated Jenkins with Active Directory for authentication and authorization purposes.  Worked with Gradle to setup the deployment of cdk apps in AWS.  Deployed python code as cdk app in AWS, which in turn deployed etl & ml pipelines.  Setup docker build pipelines for various ml images.  Have done cache setup for Gradle project to reduce the build time phase from 7 mins to 50 secs.  Have worked on creating shell and python scripts for removing underutilized resources and unwanted resources.  Have setup serverless architecture for EMR optimizer which terminates the EMR if no one is using it.  Worked on setting up Prometheus & Grafanafor optimizing the performanceof EMR clusters.  Did troubleshooting on different aws cloud services such as api gateway, Lambda, EMR, ML pipelines & ECS.  Worked on cost optimization for EMR.  Did the proof of concept on managed scaling feature of EMR and promoted it to be used in PROD.  setup Amundsen, a data discovery tool open source by Lyft and loaded non prod data from Glue into it.  Have setup the ES & Kibana in containers in ECS cluster.  Worked on setting up the federated IAM roles in AWS.  Created architecture diagrams for many ETL & ML pipelines.          Mcgraw Hill Education (Sep 2017 till Nov 2019)  Role: Senior Site Reliability Engineer  Environment:   AWS ECS, ALB, Target Groups, Auto Scaling, VPC, S3, IAM, CloudWatch, New Relic, Datadog Puppet, Ansible & CICD using Jenkinsfile.   Responsibilities/Work:  Move the applications to Containerized applications architecture on Ec2 Linux machines.  Lead SREs to automate the manual tasks.  Use Jenkinsfile to implement CICD strategy for deployments.  Created many AWS architecture diagrams for application implementations and reviewed those with security team to ensure that architecture is robust and secure. For example: cross account S3 access to EC2 through IAM role, cross account access for kinesis streams to EC2 through IAM role, public and private ECS container-based applications etc.  Wrote reusable terraform modules for creating application and its components. Implemented ECS Cluster with Auto Scaling group for various environments.  Deployed applications as containers in pods on Kubernetes clusters.  Administered and managed Kubernetes cluster on prem.  Evaluated GCP for migrating the Kubernetes workload to cloud.  Handled Spark Notebooks deployment to AWS Databricks using Jenkins with python scripts.  Created Spark Clusters in AWS Databricks using automation.  Created Jenkins Job for managing clusters in AWS Databricks.  Wrote Python script for creating SSM Parameters out of a text file, which contains secret name andits value.  Wrote python script for removing the unused available EBS volumes from Linux machines in AWS and created a Lambda function out of it using terraform.  Implemented cloudwatch monitoring, alerting and took necessary actions for improving the process efficiency.  Implemented APM (for Nodejs and Scala based apps) and Infra monitoring for EC2 Linux and PostgreSQL using New Relic and Datadog.  Reduced AWS Cost by 120k/year by shutting down older instances that were not being used.  Participated in Reserved Instances Initiative to save cost for the infrastructure.  Received monetary award for implementing CICD pipeline and driving towards automation.      Cognizant Company (April 2017 till Sep 2017)  Role: Senior AWS Devops Engineer  Environment:   AWS EC2, ELB, Auto Scaling, VPC, S3, Terraform, IAM, CloudWatch, GitLab, Jenkins & Puppet.  Responsibilities/Work:  Make application more scalable and highly available by migrating it to AWS.  Deploy applications as containers in AWS.  Lead cloud engineers for building reliable and scalable automation.  Manage and configure AWS services as per the business needs.  Coordinated with project architect and development teams for creating infrastructure.  Implement configuration management using puppet for deployments and config changes on Linux machines.  Implemented cloudwatch monitoring, alerting and took necessary actions for improving the process efficiency.  Implement APM (for Java apps) and Infra monitoring using New Relic for Linux OS.  Designed CICD pipelines using jenkins and terraform.  Integrated Jenkins with Active Directory for authentication and authorization purposes.     Accenture Company (Dec 2009 till March 2017)  Role: Worked as a Lead Devops engineer in build, deployment and release management of Client and server applications. Experienced in Banking & Insurance domains. Also worked on Cloud migration esp. in Amazon Web Services.  Environment:    EC2 Linux, S3, CloudFront, EBS, SQS, SNS, ELB, VPC and CloudWatch, Jenkins, Weblogic, AntHillPro and CruiseControl.  Responsibilities/Work:   Created many AWS architectures for migration of the applications to Cloud.   Deployed various AWS services using terraform.   Worked on build and deployment of Java and .Net applications through various build tools such as Jenkins, Anthill pro and CruiseControl tools on windows and Linux machines.   Used AD with different tools for authentication and authorization.   Worked on different App/Webservers: Tomcat, IIS & Weblogic server.   Created branches on SVN and Git for development and deployments.   Created Shell scripting: Cron jobs and log processing scripts.   Worked on continuous integration and deployment to Test environments.    Provided production release support.   Worked as a coordinator in between Onshore and Offshore Build Teams.   Resolved application and environment defects troubleshooting which impacted the application development and testing.   Set up new environments, created sandboxes for developers and deployed new applications using Lab Manager.   Worked as a Release manager for almost 7 Years. Worked on following things during that time period: Release plan, deployment to Prod servers and support during production environment issues.   Used Dynatrace tool agents for Application performance monitoring.     Worked on Environment management and scheduling.   Organized daily Devops meeting and Release Planning meetings.   Automated basic Smoke Testing for deployment verification.   Document creation: how to create Snapshot of VMs, Environment questionnaire, Release plan, Environment schedule, Release tracker, build calendar, Environment status template, Creation of MSI packages, how to create change requests, Blue prints etc.", "tokens": [{"text": "ADP", "start": 4424, "end": 4427, "token_start": 769, "token_end": 769, "entityLabel": "company"}, {"text": "Nov 2019 till Now", "start": 4429, "end": 4446, "token_start": 771, "token_end": 774, "entityLabel": "period"}, {"text": "Mcgraw Hill Education", "start": 6083, "end": 6104, "token_start": 1096, "token_end": 1098, "entityLabel": "company"}, {"text": "Sep 2017 till Nov 2019", "start": 6106, "end": 6128, "token_start": 1100, "token_end": 1104, "entityLabel": "period"}, {"text": "Cognizant Company", "start": 8195, "end": 8212, "token_start": 1468, "token_end": 1469, "entityLabel": "company"}, {"text": "April 2017 till Sep 2017", "start": 8214, "end": 8238, "token_start": 1471, "token_end": 1475, "entityLabel": "period"}, {"text": "Accenture Company", "start": 9185, "end": 9202, "token_start": 1645, "token_end": 1646, "entityLabel": "company"}, {"text": "Dec 2009 till March 2017", "start": 9204, "end": 9228, "token_start": 1648, "token_end": 1652, "entityLabel": "period"}], "relations": [{"child": 771, "head": 769, "relationLabel": "duration"}, {"child": 1100, "head": 1096, "relationLabel": "duration"}, {"child": 1471, "head": 1468, "relationLabel": "duration"}, {"child": 1648, "head": 1645, "relationLabel": "duration"}]}, {"document": "                                                GAYATRI KAKARLA  kakarla.g@husky.neu.edu  |(857)-268-9477| https://www.linkedin.com/in/gayatrikakarla/      TECHNICAL SKILLS:   Alteryx, Tableau, MySQL, JMP, Qlikview, Microsoft Project, GanttProject, Visio, SharePoint, Jira, Minitab, Excel, PowerPoint     PROFESSIONAL EXPERIENCE:   Closeloop Technologies LLC, Palo Alto, California                                                                                                                Mar 2019- present  Business Analyst    Coordinated with cross functional teams and acted as a liaison between business leaders and technical teams.    Oversaw and orchestrated the design, development, testing and project management teams for a brand-new product.    Created wireframes by gathering technical requirements and compiled project documentations/deliverables for the product.    Participated in defining test objectives and performed Manual Testing with a thorough knowledge in Functionality Testing,  Integration Testing, System Testing, Compatibility Testing, Regression Testing, Accessibility Testing, Usability Testing.    Designed test plans and worked with the development team to track and correct bugs using JIRA.    Performed mobile app & web testing for various platforms including Android and iOS.    Monitored project milestones, helped progress the product to development phase- improved the delivery performance by 25%.     Kareergraph Technologies, New Orleans, Louisiana                                                                                                          Jan 2018- Mar 2019  Business Analyst     Communicated KPIs to internal business stakeholders to track performance and business on a weekly basis.    Interacted with Development Engineers and helped develop strategies for testing products and services.    Extensively worked throughout defect life cycle from opening to closure using JIRA.     Designed internal procedures for data consistency and cleanliness thus promoting efficient analysis for clients using Alteryx.     Integra Technologies, Houston, Texas                                                                                                                                   Mar 2017- Jan 2018  Project Management/Business Analyst Intern     Analyzed financial data and generated visualizations, insights and recommendations using Tableau.     Maintained and updated database with the goal of enhancing efficiency and consistency using MySQL.    Facilitated on-going analysis and weekly metrics to understand customer requirements using surveys and behavioral data.     Bristol Myers Squibb, Devens, MA                                                                                                                                         Jan 2016-May 2016  Biologics Knowledge Management Co-op                                                                                                                                                             Enhanced efficiency by assisting the formulation of corrective and preventive actions.    Restructured technical memos, statistical adverse event tracking and dashboards for the SPC team across 4 countries.      Improved quality and reduced process variations by creating datasets, diagnostic plots and control charts using JMP.    Achieved process optimization by streamlining data extraction and evaluating CPV metrics for 5 pharmaceutical products.    Identified process improvement opportunities by monitoring adverse events using Discoverant for the Quality Control Team.     Hindustan Petroleum Corporation Limited, Visakhapatnam, India                                                                                 Dec 2012-Jan 2013   Industrial Analysis Extern                                                                                                                                                                                   Assessed Risks by identifying hazards, mentored trainees on risks pertaining to the extraction of crude oil by- products.    Updated process documentation to improve productivity and efficiency by almost 25% and maintain standards.     EDUCATION:       Northeastern University, Boston, MA                                                                                                                                    Sep 2014-Dec 2016  M.S, Engineering Management        Andhra University, Visakhapatnam, India                                                                                                                            Sep 2010-Apr 2014        B.Tech, Chemical Engineering               TECHNICAL PROJECTS:                                                                                                                                                                         Wholesale Food Products Analytics dashboard project                                                                                                    Feb 2018-Mar 2018   Analyzed YOY growth by presenting statistics related to order placements segregated by employee and customer.   Designed dashboards to observe actionable insights on Customers, Suppliers, Products, and Employees using Tableau.    Extracted raw data from Northwind database using SQL queries by restructuring data, applying joins, aggregations, sub queries.      Catalog Demand Prediction project                                                                                                                                       Feb 2018-Mar 2018   Predicted 30% increase in profit margin from the distribution of catalogs for 250 new customers using chronological data.   Selected predictor and target variable using scatter plots, p value, R-squared value from the report generated using Alteryx.   Deduced a linear regression equation by using historical data and applied it for 250 new customers.       Commercial impacts of autonomous drones                                                                                                                        Sep 2015-Dec 2015                                                            Published white paper using market research and reliable sources and industry reports on drone technology.    Analyzed R&D, cost of development, commercialization strategies and impacts on other markets in white paper.          White paper link - https://www.memsic.com/userfiles/files/SensorsMakeDrones%204-6-16.pdf   https://www.linkedin.com/in/gayatrikakarla/ https://www.memsic.com/userfiles/files/SensorsMakeDrones%204-6-16.pdf  ", "tokens": [{"text": "Closeloop Technologies LLC", "start": 332, "end": 358, "token_start": 46, "token_end": 48, "entityLabel": "company"}, {"text": "Mar 2019- present", "start": 493, "end": 510, "token_start": 55, "token_end": 57, "entityLabel": "period"}, {"text": "Kareergraph Technologies", "start": 1442, "end": 1466, "token_start": 208, "token_end": 209, "entityLabel": "company"}, {"text": "Jan 2018- Mar 2019", "start": 1596, "end": 1614, "token_start": 216, "token_end": 219, "entityLabel": "period"}, {"text": "Integra Technologies", "start": 2070, "end": 2090, "token_start": 288, "token_end": 289, "entityLabel": "company"}, {"text": "Mar 2017- Jan 2018", "start": 2237, "end": 2255, "token_start": 295, "token_end": 298, "entityLabel": "period"}, {"text": "Bristol Myers Squibb", "start": 2632, "end": 2652, "token_start": 356, "token_end": 358, "entityLabel": "company"}, {"text": "Jan 2016-May 2016", "start": 2801, "end": 2818, "token_start": 364, "token_end": 366, "entityLabel": "period"}, {"text": "Hindustan Petroleum Corporation Limited", "start": 3594, "end": 3633, "token_start": 460, "token_end": 463, "entityLabel": "company"}, {"text": "Dec 2012-Jan 2013", "start": 3736, "end": 3753, "token_start": 469, "token_end": 471, "entityLabel": "period"}], "relations": [{"child": 55, "head": 46, "relationLabel": "duration"}, {"child": 216, "head": 208, "relationLabel": "duration"}, {"child": 295, "head": 288, "relationLabel": "duration"}, {"child": 364, "head": 356, "relationLabel": "duration"}, {"child": 469, "head": 460, "relationLabel": "duration"}]}, {"document": "                                             Govind K Choudhary        GOVIND K CHOUDHARY  c.govind@gmail.com                                                                                                                             +91-9910031387     CAREER SUMMARY     17 years of experience in BI, Analytics and Management Consulting across domains   including CPG, Retail, BFSI, Telecom, etc. Deployed multiple technologies including SAS,   SPSS, R, Python, Big Data, etc. to build predictive models and derive business insights. Led   largest analytical engagement at Mindtree, spanning 20+ countries, for a leading global CPG   manufacturer. Instrumental in setting-up advanced analytics practice for platform solutions team   at TCS. A go-to man for critical and strategic assignments, worked directly with the client teams   of Fortune 500 companies and led large projects from both onsite - US, UK and APAC - and   offshore environments. Recently engaged with startups in BI, Analytics and educational fields.     Holds a B.Tech. from IIT BHU and an MBA from XIM-B.        PROFESSIONAL EXPERIENCE     XLMiND                               (Dec 2017  Present)  A startup to provide quality education to young minds     Founder & Educator        Establish an institute of excellence to help students develop interest in science stream  and realize their potential             Provide an inquiry and application based learning experience to inculcate analytical and critical thinking    Facilitate an engaging and stimulating learning environment by mentoring and training faculty members    Liaison and collaborate with schools to generate curiosity and develop interest towards STEM education     Mindtree Ltd.                                       (Jun 2016  Jun 2017)  A leading provider of IT services and business solutions     Program Director, Analytics Projects                                    Lead a large analytics engagement, spanning across 20+ countries, to support product assortment planning and   optimization for a leading global CPG manufacturer     It generated outlet level sales recommendation by deploying statistical techniques such as linear/logistic regression,   collaborative filter, cluster analysis, etc. based on multiple platforms including Hadoop, Spark, R, Python, etc.     Identify SKUs and its optimal quantity across outlet to enhance on-shelf availability and reduce stock-out    Identify SKUs with high cross-sell/up-sell potential at outlet-product level to increase product penetration     Integrate new data sources (external) to capture demographic, lifestyle and other consumer level attributes     Tata Consultancy Services (TCS)                                  (Oct 2008  Jan 2015)  A leading provider of IT services, outsourcing and business solutions     Program Manager, Enterprise Solutions (Nov 2012  Jan 2015)   Lead strategic BI/Analytics/Big Data projects (presales and delivery) by deploying SAS, R, Cognos, Hadoop, etc.     Pricing Analytics for a large US retailer to determine price position, price gap, elasticity, risk items, etc.     Energy Analytics for a S Africas retailer based on  kW/kVA, billing cost, power factor, HDD, CDD, etc.    Program management (onsite) to support and enhance SAS Estate deployment for a UK energy provider     Head, Advanced Analytics (KPO) Platform (Oct 2008  Oct 2012)  Set up the Advanced Analytics practice, define key offerings and drive client engagement activities - presales,   sales and project delivery - across multiple platforms including SAS, SPSS, R and OBIEE    Response model for Pay per View campaigns for a large Indian Direct to Home service provider    Market Basket Analysis for a Canadian retailer based on Support, Confidence, Lift and incremental value     Program management (onsite) for a US Auto client to develop IT dashboard to analyze/track key metrices      Symphony Marketing Solutions (Genpact LLC)                      (Aug 2007  Sep 2008)  A leading provider of Advanced Analytics, BI and Data Management/ Integration     Manager, Business Insights  Information Resources, Inc. (IRI)   mailto:c.govind@gmail.com        Lead project delivery and support business development efforts for strategic US CPG/Retail clients by deploying   suitable analytical framework using IRI Scanner and Panel data    Brand Decline Analysis based on key parameters like sales, distribution, promotion, pricing, NPL, etc.    Promotion Efficiency Analysis based on lift for different vehicles  Features, Display, F&D and TPR    New Launch Analysis to access impact of distribution, shelf-space, velocity, pack size, pricing, promo, etc.     Inductis India Pvt. Ltd. (EXL Service)             (Mar 2006  Jul 2007)  US based Management Consulting and Data Analytics Firm     Manager, Analytics Services                                                       Lead analytics projects for global BFSI clients by deploying tools like SAS, SPSS, CART, MARS, etc. and support   cross-sell/up-sell opportunities    Risk/delinquency model for a US credit card issuer to calculate the propensity to default/charge-off    Channel optimization for a US credit card issuer by developing a suite of response and volume models    Customer retention strategy for a leading Indian bank through attrition model and segmentation analysis     GoRemote Internet Communication Ltd. (iPass)                                    (Dec 2004  Mar 2006)  US based broadband networks service provider        Manager, Product Marketing   Provide strategic inputs in the areas of telecom market analysis, product evaluation and domain excellence    TCO/ROI tool to quantify benefits of GoRemote broadband solution over existing dial-up connections    Analyze network access cost with ISPs (350+ ISPs; 150+ countries) to identify cost saving opportunities     Frost & Sullivan (I) Pvt. Ltd                                                                 (Apr 2004  Sep 2004)  US based Market Research and Consulting Firm                               Sr. Research Analyst   Create market intelligence report and provide consulting support to telecom service providers across APAC        Study key markets (onsite) to identify investment opportunities for an APAC telecom service provider    Prepare market reports (technical, financial and competitive analysis) for various telecom service providers     Bharat Heavy Electricals Ltd. (BHEL)                                                                         (Jul 1997  Jun 2002)  Largest engineering enterprise in India in power and infrastructure sector      Engineer  Plan and execute civil projects; develop and support system programs in COBOL/Oracle    Update the pay-roll system for Y2K compliance and system migration from COBOL to Oracle    Project incharge for installation of Computerized Attendance Recording System' based on Oracle database     EDUCATION     PGDM (2004)  XIM-B, Bhubaneswar; Major: Marketing; Grade: 5.64/8.00   B.Tech. (1997)  IIT BHU, Varanasi; Branch: Civil Eng.; Grade: 7.62/10.00   HSC: 73%, SSC: 81%     TRAINING & WORKSHOP     Sales/ Presales/ Program management: Key Account Management (KAM), Power Messaging (Elevator Pitch),   Advance Presentation Skills (APS), Building Human Capital (BHC), etc.   Business Advisor Capability (BAC): A flagship programs at TCS, spread over 6-8 months, to train and prepare   high performing senior associates for business consultative and advisory role     INTEREST & ACHIEVEMENT      CAT percentile - 98.7; Selected for doctoral program (Ph.D.) at IIM Ahmedabad    Business Advisory Capability certified; Faculty award for Advance Presentation Skill    Exceptional performer at GoRemote for developing ROI tool in collaboration with sales team    Among top performer at BHEL- secured movement from Civil dept. to EDP (Computer Science)  ________________________________________________________________________________________________________________________     c.govind@gmail.com                                                                                                                             +91-9910031387  CAREER SUMMARY  PROFESSIONAL EXPERIENCE   Frost & Sullivan (I) Pvt. Ltd                                                                 (Apr 2004  Sep 2004)  EDUCATION   ", "tokens": [{"text": "XLMiND", "start": 1111, "end": 1117, "token_start": 179, "token_end": 179, "entityLabel": "company"}, {"text": "Dec 2017  Present", "start": 1149, "end": 1166, "token_start": 182, "token_end": 185, "entityLabel": "period"}, {"text": "STEM education     Mindtree Ltd.", "start": 1686, "end": 1718, "token_start": 262, "token_end": 266, "entityLabel": "company"}, {"text": "Jun 2016  Jun 2017", "start": 1758, "end": 1776, "token_start": 269, "token_end": 273, "entityLabel": "period"}, {"text": "Tata Consultancy Services (TCS)", "start": 2652, "end": 2683, "token_start": 421, "token_end": 426, "entityLabel": "company"}, {"text": "Oct 2008  Jan 2015", "start": 2718, "end": 2736, "token_start": 429, "token_end": 433, "entityLabel": "period"}, {"text": "Symphony Marketing Solutions", "start": 3892, "end": 3920, "token_start": 664, "token_end": 666, "entityLabel": "company"}, {"text": "Aug 2007  Sep 2008", "start": 3957, "end": 3975, "token_start": 673, "token_end": 677, "entityLabel": "period"}, {"text": "Inductis India Pvt. Ltd.", "start": 4658, "end": 4682, "token_start": 800, "token_end": 804, "entityLabel": "company"}, {"text": "Mar 2006  Jul 2007", "start": 4710, "end": 4728, "token_start": 811, "token_end": 815, "entityLabel": "period"}, {"text": "GoRemote Internet Communication Ltd.", "start": 5338, "end": 5374, "token_start": 919, "token_end": 922, "entityLabel": "company"}, {"text": "Dec 2004  Mar 2006", "start": 5419, "end": 5437, "token_start": 928, "token_end": 932, "entityLabel": "period"}, {"text": "Frost & Sullivan (I) Pvt. Ltd", "start": 5841, "end": 5870, "token_start": 1003, "token_end": 1011, "entityLabel": "company"}, {"text": "Apr 2004  Sep 2004", "start": 5936, "end": 5954, "token_start": 1014, "token_end": 1018, "entityLabel": "period"}, {"text": "Bharat Heavy Electricals Ltd. (BHEL)", "start": 6385, "end": 6421, "token_start": 1083, "token_end": 1089, "entityLabel": "company"}, {"text": "Jul 1997  Jun 2002", "start": 6495, "end": 6513, "token_start": 1092, "token_end": 1096, "entityLabel": "period"}], "relations": [{"child": 182, "head": 179, "relationLabel": "duration"}, {"child": 269, "head": 262, "relationLabel": "duration"}, {"child": 429, "head": 421, "relationLabel": "duration"}, {"child": 673, "head": 664, "relationLabel": "duration"}, {"child": 811, "head": 800, "relationLabel": "duration"}, {"child": 928, "head": 919, "relationLabel": "duration"}, {"child": 1014, "head": 1003, "relationLabel": "duration"}, {"child": 1092, "head": 1083, "relationLabel": "duration"}]}, {"document": "Harish Kumar Sharma       : 9999911550   : mailtohksharma@gmail.com              EXPERIENCE PROFILE     14+ Years of experience and currently working with IBM India as an API & Microservices architect since April 2010 to till date.     PROPRIETARY SKILLS      IBM API Connect 5.x, v2018, Apigee API Management   Cloud including IBM Bluemix, RedHat OpenShift, TD Private Cloud, Google Cloud   IBM WebSphere products, including WebSphere DataPower XI52, IBM WebSphere DataPower Cast Iron Cloud Appliance XH40, WebSphere Application Server v8.0,v7.0,v6.1, IBM APP Connect, IBM WebSphere Service Registry and Repository, IBM Integration Bus v9,v10,WebSphere Extreme Scale   J2EE Frameworks including Spring, Vertx   REST API with NodeJS , SpringBoot, Quarkus   NodeJS, StrongLoop scripting Framework   MongoDB, Casandra, MySQL   Version Control tools including GIT, SVN   DevOps tools including Jenkins, Nexus, Jira, Confluence, Stash.      AWARDS  AWARDS        TD Partner award in 2015  IBM Spark Award for contribution toward Amedisys Clients Success (2013)  Orion Award by IBM for recognition of Eminence and Excellence 2012  Export Blue Technical Excellence for 2012.  Export Blue Technical Excellence for 2011.  Export Blue Excellence in Project for 2010        CERTIFICATION & TRAINING  CERTIFICATION & TRAINING     IBM Certified Solution Implementer WebSphere DataPower SOA Appliances Firmware V5.0.  IBM Certified Solution Designer-Object Oriented Analysis and Design, vUML 2  IBM Certified SOA Solution Designer.  IBM Certified SOA Associate.  IBM Certified Solution Developer-Web Services Development for WebSphere Application Server V7.0,6.1.    ACADEMIC PROFILE        TECHNICAL SKILLS  TECHNICAL SKILLS     EAI Tools    :      IBM API Connect, IBM APP Connect     Apigee API Management.          IBM WebSphere DataPower Integration Appliance XI50   IBM WebSphere DataPower Cast Iron Cloud Appliance XH40          IBM API Connect, IBM App Connect     IBM Integration Bus v9, v10        IBM API Management v2,v3        IBM WebSphere Service Registry and Repository.  Database      : MongoDb, Casandra,MySql, Oracle 11g.  Languages    : NodeJs, StrongLoop, Java, JSP, Servlet, Struts, Hibernate, Spring,             Server    : WebSphere Application Server 5.1/6.1/7.0,8.0,JBoss Server 6.0, Tomcat                                                     Server.  Cloud                                : IBM BlueMix, RedHat OpenShift, TD Private Cloud   Other tools       : Apache CXF, JAXB, Castor, Mail API,ROME, Quartz, Putty, JCaptcha.  DevOps     :  Jenkins,GIT,Jira, Kubernetes, Docker     PROFESSIONAL ROLES AND RESPONSIBILITIES  PROFESSIONAL ROLES AND RESPONSIBILITIES      Experience in Scope, planning, tracking, change control for the project.  Experience in preparing the technical steps for the changes & review the implementation steps prepared by the team. ERTATIONS  Experience in requirement analysis.  Responsible for design review & code review prepared by team.  Responsible for effective communication between the team members and myself.   Provide day to day direction to the team members and regular project status to Project Manager.  Create POC in the project for provide better solution for the problem.  Involved in Security Audit and security implementation according to the project standards.  Handling priority issues & escalations.  Conducting technical sessions, reviewing the change plans and sharing the knowledge to team to build a strong team.  Responsible for Build completion & deployment code on production.              PROJECT PROFILE  PROJECT PROFILE    Amex, Phoenix  Duration           :   Aug 2019  Till Date  Position            :   Cloud Architect  Client                :   Amex   Project  :   One Data   Company          :   IBM India Pvt. Ltd      Client Introduction:  Working as a Cloud Architect with Amex and helping Amex one-data platform team to deploy their services on OpenShift and enable function team to deploy there functions on one-data on Kubernetes and manage the functions deployment and resources.    Tools Used- OpenShift, HA proxy, ETCD, Vertx, Jenkins, Groovy, FaaS    Standard Bank, South Africa  Duration           :   Aug 2018  2019  Position            :   API Architect  Client                :   SB   Project  :   Digital MarketPlace   Company          :   IBM India Pvt. Ltd      Client Introduction:  Worked on OpenBanking implementation for SB as an integration architect to implement PISP and AISP APIs. My responsibility was to complete end to end implementation of OpenBanking using IBM API Connect platform. We are using API Connect developer toolkit to develop APIs and using DevOps tools to avoid manual intervention in process to publish the APIs on API Manager.    Tools Used- IBM API Connect, SpringBoot, Jenkins, Strongloop, MongoDB.    Bank Of Cyprus, Cyprus  Duration           :   Aug 2017  2018  Position            :   Integration Architect  Client                :   BOC, Cyprus   Project  :   PSD2 , OpenBanking   Company          :   IBM India Pvt. Ltd      Client Introduction:  Setup a Digital Factory to implement API and Microservice and Complience Bank on PSD2 and OpenBanking Standards. I had worked as an integration architect to architect, design and implement  end to end solution for PISP and AISP APIs and Microservices . My responsibility in here is complete end to end implementation of PSD2 using IBM API Connect platform and Microservice implementation in Spring Boot. We are using API Connect developer toolkit to develop APIs and using DevOps tools to avoid manual intervention in process to publish the APIs on API Manager.    Tools Used- IBM API Connect, Spring Boot, Jenkins, Strongloop, MongoDB.          IBM Digital Integration Practice  Duration           :   Aug 2016  2017  Position            :   Integration Architect  Client                :   IBM   Project  :   PSD2 , OpenBanking   Company          :   IBM India Pvt. Ltd      Client Introduction:  Working on PSD2 implementation for IBM Digital Integration practice as an integration architect to implement PISP and AISP APIs. My responsibility in here is complete end to end implementation of PSD using IBM API Connect platform. We are using API Connect developer toolkit to develop APIs and using DevOps tools to avoid manual intervention in process to publish the APIs on API Manager.    Tools Used- IBM API Connect 5.x, Jenkins, Strongloop, MongoDB.      TD CORE BANKING    Duration           :   Nov 2015  July 2016  Position            :   Integration Architect  Client                :   TD Canada Trust  Project  :   Core Banking API Development   Company          :   IBM India Pvt. Ltd      Client Introduction:   TD is in adoption new technology to fit with the technology and market changes and they are converting their Legacy Core Banking Web Service to the APIs. To Develop APIs we are using Strongloop NodeJS Scripting framework and Jersey framework for Java. All developed API or adapter being deployed to TD private Cloud.      Technology Used- IBM API Management, StrongLoop, NodeJS,RestEasy    TD SAS SSDI    Duration           :   Jan 2015  Oct 2015  Position            :   Lead Designer  Client                :   TD Canada Trust  Company          :   IBM India Private Ltd    Client Introduction:    In SSDI we are responsible to develop and design new web service and API as well as support to enhance existing service. After of these months I had worked on couple of services belonging to different line of Business. I had worked- Credit Adjudication, Mortgage Insurance, Credit Protection, FATCA, Credit Report, CSL. ISNGeneration, COMS, LOAN etc   Technology Used- DataPower,WSRR,Java,Maven,Jenkins     Intact Payment Processing    Duration           :   Sep 2014  Dec 2015  Position            :   Integration Architect  Client                :   American Express (USA)  Company          :   IBM India Private Ltd    Client Introduction:    API,s Need to be built to Onboard Intact Buyers, and Suppliers and for Payment processing on BIP SaaS system. I was working as an Integration architect in the project to help the team with new Technology used.      Technology Used  IBM Cast Iron, Apigee API Management,Jenkins,Java RestAPI.          BIP SaaS    Duration           :   Apr 2010  Aug 2014  Position            :   Advisory IT Architect  Client                :   American Express (USA)  Company          :   IBM India Private Ltd    Client Introduction:    BIP SaaS (Buyer Initiated Payment-Software as a Services) is a consolidate hub where domestic & international Trading Partners(Buyer, Supplier And American Express) can set up,view,review and store information related to EIPS business they conduct with their respective customers. This is basically future scope project related to Secure Transition between Buyer and Supplier through P-Card, ACH & IWire.These are special virtual card introduced by AMEX provides only to corporate customer and there is no cash limit for this card. In this application only registered in BIP database can avail the P-Card, ACH & IWire .Here Buyer can invites different Suppliers for enrollment, only after successful enrollment of supplier transaction can take place.      Design description: The BIP application will use a j2ee web application to perform the enrollment, payment management, hierarchy maintenance and user provisioning function required. A suite of j2ee batch processing utility will be created and reused to perform the physical processing of payment files authorization and settlement and payment confirmation.    Globester    Duration           :   Jun 2009  Apr 2010  Position            :   Software Developer  Client                :   Globester(USA)  Company          :   Tripster Solution Pvt. Ltd.    Client Introduction:   Globester is a service provider which provides Online Air Travel reservation system to the passenger from USA to outside the countries and inside USA travel. This website have a search engine at the home page to the user when user come to globester web site .Working process of Globester is, this site provide three type of travel searches One Way, Round Trip,MultiCity.When User Come to the site and provide required information like Origin City, Destination City, Departure date, Returning date & search Type  & click on search button then we communicate with third party(Amadeus) & send this query to Amadeus to get the price & availability of the flights & show result to the use on the result page. When user clicks on a flight by which he want travel at that moments we send another queries for reconfirm the availability & check the price. If user selected flight is available then send him to the purchase page otherwise send back to result page for select another flight segment. At purchase page we get required information from the user & send other queries to generate the PNR.If everything process fine then user reservation will successfully done.    BIP SaaS:    Duration           :   Aug 2008  Apr 2009  Position            :   Engineer  Client                :   American Express  Company          :   IBM India Private Ltd on the Payroll of Collabera    Client Introduction:   BIP SaaS (Buyer Initiated Payment-Software as a Services) is a consolidate hub where domestic & international Trading Partners(Buyer, Supplier And American Express) can set up,view,review and store information related to EIPS business they conduct with their respective customers. This is basically future scope project related to Secure Transition between Buyer and Supplier through P-Card.  P-Card is a special virtual card introduced by AMEX provides only to corporate customer and there is no cash limit for this card. In this application only registered in BIP database can avail the P-Card. Here Buyer can invites different Suppliers for enrolment, only after successful enrollment of supplier transaction can take place.    Fareterminal :    Duration           :   Aug 2007  Aug 2008  Position            :   Software Engineer  Company          :   Tripster India Pvt. Ltd.    Project Details:    Fareterminal is a USA website which provide air travel reservation from USA to outside the World for Online & Offline agents.Fareterminal is introduced by Globester for that type of travel agents which dont have GDS(Global Distribution System) access to provide the travel solution. By using fareterminal they can easily made the booking for their customers. Agents only require to register at fareterminal & approval from admin after that they are able to made the online booking & they can charge their own margin on the customers. After successful booking agent get a confirmation mail & send mail to customers.            CRMS :    Duration           :   Oct 2005  July 2007  Position            :   Software Engineer  Company          :   Sigma Info Solutions.      Project Details:   CRMS provide all up-to-date information of all the computer resource available in the company. These projects help to overcome the difficulties to tracking the resource manually by providing customized resource. CRMS help to effectively & timely utilization the records.      Main objective of this project is:-    Provide an Automated System    Provide Multi-user & user friendly Interface.    Provide Maximum Securities Data.    Provide different type of user interface depend on type of user log on.            6/6", "tokens": [{"text": "IBM India Pvt. Ltd", "start": 3775, "end": 3793, "token_start": 654, "token_end": 658, "entityLabel": "company"}, {"text": "Aug 2019  Till Date", "start": 3635, "end": 3654, "token_start": 624, "token_end": 628, "entityLabel": "period"}, {"text": "IBM India Pvt. Ltd", "start": 4334, "end": 4352, "token_start": 764, "token_end": 768, "entityLabel": "company"}, {"text": "Aug 2018  2019", "start": 4192, "end": 4206, "token_start": 735, "token_end": 738, "entityLabel": "period"}, {"text": "IBM India Pvt. Ltd", "start": 5032, "end": 5050, "token_start": 894, "token_end": 898, "entityLabel": "company"}, {"text": "Aug 2017  2018", "start": 4874, "end": 4888, "token_start": 862, "token_end": 865, "entityLabel": "period"}, {"text": "IBM India Pvt. Ltd", "start": 5931, "end": 5949, "token_start": 1056, "token_end": 1060, "entityLabel": "company"}, {"text": "Aug 2016  2017", "start": 5781, "end": 5795, "token_start": 1026, "token_end": 1029, "entityLabel": "period"}, {"text": "IBM India Pvt. Ltd", "start": 6656, "end": 6674, "token_start": 1189, "token_end": 1193, "entityLabel": "company"}, {"text": "Nov 2015  July 2016", "start": 6480, "end": 6499, "token_start": 1155, "token_end": 1159, "entityLabel": "period"}, {"text": "IBM India Private Ltd", "start": 7254, "end": 7275, "token_start": 1301, "token_end": 1304, "entityLabel": "company"}, {"text": "Jan 2015  Oct 2015", "start": 7131, "end": 7149, "token_start": 1276, "token_end": 1280, "entityLabel": "period"}, {"text": "IBM India Private Ltd", "start": 7900, "end": 7921, "token_start": 1424, "token_end": 1427, "entityLabel": "company"}, {"text": "Sep 2014  Dec 2015", "start": 7762, "end": 7780, "token_start": 1397, "token_end": 1401, "entityLabel": "period"}, {"text": "IBM India Private Ltd", "start": 8422, "end": 8443, "token_start": 1528, "token_end": 1531, "entityLabel": "company"}, {"text": "Apr 2010  Aug 2014", "start": 8284, "end": 8302, "token_start": 1500, "token_end": 1504, "entityLabel": "period"}, {"text": "Tripster Solution Pvt. Ltd.", "start": 9750, "end": 9777, "token_start": 1757, "token_end": 1761, "entityLabel": "company"}, {"text": "Jun 2009  Apr 2010", "start": 9623, "end": 9641, "token_start": 1733, "token_end": 1737, "entityLabel": "period"}, {"text": "IBM India Private Ltd", "start": 11124, "end": 11145, "token_start": 2011, "token_end": 2014, "entityLabel": "company"}, {"text": "Aug 2008  Apr 2009", "start": 11005, "end": 11023, "token_start": 1988, "token_end": 1992, "entityLabel": "period"}, {"text": "Tripster India Pvt. Ltd", "start": 12056, "end": 12079, "token_start": 2180, "token_end": 2185, "entityLabel": "company"}, {"text": "Aug 2007  Aug 2008", "start": 11972, "end": 11990, "token_start": 2163, "token_end": 2167, "entityLabel": "period"}, {"text": "Sigma Info Solutions", "start": 12848, "end": 12868, "token_start": 2322, "token_end": 2324, "entityLabel": "company"}, {"text": "Oct 2005  July 2007", "start": 12763, "end": 12782, "token_start": 2305, "token_end": 2309, "entityLabel": "period"}], "relations": [{"child": 624, "head": 654, "relationLabel": "duration"}, {"child": 735, "head": 764, "relationLabel": "duration"}, {"child": 862, "head": 894, "relationLabel": "duration"}, {"child": 1026, "head": 1056, "relationLabel": "duration"}, {"child": 1155, "head": 1189, "relationLabel": "duration"}, {"child": 1276, "head": 1301, "relationLabel": "duration"}, {"child": 1397, "head": 1424, "relationLabel": "duration"}, {"child": 1500, "head": 1528, "relationLabel": "duration"}, {"child": 1733, "head": 1757, "relationLabel": "duration"}, {"child": 1988, "head": 2011, "relationLabel": "duration"}, {"child": 2163, "head": 2180, "relationLabel": "duration"}, {"child": 2305, "head": 2322, "relationLabel": "duration"}]}, {"document": "Rupali Karn  Philadelphia, PA | (217) 381- 4918  karnrupali@gmail.com    SUMMARY           Experienced and vivacious Business System Analyst with over 5 years of experience consulting, evaluating and improving business systems for Higher Education, Ecommerce and Entertainment domains.   Expertise with ServiceNow implementation, Salesforce Sales Cloud and Salesforce.comSandboxproductionenvironments.   Strong visual modeling and business process modeling skills with tools like MS Visio, Lucid Charts, Smartsheet and well experienced in developing Use Case Models, Context Diagrams and UML CASE tools, screen mockups.   Knowledge of formulating SQL queries, creating reports in Microsoft Excel (VLOOKUPs, pivot tables, macros), Google Sheets and providing data visualizations in Tableau    SKILLS            Computer Skills: Microsoft Office, Microsoft Access, Microsoft Excel, Microsoft Project, SharePoint, Visio.    Technical Skills: JIRA, GitHub, PERL, Bugzilla, PHP, Oracle 11g, Microsoft Dynamics (DAx), Salesforce.Com    Data Analysis: Data Migration, Visualization, Data Mapping, Tableau, SAS E-Miner, SQL, MySQL.    Testing Skills: Unit Testing, Regression Testing, Smoke Testing, User Acceptance Testing    Analytical and Management Skills: Business Analysis, SDLC, Agile SCRUM , Project Management    WORK EXPERIENCE           BUSINESS ANALYST    08/2019 - Present  Flexon Technologies Inc. - New York City, New York USA                 Project: Fox Corporation                       Managed Ad Sales- Pitch project expectations with team members and other stakeholders and develop relationships and analyze the changes in the user stories to support the development process and communicate changes in established expectations to the project team and business users. Worked as part of the Agile Scrum team to deliver comprehensive changes to our internal/external business.  Responsibilities:  Primarily developed and groomed Agile epics, use stories in Atlassian JIRA for in depth understanding of customer requirements, business processes, and application(s) functionality.   Facilitated requirements elicitation in Joint Application Development (JAD) sessions with TUBI and BTN streaming channels to on-board their respective Ad Sales process into Fox Corporations Pitch by communicating with various background and skills, documented Scope Definition, Functional Specifications, Use Case document , Software Requirements Specification (SRS) and created User Interface Mockups/Prototypes using Smartsheet, Lucid Charts and QUIP.  Mapped functional requirements into test cases and developed Agile user stories with acceptance criteria(s) for all applications and maintained product backlog, Agile user stories and tasks in JIRA.  Maintained project plans, task, defect and issues lists, led meetings to discuss all aspects of the projects execution, developed test scripts, executed UAT, participated in the QA process, and developed training materials and system documentation.  Used MS Visio to carry out business modeling, data modeling (ER Diagrams), Screen Prototype, Object modeling (use case) and wireframes to develop business architecture for rapid and controlled Application development.  Worked with various Salesforce.com objects like Accounts, Contacts, Leads, Opportunities, Reports and Dashboards.  Designed and documented about  Custom Tabs, Objects, PickLists, Dependent picklists, Validation Rules, Workflows and Triggers.  Deployed, configured and supported the prototype application for its demo /UAT for Linear & Digital business platforms, prepared a deck for UAT & gathered feedback and followed up with further updates on the prototype and related documents.      Project: Driscolls Inc.                 Performed requirements gathering, writing and managing highly complex requirements for ServiceNow and MDM implementation . Conducted the full SDLC lifecycle to implement ServiceNow Software and master data warehouse  including requirement analysis, design, coding, testing, and training activities. Monitored the migration of Zendesk with ServiceNow and developed ITSM/ITIL processes, roles, business procedures to support (ITSM) best practices.  Responsibilities:    Supported the execution of the Configuration Management (CMDB) process, ensure it remains consistent with the ITSM strategy, global process goals and ensure coordination with all other IT processes    Collaborated with Process Architect in creating the TO-BE state process flows for Driscolls Business groups using Microsoft Visio.   Acted as liaison between 217 ServiceNow ITIL users (USA, Europe, Mexico & China) and Driscolls Information Services in the analysis, design, testing for application development to verify optional operational performance that meet business needs.   Responsible for Data Extraction, Data Compilation, Data Analysis, Data Manipulation and Data Validation using SQL queries in a MS SQL Server 2005 environment.   Create technical design documentation for the data models, data flow control process, metadata management.   Extensively involved in the modeling and development of Reporting Data Warehousing System.   Led the definition of operational reports, decision support tools, KPIs, dashboards, and analytic models that provide enhanced insight and improve organizational decision-making.   Identified  correct data sets and variables, gathered and integrated data from disparate sources using SQL along with designing and maintaining data systems and databases.   Created Source-to-Target data Mapping document and ensured that data is mapped correctly across applications maintaining data quality using SQL.   Developed Tableau visualizations and dashboards using Tableau Desktop,    Prepared graphical depictions of Use Cases, Use Case Diagrams, State Diagrams, Activity Diagrams, Sequence Diagrams using Microsoft Visio, Lucid Charts.      BUSINESS SYSTEMS ANALYST 06/2018  06/2019   Auguste Escoffier School of Culinary Arts  Schaumburg, Illinois USA  Led as Subject Matter Expert (SME) for the Student Information System (CampusVue) and administered user accounts, groups and security settings. Conducted monthly stats and analysis for roadmap reports and dashboards used for SIPOCs, KPIs and metrics and communicated via cloud-based productivity systems such as SiSense & Trello.  Responsibilities:    Elicited business and technical requirements for application development and documented requirements as user development stories     Executed 3-4 SCRUM/sprint meetings and provided accomplishments during daily-standup meetings. Participated in Sprint Planning, Daily Stand-ups and Sprint Retrospectives.    Assisted the Product Owner and aligned Product Backlog in Trello to prioritize work within agile teams and maintain a backlog of future work for consideration/prioritization    Administered and monitored Salesforce Pardot integration with Student information systems to generate 51% successful leads (student enrollment) and move them quickly through the pipeline    Owned and drove backlog grooming and management, prioritize user stories, create acceptance criteria and drive testing and delivery    Used SQL queries to collect relevant information from the company's database.    Worked closely with Business Users to understand the Intended functionality of the legacy System which involves integration with SQL Server for Data Processing    Delivered Visio diagrams/wireframes (using swim lanes), process flows and Excel wire frames to 5-10 stakeholders to socialize the stakeholders on the development process.    Documented results of applied work processes and practical application of technical standards and provided Summary Reports results to management.    Collaborated with project managers and QA analysts to elicit quality attribute requirements for applications, identifying gaps and challenging assumptions based on an understanding of business operations and priorities   GRADUATE STUDENT ASSISTANT 08/2017  12/2017  Office of Human Resources at UIS  Springfield, Illinois USA  Managed reference information and account coding surrounding the student insurance waiver associated with Blue Cross Blue Shield insurance. Accessed banking reports and tracked all daily payments, throughout the University system and internally, through an internal SharePoint site database.  Responsibilities:    Engaged in data migration within the U of I system incorporating SQL, ensured data integrity and catered to prospective and enrolled students in student insurance plans as well as employee benefits.    Identified variances between expenses & costs to monitor budget variances and facilitated more effective planning and budgeting/forecasting of waivers.    Developed Tableau visualizations and dashboards using Tableau Desktop, Pie Charts, Bar Charts, Density Charts, Donut Charts; facilitated robust features in charts such as creating bar charts in tooltip    Developed and monitored KPIs for fruitful cost utilization and other fundamentals for all included states using Tableau interactive dashboard     BUSINESS ANALYST 05/2015  04/2016  Intelliswift Software Inc.  Mumbai India  Operated a team of developers and testers for open order management for new French retailer websites and improve on-time availability. Collaborated with cross-functional teams on Product Development from planning, requirement gathering, development, testing, deployment, and performance measurement.  Responsibilities:    Managed retailer sales data reporting process for 51 French retailers and ensure weekly and monthly reporting accuracy    Monitored allocation status of retailer websites (approx. 200) and delivery date compliance.    Extracted, Transformed and Loaded (ETL) data from French e-commerce websites like Spartoo, Zappos to display it on a shopping portal.    Created and administered 139 XML scripts and collected qualitative data from GitHub repository and provided statistically analyzed information to generate visually compelling & actionable reports.      BUSINESS ANALYST - INTERN  01/2015  04/2015   Labyrinth Cinematic Solutions Pvt. Ltd.  Mumbai India  Analyzed/ documented the impact of new functionalities with the HR team, for HR reporting needs and supported departments and HR communication initiatives.  Responsibilities:    Generated reports and data dashboards using Excel to understand project information and presented these reports in PowerPoint presentations to clients, Director and sponsors    Designed and updated website using HTML 4, HTML5, PHP, JavaScript and supported all backend IT systems.    EDUCATION               Master of Science in Management Information Systems (MIS) 08/2016 12/2017     University of Illinois at Springfield     Master of Computer Application (MCA) 07/2012 - 05/2015     University of Mumbai     Bachelor of Computer Application (BCA) 08/2009 - 04/2012     SNDT University", "tokens": [{"text": "06/2018  06/2019", "start": 5916, "end": 5932, "token_start": 978, "token_end": 980, "entityLabel": "period"}, {"text": "Auguste Escoffier School of Culinary Arts  Schaumburg", "start": 5935, "end": 5988, "token_start": 982, "token_end": 989, "entityLabel": "company"}, {"text": "Intelliswift Software Inc", "start": 9116, "end": 9141, "token_start": 1482, "token_end": 1485, "entityLabel": "company"}, {"text": "05/2015  04/2016", "start": 9098, "end": 9114, "token_start": 1478, "token_end": 1480, "entityLabel": "period"}, {"text": "UIS  Springfield", "start": 8027, "end": 8043, "token_start": 1312, "token_end": 1314, "entityLabel": "company"}, {"text": "08/2017  12/2017", "start": 7980, "end": 7996, "token_start": 1303, "token_end": 1305, "entityLabel": "period"}], "relations": [{"child": 978, "head": 982, "relationLabel": "duration"}, {"child": 1478, "head": 1482, "relationLabel": "duration"}, {"child": 1303, "head": 1312, "relationLabel": "duration"}]}, {"document": "SANTOSH KUMAR  Cell: +91-9910872433  email: santosh-kum@hcl.com    To grow the career by meeting client expectation using latest tools & technologies, also keeping myself updated.    Resume Summary   IT Professional having 11 Years of experience including 3 Years of Team handling.  Worked on Machine Learning, Deep-Learning, Tensor-Flow, Neural-Networks, Reinforcement-Learning, Artificial-Intelligence, Python.  Co-ordinated with the client.  Worked as per Agile Methodology.  Experience in Apache Spark, Scala, Map & Reduce.  Strong Understanding of Hadoop EcoSystem, HDFS, BigData.  Experience in YARN, Spark Core, Mesos.  Development Experience in Spark-Sql, Hive, HiveQL, Kafka, Spark-Streaming.  Exposure of RDD, DataFrame, DataSet.  Experience in RPA ( Robotics Process Automation ), UiPath, BluePrism.  Development of the Functional specifications, HLD (High level Design).  Exposure of project on Cloud Platform using Windows Azure.   Working experience of Version controlling repository tools like VSS 6.0, TFS and TFS-Cloud.  Experience in developing IVR (Interactive Voice Response) system.  Experience in developing VoIP (Voice over Internet Protocol) based billing applications.  Exp. in Requirement gathering, planning, designing, coding, unit-testing, deployment & maintenance.  Efficiently administered peers by accomplishing task allocation & deliverables for production server as well as accountable for interdepartmental communication, timeline analysis, task list management.   Received Excellence & Service dedication Award.                Technical Skills  AI Tools:  Machine Learning, Deep-Learning, Tensor-Flow, Neural-Networks, Python Reinforcement-Learning, Artificial-Intelligence  Big-Data Tools:  Spark, Scala, Spark-Sql, Hive, HiveQL, Kafka, RDD, Spark-Streaming  Technologies:  Hadoop, BigData, HDFS, Map & Reduce, YARN, Spark Core, Mesos  RPA-Tools:  UiPath (RPA), BluePrism (RPA), Robotics Process Automation  Platform:  Cloud, AZURE, Linux, Windows, IIS 5/6/7.5  Design Document:  Rational Rose (Class-Diagram Design), UML, HLD                Organisational Exposure                                    HCL Technologies Ltd.                   Senior Consultant                   Since Feb15                  Ebix Software India Pvt. Ltd.                   SS-2 (Senior System Analyst2)                   Jan14Jan15                  British Council on Orcapods Payroll                   Senior S/w Eng. (Also Team handling)                   Dec12Oct13                  Net4 India Ltd  Senior S/w Eng. (Also Team handling)                   Nov07Sep12                  Divergent Infosoft Technologies Pvt. Ltd.                   Software Engineer                   Nov06May07                    Projects    Ecom-Prod-TF-Image-NN   Client: HCL CoE Product  Tools: Machine Learning, AI, Tensor-Flow, Neural-Networks, Python, Deep-Learning, Spark  Synopsis: This project is a product for Ecommerce platform, where model recommends for the trendy fashion items based on image processing using Tensor-Flow tools and predicting through DNN-Classifier (Neural-Network algorithm) by doing deep learning for similar fashion items image. Before of model prediction, a lot more data-massage has been done through Spark, Map & Reduce for train-data & test-data. Also used the SKLearn toolkit of Python.       Ecom-Prod-Discrete-KNC   Client: HCL CoE Product  Tools: Machine Learning, AI, KNeighbors-Classifier, Spark, Python, Big-Data, Hadoop, HDFS  Synopsis: This project is a product for Ecommerce platform, where model recommends for all non-fashion items that can be classified based on their specifications. Here the features are not image based, rather its a discrete value. Predicting through K-Nearest-Neighbors-Classifier Algorithm. Before of model prediction, a lot more data-massage has been done through Spark, Map & Reduce for train-data & test-data. Also used the SKLearn toolkit of Python.       P&G_Resins_Price_Anomaly   Client: Procter & Gamble  Tools: Machine Learning, AI, Random-Forest, Spark, Python, Big-Data, Hadoop, HDFS  Synopsis: This project is to make Procter & Gamble business more efficient by detecting price anomaly of Resins. There are some influencing factors for Resins. Price history of those influencing factor kept stored in the system. Based on which, trends has been generated. News-item for Resins and the influencing-factor also kept stored in the system. Now, the past trends and the news-item combined together to design the model.      DTF (Data Testing Framework)   Client HCL D&A CoE Product  Tools Hive, Hadoop, Big-Data, HDFS  Synopsis This project is to validate the data as per their configuration for different constraints. Highly customizable on run-time basis. A complete framework to test the data for their consistency.       LogProcessor   Client: Ebix  Tools: Spark, Hadoop, Scala, Big-Data, HDFS, YARN, Map & Reduce  Synopsis: This project is to manage the Staging & Production Server of Infrastructure team. Basically, All application get deployed on the Production Server of infrastructure team. And their log data is high volume data, a type of Big-Data. That need to be filtered for specific purposes, And then, mapped to desired purpose and reduced it to meaningful analysis.       Reliance ITP Documents   Tools Spark-Streaming, Scala, Apache Kafka, Cassandra, Location API  Synopsis Reliance which is developing the SAAS based product for sending the campaign offers to store-customers on behalf of stores. These offers could be in the form of SMS, PUSH Message, and email. Each user will get different product offer according to the user behaviour/choices and other attributes.       Jarvis_Coupa   Client:   Procter & Gamble  Tools:   BluePrism, RPA (Robotics Process Automation)  Synopsis: This project is to implement a BOT of the automation process of end to end flow of PR (purchasing request) by user to Buyer with the bidding process among the suppliers.      PolarisCVO   Client Polaris Account (HCL)  Tools UiPath, RPA Technologies ( Robotics Process Automation )  Synopsis This project is to automate the process of Individual dealer or all dealers Claim Versus Ordered. To eliminate the error prone manual operation. Enhanced the system by automating approval process.      PolarisC2C   Client Polaris Account (HCL)  Tools UiPath, RPA Technologies ( Robotics Process Automation )  Synopsis This project is to automate the process of Polaris Dealers Case to Claim. To eliminate the error prone manual operation. Enhanced the system by automating approval process.      ATRCore   Client Merck Account (HCL)  Tools UiPath, RPA Technologies ( Robotics Process Automation )  Synopsis This project is to automate the process of reconciliation/adjustment of financial transaction. To eliminate the error prone manual operation. Enhanced the system by automating approval process.       MerckCFM   Client Merck Account (HCL)  Tools UiPath, RPA Technologies ( Robotics Process Automation )  Synopsis This project is to Automate the Inventory-Transfer-Notes entry. Automated the creation of Invoice based on Dealer-Order-notes, Automated the entire process of Breakages-Expired-NonMovable product, Automated the Dealers-Payment entries, Automated the Physician-Sample-Transfer-Notes entry.            Scholastics  MCA from IGNOU with 62.78 %  Advance Diploma in Computer Application from IGNOU with 62.33 %   Certificate in Computing from IGNOU with 82 %  Bachelor in Arts, Sociology ( Honours ) from PATNA UNIVERSITY with 57.25 %  10+2 Science ( Math ) from BIEC with 62.33 %                  Personal Dossier  Fathers Name:       Mr. Rameshwar Prasad  Date of Birth:    28th December, 1979  Sex:    Male   Marital Status:   Married   Nationality:   Indian   Languages Known:  English and Hindi  Permanent Address:   S/o Mr. Rameshwar Prasad, Kazi-bagh, Gulzar-bagh, Patna-800007  Present Addr.:   House # 8, Doddanagamangla, Electronic-City Phase-2, Bangalore - 560100   Contact  Number :  +91-9910872433  E-mail ID:    santosh-kum@hcl.com            Awards, Honours & Certificates  Excellence Award by Net4 India Ltd  Service Dedication Award by Net4 India Ltd  Speech Competition Award                       Certification on Information Management by British Council", "tokens": [{"text": "HCL Technologies Ltd", "start": 2138, "end": 2158, "token_start": 425, "token_end": 428, "entityLabel": "company"}, {"text": "Since Feb15", "start": 2214, "end": 2225, "token_start": 432, "token_end": 433, "entityLabel": "period"}, {"text": "Ebix Software India Pvt. Ltd", "start": 2243, "end": 2271, "token_start": 435, "token_end": 441, "entityLabel": "company"}, {"text": "Jan14Jan15", "start": 2339, "end": 2349, "token_start": 449, "token_end": 449, "entityLabel": "period"}, {"text": "British Council on Orcapods Payroll", "start": 2367, "end": 2402, "token_start": 451, "token_end": 455, "entityLabel": "company"}, {"text": "Dec12Oct13", "start": 2476, "end": 2486, "token_start": 469, "token_end": 469, "entityLabel": "period"}, {"text": "Net4 India Ltd", "start": 2504, "end": 2518, "token_start": 471, "token_end": 473, "entityLabel": "company"}, {"text": "Nov07Sep12", "start": 2575, "end": 2585, "token_start": 487, "token_end": 487, "entityLabel": "period"}, {"text": "Divergent Infosoft Technologies Pvt. Ltd.", "start": 2603, "end": 2644, "token_start": 489, "token_end": 494, "entityLabel": "company"}, {"text": "Nov06May07", "start": 2699, "end": 2709, "token_start": 499, "token_end": 499, "entityLabel": "period"}], "relations": [{"child": 432, "head": 425, "relationLabel": "duration"}, {"child": 449, "head": 435, "relationLabel": "duration"}, {"child": 469, "head": 451, "relationLabel": "duration"}, {"child": 487, "head": 471, "relationLabel": "duration"}, {"child": 499, "head": 489, "relationLabel": "duration"}]}, {"document": "Ch.V. Raghurama Sarma                                                                                                                       Raghuram.v@outlook.com                                                                              Azure Data Engineer Associate  +91-6303040205  www.linkedin.com/in/raghuram-sharma-6b8b2699     About me   8 years of IT experience in Development and Maintenance. 3 years of Experience in Azure Data Platform. Experience includes Data warehousing, performance monitoring & tuning, T-SQL, index optimization. Perform a variety of SQL Server tasks related to problem resolution and monitoring, administration, performance, security and corporate strategy.    Career Profile                                                       Synapse Development engineer with Mindtree Ltd (10th Jan 2018  till date)   Role: Data Engineer.  Professional 3 years of experience  in MSSQL Server 2008/2008 R2/2012 includes  Installing, Upgrading, Configuration, Managing Azure services, Backup and Recovery, Moving Databases, Security, Upgrades and Migrations, SQL Server Performance, patching activity, Monitoring and Troubleshooting, BI Support, Data acquisition and ETLs.  MS SQL DBA with Fidelis Corporate Solution ltd  (November 2016  till December 2017)   Role: Database Administrator  Professional 1year of experience in MSSQL Server 2008/2008 R2/2012 includes Production support, data processing, data loading, and deployments.   System Administrator with M/s. STEEL EXCHANGE INDIA LTD (March 2013 till November 2016)   Role: System Administrator  Troubleshooting ofWIFI,LANconnectivityand proxy related issues in differentclient Project. Working knowledge of office automation products and computer peripherals, like printers and scanners.    Professional Certifications                                                                    Certified Azure Fundamentals, Power Platform Fundamentals.  Certified Azure Data Engineer Associate.    Technical Skills  Languages  SQL, .Net, Python  Key Areas   TSQL, Azure SQL Data Warehouse , Azure Data factory, Azure Data bricks  Database  SQL Developer, Azure SQL DB, Azure Data Ware House    Jan 2018-Till date: Mindtree Ltd, Hyderabad   Employer: Mindtree Ltd  Type    : Maintenance, Development & Support  Tools    : Azure SQL DW, Azure Data bricks, Data factory, Azure Data Lake, Blob containers  Client    : Microsoft  Managing  all transition meeting with management and prepared the total transition plan for Azure EDW migration from existing SQL server 2012  Performance monitoring of Azure SQL DW and optimizing user queries as per requirement  Implementing ETL process using Polly base , AZcopy , BCP   Creating external sources and tables for AZURE DW.  Deploying SSIS projects as per the new requirement.  Scheduling the SSIS jobs Azure Pipelines and bugs fixing in production level.  Worked on data ingestion from various data sources and data types.  Handled Troubleshooting issues with Azure Data Lake & Azure Blob integrated with SQL DW.  Authoring Pipelines in Data Factory for data Migration from On-Premises to Cloud.  Experienced in creating Power Bi reports, Power Automate Flows, Power Apps for different Data sources like SQL Server, SQL DW, and Share Point.  Strong Knowledge in SQL Server 2012 with MSBI (SSIS, SSRS, SSAS)  Good Knowledge of .Net, SQL and NoSQL databases, Power shell  Trained in Azure Synapse Workspaces (New Feature in Azure Synapse Analytics).  Handling a Team to provide deliverables to client.  Published Power BI Reports in the required originations and Made Power BI Dashboards available in Web clients and mobile apps.  Nov 2016 Dec2018: Fidelis Corporate Solutions Pvt. Ltd., Bangalore as a Database Administrator  Managing the Database queries & handling severity issues in Production Live.  Involved in the KT session to the new resources about functionality and high level architecture.  If there are any issues, try to resolve it in given SLA, if not possible escalate it well before time.  Direct Communication with client for severe issues and technical documentation.  Status reporting to onsite team on behalf of entire team.  Space Management and Capacity Planning, Database Backup and Recovery, Creation of index clustered/non clustered, Index Fragmentation, Rebuild or reorganize indexes, Managing and updating Statistics).        March 2013-November 2016: Power Division M/s. STEEL EXCHANGE INDIA LTD, Sreerampuram, Vizianagaram  3.8 years of experience as a System Administrator in Steel exchange India Ltd. (Vizag profiles group)  Roles and responsibilities:   Hands-on experience with Emerson Process Management studio.     Hands-on experience with Windows/Linux environments.     Extend computer support for system's software and hardware.     Hands on experience in Desktop related issues.     Troubleshooting ofWIFI,LANconnectivityand proxy related issues in differentclient    Project.      Working knowledge of office automation products and computer peripherals, like printers and scanners.    Academics:  Completed Graduation in B. Tech (ECE) from Raghu Engineering College in 2012.  Languages:   Telugu --Native language  English --fluent, Writing-Business Level   Hindi --Conversational Level    Declaration:     I hereby confirm that the information given above is true to best of my knowledge.     Date :20-04-2021                                                                                                                                                                       Signature    Place: Hyderabad                                                                                                                                                   Raghuram", "tokens": [{"text": "Mindtree Ltd", "start": 800, "end": 812, "token_start": 89, "token_end": 90, "entityLabel": "company"}, {"text": "10th Jan 2018  till date", "start": 814, "end": 838, "token_start": 92, "token_end": 97, "entityLabel": "period"}, {"text": "Fidelis Corporate Solution ltd", "start": 1212, "end": 1242, "token_start": 166, "token_end": 169, "entityLabel": "company"}, {"text": "November 2016  till December 2017", "start": 1245, "end": 1278, "token_start": 172, "token_end": 177, "entityLabel": "period"}, {"text": "M/s. STEEL EXCHANGE INDIA LTD", "start": 1484, "end": 1513, "token_start": 211, "token_end": 218, "entityLabel": "company"}, {"text": "March 2013 till November 2016", "start": 1515, "end": 1544, "token_start": 220, "token_end": 224, "entityLabel": "period"}], "relations": [{"child": 220, "head": 211, "relationLabel": "duration"}, {"child": 172, "head": 166, "relationLabel": "duration"}, {"child": 92, "head": 89, "relationLabel": "duration"}]}, {"document": "                                               Name      Big Data Analytics/Data Science Professional      Sateesh Rai, M.Sc. (Statistics), MCA   Mobile: +91-8527934114; Email: skrai75@gmail.com   Executive Summary  18-year experienced Big Data Analytics/Data Science Certified Consultant/Architect and Leader, leveraging Big data,   Visualization, AI/ML(With Patents) skills in empowering organizations analytics capabilities. Experienced in   BFSI/Retail/CPG/Telecom/Manufacturing/Pharma/Healthcare domains with execution expertise in Agile and   Waterfall methodologies with global rollouts (On Premise and On Cloud).   Proficiency/Skills/Responsibilities    Experienced in leading/Architecting and consulting Data Science/Big Data Analytics projects with strong  hands on exp. in Presales, RFI/RFP, Solution Design and Architecture.    Created Sales Collaterals, product specs, feature/benefit sheets. Strategic Alliance with sub-contractors for a  cost-effective solution.      Managed relationships with customers. Created statements of work (SOWs) for new projects.     Strong experience into Analytics strategy & capability and capacity building including leading and managing  POCs for customers.     Developed and executed Business Transformation and Transition Plans, IT Strategic Plans, Information  Management Strategies, and Digital Strategies based on Enterprise Architectures.     Experienced in Data Strategy, MDM, Data Governance, Data Lineage, Data Quality, Data Security and  GDPR other compliances with proficiency in different data access methods.    Experienced in Data Warehouse/Data mart architecture and expertise in ETL, data modeling for OLAP &  OLTP systems from design and analysis to implementation.     Defined and created technology roadmap for the solutions with data and cloud analytical tools like Azure Data  Factory, Azure Data Bricks, Azure Synapse, Azure Data Lake Store, AWS S3, EC2, EMR, Redshift and   architected Data Lake\\DW Augmentation solution for various organizations.     Deep understanding of Regression, Classification and Clustering algorithms e.g. Decision Tree, SVM,  Random forest, k-NN, k-Means, Neural Network, Gradient boosting etc.    Experienced in Fraud & Risk detection, customer/product segmentation, recommendation, customer  sentiment, Life time value, personalized marketing, speech/Image recognition use cases etc.     Experienced in Artificial Intelligence with Machine Learning/NLP/Deep Learning and other evolving  technologies with methods of integration to current organization systems.     Experienced with Machine learning libraries and tools Sci-kit-Learn, NumPy, Pandas, NLTK, SpaCy etc.    Experienced and good understanding of Azure/AWS/GCP Cloud solutions applications/tools.     Experienced in Hadoop, Pig, Hive, Spark, Sqoop, Flume and Kafka with No-SQL databases.     Experienced in writing SQL, Python, Excel (Pivot tables, Charts, Visualization), Tableau and Power BI.   EDUCATION    M.Sc (Statistics) & M.C.A from VBS Purvanchal University.   CERTIFICATIONS    Executive Data Science Specialization from JHU, USA    TOGAF9 Certified from The Open Group USA    Cloudera Apache Hadoop Developer Certified    Six Sigma Green Belt Certified    Certified Scrum Master (CSM) by Scrum Alliance    PMP Certified from PMI USA    Software Product Management from University of Alberta    PATENTS    Publication: Systems and Methods for AI enabled metal 3D printing - No.202011054553 A              mailto:skrai75@gmail.com      Big Data Analytics/Data Science Professional      Projects Profile   Organization: Orient Electric Limited      11/2019  05/2020   Role Principal Consultant - Data Science-Analytics & BI   Project Responsibilities:   As Head - Principal Consultant of Analytics, Responsible for    Driving Data Specific initiatives, building and growing business relations with customers/consumers.    Strategic planning, Architecting for all analytics projects.    Created Capacity plan for setup of Analytics environment.    Project/Program management of data and digital transformation projects.    Identification and approval of specific analytics Use Cases from management.    Using Machine Learning and Python to manipulate data and draw insights from large data sets for the  identified use cases.    Mentoring data analytics team and constantly honing their professional skills, and preparing them for the  future roles.      Organization: Ernst and Young India LLP     06/2018  10/2019   Role Sr.Analytics Architect/COE Lead/Manager Data Science      Project Responsibilities:   As Sr. Analytics Architect/COE Lead, Responsible for    Setting up data science COE for EY in Gurgaon, Built and grown business relationships with customers for  Data strategic projects.    Consulting and Strategic planning for advance analytics/deep learning projects.    Enabled teams in implementing machine learning algorithms/models through R and Python.    Designed adaptive, reliable and durable architectures for reporting/analytics systems.       Organization: Wipro Limited       08/2016 01/2018   Role Data Science Evangelist/Sr.Analytics Architect/Sr.Practice Manager   Practice Responsibilities:   As a Data Science evangelist/Sr.Analytics Architect/Sr. Practice Manager, Responsible for    COE/Practice management with Client interfacing, due diligence and Identification of Opportunities.  Creating the Budgets, Project Plan, Tracking variance and Milestones.     Project Consulting and proposed data monetization for financial and other related institutions    Researching on designing new processes for building large, complex data sets. Strategizing new uses for data  and its interaction with data design with Statistical modeling.    Big Data and Advance analytics high level estimations, Preparation and responses to RFPs.    Architected and devised Data Lake\\DW Augmentation solution for Financial institutions    Supporting delivery teams in various Big Data\\Adv.Analytics projects     Organization: Tata Consultancy Services Limited     10/2012  08/2016   Role Sr. Analytics Architect/Sr.Manager   Project Responsibilities:   As a Analytics Architect/Manager, Responsible for    Leading and managing data Science teams.    Designed adaptive, reliable and durable architectures for reporting/analytics systems worked on Credit Risk  data models.     Predicted trends and patterns, by using Predictive Analytics through R and Visualization through Tableau  reporting systems.   Operating Systems Unix, Windows   Langs/Databases C++, Java, SQL, PL/SQL, R, Python/ Oracle   Tools & Utilities Jira, Confluence, MS Project, Git, GitHub, Talend, Informatica, MapReduce, HDFS, Hive, Pig,   HBase, Sqoop, R/Spark/Python for ML, NumPy, Pandas, Sci-Kit-Learn, spaCy, NLTK, Big   Insight, IBM Watson, Azure Data Lake, Azure Data Factory, Azure Data Analytics, SQL Data   Warehouse, AWS S3, EC2, EMR, Redshift        Big Data Analytics/Data Science Professional       Project estimations, responding RFPs to achieve project/program goals.    Guiding teams to create Big Data Strategies and documentations for analytics implementations.     Organization: Birla soft Limited       07/2011  03/2012   Role Analytics- COE Lead/Sr.BI Architect/Sr. Manager   Project Responsibilities:   As an Analytics-COE Lead Manager/Sr.Architect, Responsible for    Architect, design and communicate high-level structural and behavioral aspects of a system to enable and  guide the design and development of integrated solutions that meet current and future business needs in   alignment with corporate IT standards and architecture guiding principles.    Build and grow any business relationships vital to the success of the Big Data strategic projects.    Estimated the resources and participants needed to achieve project/Program goals.     Organization: FLSmidth Limited       12/2010  07/2011   Role Sr.Manager/ Senior IT Specialist   Project Responsibilities:   As a Senior IT Manager, Responsible for    Impact Analysis, solution design for the requirements with platforms.    Creating the Budgets, Project Plan, Tracking variance and Milestones.     Organization: Infosys Technologies Limited     12/2007-12/2010   Role Project Manager      Project Responsibility:   As a Project Manager, Responsible for    Prepared Detail Design and provided guidance for Support and maintenance activities. Configuration, release  documents & metrics and managing project risks.    Gathered the requirement, completed the proof of concept, Designed, Developed and Tested Physical Layer,  Business Layer and Presentation Layer of OBIEE      Organization: NIC & Others    06/2000  12/2007   Role Team Member/Consultant         Place:            Date:                                        (Sateesh Kumar Rai)    ", "tokens": [{"text": "Orient Electric Limited", "start": 3588, "end": 3611, "token_start": 633, "token_end": 635, "entityLabel": "company"}, {"text": "11/2019  05/2020", "start": 3617, "end": 3633, "token_start": 637, "token_end": 639, "entityLabel": "period"}, {"text": "Ernst and Young", "start": 4436, "end": 4451, "token_start": 773, "token_end": 775, "entityLabel": "company"}, {"text": "06/2018  10/2019", "start": 4466, "end": 4482, "token_start": 779, "token_end": 781, "entityLabel": "period"}, {"text": "Wipro", "start": 5046, "end": 5051, "token_start": 880, "token_end": 880, "entityLabel": "company"}, {"text": "08/2016 01/2018", "start": 5066, "end": 5081, "token_start": 883, "token_end": 884, "entityLabel": "period"}, {"text": "Tata Consultancy Services Limited", "start": 5999, "end": 6032, "token_start": 1036, "token_end": 1039, "entityLabel": "company"}, {"text": "10/2012  08/2016", "start": 6037, "end": 6053, "token_start": 1041, "token_end": 1043, "entityLabel": "period"}, {"text": "Birla soft Limited", "start": 7141, "end": 7159, "token_start": 1265, "token_end": 1267, "entityLabel": "company"}, {"text": "07/2011  03/2012", "start": 7166, "end": 7182, "token_start": 1269, "token_end": 1271, "entityLabel": "period"}, {"text": "FLSmidth Limited", "start": 7826, "end": 7842, "token_start": 1387, "token_end": 1388, "entityLabel": "company"}, {"text": "12/2010  07/2011", "start": 7849, "end": 7865, "token_start": 1390, "token_end": 1392, "entityLabel": "period"}, {"text": "Infosys Technologies", "start": 8140, "end": 8160, "token_start": 1442, "token_end": 1443, "entityLabel": "company"}, {"text": "12/2007-12/2010", "start": 8173, "end": 8188, "token_start": 1446, "token_end": 1448, "entityLabel": "period"}, {"text": "NIC", "start": 8616, "end": 8619, "token_start": 1520, "token_end": 1520, "entityLabel": "company"}, {"text": "06/2000  12/2007", "start": 8632, "end": 8648, "token_start": 1524, "token_end": 1526, "entityLabel": "period"}], "relations": [{"child": 637, "head": 633, "relationLabel": "duration"}, {"child": 779, "head": 773, "relationLabel": "duration"}, {"child": 883, "head": 880, "relationLabel": "duration"}, {"child": 1041, "head": 1036, "relationLabel": "duration"}, {"child": 1269, "head": 1265, "relationLabel": "duration"}, {"child": 1390, "head": 1387, "relationLabel": "duration"}, {"child": 1446, "head": 1442, "relationLabel": "duration"}, {"child": 1524, "head": 1520, "relationLabel": "duration"}]}, {"document": "Satish M S, Prince 2, CSM, ITIL.  Mobile: +91 98452 00366| E-mail: satish.ms@hcl.com     Project Manager (BFSI Domain)  CSM & Prince 2 certified, Innovative & solutions focused IT professional, specialized in leading high-performing & multi-disciplinary teams from product development through successful product launches. Accustomed to managing multiple projects & priorities in fast-paced environments. Total 15 years of work experience, Spanning across, Service Delivery, Infrastructure, Projects, Transition, Migration, Data Centre & Environments Management. I am a highly focused and process oriented Person. My main expertise is Data Center, Service desk, Project Management, Infrastructure, Transition, Migration, Service Delivery, and DR/BCP.  Combine technical expertise with strong business acumen and operational understanding; ensure all technical Strategies and activities align with corporate goals.  Expertly oversee large-scale IT projects, networks, and infrastructuresfrom initial planning and development to implementation and enhancements.  Proven leader adept at building and motivating diverse technical teams to achieve outstanding results and complete projects on time and within budget.      Skills  Software:-  MS Office (MS Word, Access, Excel, PowerPoint), MS Project, MS Visio.  Platforms:-  Windows, UNIX, LINUX, Database, Middleware (Web Sphere, MQ), Exchange 2000, SQL, FileNet, Thunderhead, Oracle, Lend Fast.  Tools:-  JIRA, Service Now, AHD, SDM, Bamboo, SVN, Connect Direct, Control-M, Right Fax, Share Point, RTC, RAM, HP-QC, ECOSYS, Zabbix, Splunk, Bit Bucket, Jenkins.   PROFESSIONAL EXPERIENCE  HCL Technologies Ltd., Bangalore as Sr. Technical Lead     Dec 2020 - Present  Phalita Company Pvt. Ltd., Bangalore as Sr. Technical Recruiter on contractual basis   June 2019  Oct 2019.  Wipro Technologies Pvt. Ltd., Bangalore as Service Delivery Lead     Aug 2017  Sept 2018.  Projects & Accomplishments:  Work within cross-functional teams including representation from Businesses, Compliance, Tech Offices & Tech Shared Services. Help the team self-organize & ensure it strives to make each increment of functionality potentially shippable.  Facilitates Scrum ceremonies which include daily stand up, sprint planning, sprint reviews with demo and retrospective. Responsible for ensuring team follows all scrum and company processes. Collects project estimates where required from the project team leads.  Works with release coordinator and other Scrum Masters to develop release calendar and timeline, release tracking and required documentation.  Works with QA and Technical Operations personnel in order to configure and prepare the necessary testing environments. Manages release to live environment.  Act as the primary business liaison to the business segment, through facilitating communication between Development, QA, Product Management, Technical Operations and Segment business leaders on new product releases, technical outages, and routine maintenances (hosting moves).  Superb communication and presentation to key business stakeholders are instrumental.  Develop, facilitate and maintain best of breed product support tools including run books, monitoring, and knowledge bases to aid in the product support process.  Have good hands-on experience working in BFSI domain related projects & have handled projects to understand Risks & Implications to the particular line of business.  Cognizant Technology Solutions Pvt. Ltd., - Bangalore as Manager  Projects            Sept 2014 to April 2017.                                                    Projects & Accomplishments:  Lead a medium sized Agile/Scrum project with 50+ consultants for a health care project to completion on time.  Led efforts to turn around the project(s) by doing a deep dive analysis and having a focus group to address pain points and suggest changes to the operation process  Established delivery processes & implementation methodologies and led business development efforts, client management & assisted in technology roadmap preparation of an Insurance client.   Deliver all Transition Projects and Programmes in line with recognized best practice methodology (managing risk, governance, quality assurance, issue resolution, reporting) , where the successful implementation of service resource skills and processes are just as critical to project success as technology.  Ensured application availability and data integrity through preventative maintenance and upgrades  Resolved conflict, improved morale and established clear goals by effectively managing timelines and shared resources with special emphasis on building relationships across departments  Practiced backlog grooming, release and sprint planning, daily standups, impediment removal  Ensured the development teams are practicing the agile principles. Completed product demonstrations, retrospectives, and resolve action items. Assisted Product Owner with Backlog maintenance  Collaborated with the team, architect and product owner to ensure rightness of features and design  Developed, managed and tracked project plan to implement requested features  Led infrastructure projects to augment server capabilities, and address database and networking needs  Resolving project issues and problems & be during the escalation point & support the team when they are unable to resolve production related issues & help drive to closure.  Assigning tasks to individual project team members & keeping track of it on a daily basis by having a daily stand-up call with the team & try to remove any impediments.     Cap Gemini India Pvt. Ltd., Bangalore as Sr. Consultant                                             Oct 2012 to Sept 2014.  Projects & Accomplishments:  Responsible to liaison with customer and other key stakeholders to assess the business needs of the customer and identify key challenges, constraints, and risks and thereby define the project scope after extensive due diligence.  Responsible for getting sign-offs and communicating the approved SLA requirements to the lead technical architect and the implementation team  Responsible to track the scope and schedule of the project as per Project Plan and ensure minimal changes to project  Prepare other subsidiary plans like quality plan, communication plan, risk management plan, change control plan, disaster recovery plan  Responsible to move project into steady state operations after successful KT, parallel run and warranty phase.  Prepare dashboards to communicate the progress regarding scope, schedules, and cost to all stakeholders.    ANZ Technology  Bangalore, as Environment Manager                                                              July 1999 to Sept 2012.  Projects & Accomplishments:  Environment Manager - Mortgages Technology  Managed a team of technical SMEs to provision hardware and software for all development and test environments across the program.  Ensured the availability and configuration across 25+ development and test environments are managed to adhere SLA.  Resource and workload management of permanent and contract employees specializing in infrastructure design, Windows build, database services, network services, security engineers, environment analysts and configuration managers.  Support technical implementation manager for cut-over planning and go-live activities.  Establish a DevOps capability to automate manual tasks performed by technical resources.  Application Support Manager - Retail Lending System   Technology platform owner of the RLS platform accountable for managing all projects, releases and enhancement delivery.  Accountable for the total cost of ownership (TCO) for RLS which includes the operating cost to run the platform.  Demand management, pipeline prioritization and scheduling of all initiatives on RLS.  Accountable for the relationship and delivery management of software vendor including commercial ownership of MSAs and SOWs.   Conduit to the business and production support teams for platform maintenance.   Responsible for the resource management of all direct and dotted line resources, establishing annual KPIs and carrying out performance appraisals.   Provide governance and management oversight across all test phases.  Release Manager - Retail Lending System  Senior manager accountable for release and implementation management of changes to the RLS platform through the program Pilot roll-out phase.  Manage a pool of resources from ANZ and external vendors from development to implementation.  Define technical implementation strategies and co-ordinate technical SMEs during implementation go-live weekends for all releases.  Negotiate approvals from various technology stakeholders to implement changes to the platform.  Technology platform owner of the RLS platform accountable for managing all projects, releases and enhancement delivery.  Demand management, pipeline prioritization and scheduling of all initiatives on RLS.  Accountable for the relationship and delivery management of software vendor including commercial ownership of MSAs and SOWs.  Conduit to the business and production support teams for platform maintenance.  Responsible for the resource management of all direct and dotted line resources, establishing annual KPIs and carrying out performance appraisals.  Provide governance and management oversight across all test phases.  Implementation Lead  User Migration NT to Win2k  Managed Windows 2000 project to migrate over 450+ seats and 50+ servers in four different locations & 50 remote laptops. , Applications, Profiles, Mailboxes to the new environment Windows 2000 (AD) secured environment. Migration was carried out on a phase by phase manner by taking the necessary business head approvals & was done on a weekend for about 4 weeks & finished the migration successfully.  Migration of the Production Windows 2000 servers was carried out in a phase by phase manner i.e., Domain Controllers, Exchange, Print & Build Servers & the NAS filers with the help of HP & NetApp Engineers by taking the necessary business approvals over the weekend & completed it successfully without any disruption in the business. Successfully completed   Roll-out under budget and 3 months ahead of schedule; exceeded customer satisfaction targets, standardized disjointed desktop back-end, and stabilized infrastructure  Led pilot migration to test processes, anticipate deployment challenges, and maximize efficiencies.  Substantially increased application response time by initiating a capacity planning analysis project to research storage trends and make recommendations on when to add storage.  Business Continuity/DR/Emergency Management Consultant  Reinforce the need for a Business Continuity program, resilience strategies and verify recovery objectives are sufficient to recover the business as identified by the organizations requirements.   Facilitate the Business Continuity Management testing program. Work with business departments to understand test objectives, scope and assist in the execution of the test assess the business continuity implications of proposed technological or organizational changes.   Co-ordinate revisions to existing business continuity plans or procedures through change control methods  Drive visibility of existing business continuity programs through training, information sessions and exercises  Mature existing corporate wide business continuity programs across multiple teams and discipline while maintaining audit readiness.  Implement or improve site emergency management procedures to ensure employee safety is a priority  Design and administer recovery support and direct business resumption staff during a disaster in the implementation of response and alternate operating strategies.          Infrastructure Lead - ACE Program Thunderhead Platform  Played a key leadership role on the design, development & implementation in setting up new environments for ACE Program. This was a 1 year project, was involved in setting up environments from DEV till P&V later stages got involved in setting up Prod & DR environments and also provided Production support.  Co-ordinated with windows team in getting the servers built & with SQL Team for DB configuration/installation etc.,  Performed installation/configuration  of Thunderhead on the respective environments i.e., DEV, ST, INT, QA, Prod & DR documented the same,  Performed release management activity by putting a plan in place for non-prod environments by coordinating with the respective teams for controlled/managed environments.  Performed deployment related activities in non-prod & prod environments as per the release notes provided by development team.  Learnt product named Thunderhead & trained people on the same using the product & the application which was built on this platform. Involved in production support & later transitioned to Production Team. Currently involved in BAU support activity for this application.    Esanda Finanz & Leasing Ltd., - Bangalore, as Sr. Systems Engineer        June 1994  July 1999.  Responsibilities:-  Oversee the administration of LAN / WAN.  Oversee the administration and maintenance of IT infrastructure.  Introducing and installing new system upgrades and security backups for hardware and software systems.  To ensure the smooth running of all the systems, including anti-virus software, print services and e-mail provision and to adhere the software licensing laws.  To oversee troubleshooting, system backups, archiving, and disaster recovery and provide expert support whenever necessary.  Build and maintain vendor relationships and manage the purchase of hardware and software products.  To manage and set up IT Infrastructure of Mumbai, Bangalore Office & establish connectivity between these two offices as well.  To ensure that all the facilities meet the needs of the end user with appropriate advice & support.   EDUCATION & CERTIFICATIONS  Bachelor of Commerce from Bangalore University Year 1993.  Project Management Professional (PMP), In Progress  Professional UNIX, C & Oracle from TULEC Bangalore.  Completed Oracle Cloud Infrastructure Architect Workshop.  Completed Oracle Cloud Infrastructure Foundation Associate.  Completed AWS Solutions Architect Associate Level course from Simplilearn.  Completed DevOps Foundational Overview Training.  Prince2 PR actioner Certified in Project Management   Certified in Scrum Master from Scum Alliance.  Certified in Enterprise Architect from TOGAF Open Group year 2011  MCSE in windows 2000 track, 2003.  Foundation Certified in (ITIL v2 & v3) in IT Service Management Year 2004   Certified in FileNet Administration v3.5   Completed Fast track training course in vSphere v4.0 from HP Bangalore.   PREVIOUS ASSIGNMENTS  HCL Technologies Ltd.,     Dec2020  Present  Phalita Company Pvt. Ltd., (Contract Basis)    June 2019  Oct 2019  WIPRO Technologies Pvt. Ltd., Lead Consultant   Aug 2017  Sept 2018  Cognizant Technology Solutions Pvt. Ltd - Project Manager                  Sept 2014  April 2017  Cap Gemini Consulting Pvt. Ltd., - Environment Manager    Oct 2012  Sept 2014  ANZ Operations and Technology Pvt. Ltd - Environment Manager       July 1999  Sept 2012   (Non-IT Experience)  Esanda Finanz & Leasing Ltd., - Sr. Systems Engineer                                        June 1994  July 1999  Dalal Street Journal Group - Computer Operator                                            June 1993  June 1994  Blow Plast Pvt. Ltd., - Marketing Executive                                                        April 1993  June 1993    Sensitivity: Internal & Restricted  Sensitivity: Internal & Restricted", "tokens": [{"text": "HCL Technologies Ltd", "start": 1634, "end": 1654, "token_start": 317, "token_end": 320, "entityLabel": "company"}, {"text": "Dec 2020 - Present", "start": 1693, "end": 1711, "token_start": 328, "token_end": 331, "entityLabel": "period"}, {"text": "Phalita Company Pvt. Ltd", "start": 1713, "end": 1737, "token_start": 333, "token_end": 338, "entityLabel": "company"}, {"text": "June 2019  Oct 2019", "start": 1800, "end": 1819, "token_start": 349, "token_end": 353, "entityLabel": "period"}, {"text": "Wipro Technologies Pvt. Ltd", "start": 1822, "end": 1849, "token_start": 356, "token_end": 361, "entityLabel": "company"}, {"text": "Aug 2017  Sept 2018", "start": 1891, "end": 1910, "token_start": 368, "token_end": 372, "entityLabel": "period"}, {"text": "Cognizant Technology Solutions Pvt. Ltd", "start": 3435, "end": 3474, "token_start": 624, "token_end": 630, "entityLabel": "company"}, {"text": "Sept 2014 to April 2017", "start": 3521, "end": 3544, "token_start": 638, "token_end": 642, "entityLabel": "period"}, {"text": "Cap Gemini India Pvt. Ltd", "start": 5597, "end": 5622, "token_start": 969, "token_end": 975, "entityLabel": "company"}, {"text": "Oct 2012 to Sept 2014", "start": 5697, "end": 5718, "token_start": 982, "token_end": 986, "entityLabel": "period"}, {"text": "ANZ Technology", "start": 6598, "end": 6612, "token_start": 1135, "token_end": 1136, "entityLabel": "company"}, {"text": "July 1999 to Sept 2012", "start": 6709, "end": 6731, "token_start": 1144, "token_end": 1148, "entityLabel": "period"}, {"text": "Esanda Finanz & Leasing Ltd", "start": 15263, "end": 15290, "token_start": 2582, "token_end": 2587, "entityLabel": "company"}, {"text": "June 1994  July 1999", "start": 15355, "end": 15375, "token_start": 2594, "token_end": 2598, "entityLabel": "period"}, {"text": "Dalal Street Journal Group", "start": 15377, "end": 15403, "token_start": 2600, "token_end": 2603, "entityLabel": "company"}, {"text": "June 1993  June 1994", "start": 15467, "end": 15487, "token_start": 2608, "token_end": 2612, "entityLabel": "period"}, {"text": "Blow Plast Pvt. Ltd", "start": 15489, "end": 15508, "token_start": 2614, "token_end": 2619, "entityLabel": "company"}, {"text": "1993  June 1993", "start": 15594, "end": 15609, "token_start": 2625, "token_end": 2628, "entityLabel": "period"}], "relations": [{"child": 2625, "head": 2614, "relationLabel": "duration"}, {"child": 2608, "head": 2600, "relationLabel": "duration"}, {"child": 2594, "head": 2582, "relationLabel": "duration"}, {"child": 1144, "head": 1135, "relationLabel": "duration"}, {"child": 982, "head": 969, "relationLabel": "duration"}, {"child": 638, "head": 624, "relationLabel": "duration"}, {"child": 368, "head": 356, "relationLabel": "duration"}, {"child": 349, "head": 333, "relationLabel": "duration"}, {"child": 328, "head": 317, "relationLabel": "duration"}]}, {"document": "                                               Shagun Kala     Machine Learning Engineer       About: Jaipur, Rajasthan | kala.shagun@gmail.com | +91 8385898002    Links: linkedin.com/in/shagunkala | github.com/Shagun-25 | medium.com/@kala.shagun      EDUCATION   Malaviya National Institute of Technology (MNIT Jaipur)              May 2019  B. Tech. Civil Engineering                                               CGPA 8.32  GATE 19 CIVIL  AIR 885   Subodh Public School                                                   May 2015           Intermediate (10+2)                                                                          CBSE  94.4%     CERTIFICATIONS & TECHNICAL SKILLS   Programming: Python, R, SQL, JAVA, C++, Linux Shell  Frameworks: Flask  Software: Google Cloud Platform, Jupyter Notebook, RStudio, Eclipse, SharePoint, MS Office Suite, GitHub,   Corel Draw, Photoshop  Data Science: Data Cleaning and Interpretation, EDA, Feature Engineering, Modeling, Optimization, Deployment  Machine Learning:  - Classification: Logistic Regression, KNN, SVM, Naive Bayes, Decision Trees, Ensembles  - Regression:  Linear Regression, KNN, SVR, Decision Trees, Ensembles  - Clustering:  K-Means, Hierarchical  - Deep Learning:  NN, CNN, RNN, Transformers, TensorFlow Model Quantization, Keras  - Time Series:  Rolling Mean, Dickey Fuller Test, ARIMA, SARIMAX, Facebook Prophet, LSTM, Anomaly Detection  Certifications: Applied AI Course, Udemy: Python for Data Science and Machine Learning Bootcamp,              DataCamp: Introduction to Python, Intermediate Python course.    BUSINESS EXPERIENCE  Capgemini, Pune, India   Senior Analyst                                   Jul 2019  Present    Certified Capgemini Automation Practitioner    Working in HPC Development Team for a top European car manufacturer.    Work involve Python and JAVA Development and creating Shell Scripts for Linux Servers.    Also, created various automation solutions using Python for other client projects under Mechanical Design and  Simulation (MDS) department.    Won the award for Best Automation Use Case for Nov. 2019 Month in Capgemini.      PROJECTS  Grant Access using Machine Learning  Case Study using a Kaggle Competition Dataset    Built an employee access control system, which automatically approves or reject employee's resource application.    This is an End-to-End project starting from Exploratory Data Analysis (EDA) till Web Application Deployment using Flask.    Dataset Used: Kaggle Competition - Amazon Employee Access Challenge    Technology/Concepts Used: Python, Various Encoding Techniques, Classification Algorithms, Ensembles, Forward  Feature Selection, Hyperparameter Tuning, Flask Web Framework.    The performance score achieved from the best model lands at 12th percentile rank on Kaggle leaderboard.    Blog Link: https://medium.com/analytics-vidhya/grant-access-using-machine-learning-829ad82ffefb               mailto:kala.shagun@gmail.com mailto:medium.com/@kala.shagun https://medium.com/analytics-vidhya/grant-access-using-machine-learning-829ad82ffefb   Stock Market Prediction using News Sentiments  Detailed Time Series Analysis                                                                         An End-to-End Project to predict Stock Market Prices starting from Data Collection stage till Model Quantization for  Production use.    Dataset Used: Scraped Nifty Index Stock Prices and Twitter News Data.    Technology/Concepts Used: Python, Data Cleaning, Feature Engineering, Dickey-Fuller Test, Anomaly Detection,  TensorFlow Model Quantization, Building final pipeline with optimized model.    Algorithms used: ARIMA, SARIMAX, Facebook Prophet, LSTM.    This case study is based on a research paper:   Stock Price Prediction Using News Sentiment Analysis from IEEE 2019 Conference-  http://davidanastasiu.net/pdf/papers/2019-MohanMSVA-BDS-stock.pdf    Blog Link: https://medium.com/@kala.shagun/stock-market-prediction-using-news-sentiments-f9101e5ee1f4     https://medium.com/@kala.shagun/stock-market-prediction-using-news-sentiments-dc4c24c976f7      REWARDS AND ACHIEVEMENTS    Won the award for Best Automation Use Case for Nov. 2019 month in Capgemini.     Secured AIR 885 in Civil, GATE 2019 exam with the GATE score of 734.    Secured a Top 100 National Ranking in Rubiks Cube Speed solving as stated in the latest Official Results of WCA (World  Cube Organization).    Among the fastest Rubiks Cube Speedsolver in the Rajasthan state.    Organized the first ever official WCA affiliated Rubiks Cube Competition Pink City Open 2018 in the city of Jaipur.    Served as the Technical Secretary of MNIT Jaipurs Annual Cultural Fest- Blitzschlag18 held from 2nd- 4th February.    Won first prize in Bridge-o-Mania, Bridge Building Competition held at MNIT Jaipur in November 2017.    Ranked among the top 1% Candidates in JEE Mains 2015, securing AIR 11043.   Won a Rubiks Cube Competition in Blitzschlag 2013.      INTERESTS/HOBBIES    Computer programming (Machine Learning, Data Science)    Solving Rubiks Cube    Graphic Designing    Playing Cricket, Badminton and Billiards   http://davidanastasiu.net/pdf/papers/2019-MohanMSVA-BDS-stock.pdf https://medium.com/@kala.shagun/stock-market-prediction-using-news-sentiments-f9101e5ee1f4 https://medium.com/@kala.shagun/stock-market-prediction-using-news-sentiments-dc4c24c976f7  ", "tokens": [{"text": "Capgemini", "start": 1606, "end": 1615, "token_start": 270, "token_end": 270, "entityLabel": "company"}, {"text": "Jul 2019  Present", "start": 1680, "end": 1697, "token_start": 279, "token_end": 282, "entityLabel": "period"}], "relations": [{"child": 279, "head": 270, "relationLabel": "duration"}]}, {"document": "                                          E-MAIL shubhranshu92@gmail.com                  CELL +91  8800959748      Shubhranshu Atray    +91-8800959748                       shubhranshu92@gmail.com             A consultant with more than 4 years experience in model risk management, developing and validating credit risk models.     Consultant, Developed and validated models for multi-geographical banks   Developed a nowcasting model to estimate the near-term economic impact of Covid-19 crisis and identified scenarios for individual   sectors/industries based in Europe   Developed survival models to estimate the impact of Covid-19 on the U.S. Commercial real estate   Improved efficiency of periodical CCAR validations using priority based sectional validation and introduced efficiency levers;   standardized code library, pre-populated text document and created a procedures document   Validated Merchant processing account model for a US banking firm with high level of complexity containing more than 30 sub-  models leading to improved   Validated qualitative models for a top US banking firm including brokerage services, collection, legal etc.   Impact  Improved the efficiency of cyclical validation which was circulated internally and lauded by leadership     Senior Consultant, Pioneered current expected credit loss methodology for Multinational BHC   Orchestrated strategy, mapped changes, formulated gap analysis, analyzed methodologies, & enabled end to end development for the   CECL model development   Forecasted new capital requirements through empirical, econometric & predictive modelling techniques for business   heads/stakeholders   Impact  Consulting the client to meet capital requirements & analyzing the impact on portfolio   Consultant II, Developed a model & administered loss forecasting for comprehensive capital Analysis & review    Revamped loss forecasting models (PD, EAD, LGD) for mid-cycle & annual CCAR submission of a multi-billion $ portfolio    Anticipated and responded to concerns from internal modeling audit team and made changes which met their concerns regarding   model methodology, variable selection, and back-testing framework    Envisioned & enhanced the loss forecasting workflow by consolidating three work streams into one which led to reduced runtime & risk   concerns for end user, automated reports led to faster delivery & improved insights    Oversaw forecasting runs & developed loss-walks for explaining the changes due to various risk drivers in the portfolio    Impact  Delivered the models/loss analysis for CCAR submission (high risk/priority for bank) & examined results for insights   Consultant I, Developed models & integrated processes for improved efficiency     Refined loss forecasting models (PD/EAD) for accuracy on business metrics for CCAR submission    Led the automation exercise for loss forecasting process leading to reduced human errors & runtime  8 hours to 1.5 hours    Awarded promising new-comer award for leading the automation of workflow    Impact- Awarded Best-Team across 30 other teams in annual evaluation for delivering impact for Clients business   Business Analyst, Spearheaded creation of Bikes website in ASEAN region & other countries    Engineered a categorization framework for Bikes in South Asian market to standardize the filters for Users    Accelerated the process of launching a new feature/section on website by streamlining & dividing work responsibilities    Gathered intel on Auto industry trends, growth hacks and process improvements for improved execution of project    Tracked Consumer Journey and pitched a case to develop a new source of revenue for the business    Impact  Leveraged SEO, SEM, analytics & management practices to bring Cardekho to a total of 20 countries in a span of less than 5 months          Academic  Profile             Degree/Examination Institute CGPA/% Year       B.Tech. in Mechanical Engineering IIT Roorkee 7.9 2011-2015        XII Modern School, Noida 94.4 2010       X Modern School, Noida 91.8 2008            Professional Experience         Risk Dynamics | Mckinsey                                          Aug 19 - Present         Exl Services                                         Jul 16  Aug 19         Car Dekho                                        Jun 15  Mar 16    mailto:shubhranshu92@gmail.com      E-MAIL shubhranshu92@gmail.com                  CELL +91  8800959748       Indian Delegate, Energy & Sustainability panel at HPAIR  Asias largest student meet organized by Harvard University   Participated in Panel discussions on Energy Access in the Developing World, Innovations in Energy: Seeking Renewable and   Alternative Sources and Branding in Asia      Organized a paper presentation event for annual technical fest, over 40 teams participated nationwide   Led a 6 membered team to a successful promotion of the event        Executive Member, MIESS  2012 - 2013    Collaborated to start a departmental newsletter to tackle revenue challenges   Organized foreign internship talk, >150 students attended   Administered the intra college publicity campaign, lead to increased Facebook page traffic and campus presence        RHEX Bot, Shristi  2013 - 2014    Led a team of 4 member to design & fabricate a hexapod for maneuvering on all terrain   Won II prize and received newspaper coverage for the achievement    Secured All India rank of 1761 among 450,000 students in JEE-2011 (< top 0.5%)                                                                    2011   Gold Medalist for topping school and exceptional performance in PCM                                 2010   Three times school scholarship receiver.                                                                                 2010        Extra  Curricular Activities      Team leader of II prize winning team in Annual Intra college fest for RHEX bot                                  2014   Member of III prize winning team Annual Intra college fest for laptop controlled car  2013   Member of rural education outreach program under National Service Scheme                   2011             International Conferences         Delegate, Harvard Project for Asian and International Relations  August 2015     Positions of Responsibility         Co - Coordinator, Cognizance    March 2013     Achievements         Scholastic Achievements         Skills     MS Office, MS Excel, Google Analytics, SAS, R & SQL   mailto:shubhranshu92@gmail.com  ", "tokens": [{"text": "Mckinsey", "start": 4115, "end": 4123, "token_start": 668, "token_end": 668, "entityLabel": "company"}, {"text": "Aug 19 - Present", "start": 4165, "end": 4181, "token_start": 670, "token_end": 673, "entityLabel": "period"}, {"text": "Exl Services", "start": 4190, "end": 4202, "token_start": 675, "token_end": 676, "entityLabel": "company"}, {"text": "Jul 16  Aug 19", "start": 4243, "end": 4257, "token_start": 678, "token_end": 682, "entityLabel": "period"}, {"text": "Car Dekho", "start": 4266, "end": 4275, "token_start": 684, "token_end": 685, "entityLabel": "company"}, {"text": "Jun 15  Mar 16", "start": 4315, "end": 4329, "token_start": 687, "token_end": 691, "entityLabel": "period"}], "relations": [{"child": 670, "head": 668, "relationLabel": "duration"}, {"child": 678, "head": 675, "relationLabel": "duration"}, {"child": 687, "head": 684, "relationLabel": "duration"}]}, {"document": "                                                      SHYAM CHOWDARY UPPALAPATI     San Jose, CA| +1 408-506-0081| Ushyam2002@yahoo.com |www.linkedin.com/in/shyam-chowdary-uppalapati/       Leader with a demonstrated success through 20+ years in technology, relationship management, account   management, BI/Cloud Integration, Project & Delivery Management in the hi-tech & manufacturing  industries       In addition to client mining & delivery oversight of an application portfolio, established and grew the  BIDW Practice at eTouch from ground up to $6 million revenue within 2 years, winning new clients like  VMWare, Symantec, Intuit & BAL Global.    At TechM, grew & delivered $15 million in new BI/DW engagements at Cisco, positioning Tech M as #1 BI   partner of choice.    Led Ciscos Transformational journey from Oracle to Teradata as Preferred Vendor Partner.    Delivered 160 + projects and services in the areas of HiTech, Manufacturing, Auto & BFS domain with a     revenue of $60million, 350+ FTEs across US, Australia & India.    Extensive experience in facilitating business requirements gathering (BRDs /User Stories) and converting  them to solutions/ technical specifications to drive and influence business adoption.       CAREER SYNOPSIS        USEREADY| Jun 2020- Feb2021  Regional Vice President- Customer Success     Manage strategic-high growth accounts in the West Coast region and grow / oversee consulting/tech   delivery of BI / DW and app services tracks.     e-TOUCH INC, A VIRTUSA COMPANY| Jan 2013-May 2020  Director      Led delivery, and supported presales and account management. Setup global delivery teams, and  oversaw complex technical projects & programs from requirements definition, risk analysis & management,  to final delivery, managing both internal and external resources.       Established and grew the BIDW Practice from ground up to a team of 50+ people & $6 million. Secured  key contracts with WebEx, VMWare, Symantec, Intuit & BAL Global    Conducted roadshows and executed POCs for data analytics needs.    Saved Cisco $1.5M annually with supply chain solutions resulting in next-generation technologies   flexibility and scalability and report reconciliation    Achieved utilization of 100% at onsite & 90% at offshore.    Achieved 10/10 CDI from Hitachi for joint product development of Dark Data KYC Compliance Solution.     APOLLO EDUCATION GROUP| Jul 2011-Dec 2012  IT Director- Offshore Operations     Led selection & operations of new offshore/near-shore/onsite service providers.      NTT DATA| Oct 2010-Jul 2011  Senior Manager- Program Management/ Delivery     Led a $ 2.5 million stream of programs with 40 members in BI / DW.   TECH MAHINDRA| Sep 1992-Oct 2010  Program Manager  Engagement Manager  Senior Programmer  Trainee     2004  2010  Engagement manager  Cisco       Boosted annual revenues by $15 Million per annum, expanding the of data warehousing and BI projects  engagement to 260 associates in a 1-year span driving 100% account growth and positioning Tech M as  #1 BI partner of choice.   about:blank          Led Ciscos Transformational journey from Oracle to Teradata across Sales, Marketing, Manufacturing,   Customer advocacy, Finance & HR. Migrated 110+ disparate reporting applications from various  functional data marts.     Awarded Best Relationship Manager 2007.        1992  2004  Programmer / engagement manager       Role: Joined the organization as Trainee Engineer and grew through a series of positions as Senior  Programmer, Project/ Engagement Manager and later as Program Manager to drive large scale  transformation projects for major clients including Ford, Caterpillar, and GE.         Led the global team to implement award-winning IT project for GE Medical Systems (GEMS) that won  the annual SIX SIGMA Best of the Best Award category at GEMS.     Increased FTE count in projects and reduced onsite subcontractors that led to a growth in profitability by  20% per project and also led to an increase in customer satisfaction and employee morale.    Coached and mentored project teams in responsive support and technical quality, maintained an   attrition rate of < 5% through the year.       EDUCATION & PROFESSIONAL DEVELOPMENT        PMP Trained | PMI, San Jose, CA    PGDCA |A P Productivity Council    B E Electronics | Dr Babasaheb Ambedkar Marathwada University    ", "tokens": [{"text": "SYNOPSIS", "start": 1251, "end": 1259, "token_start": 224, "token_end": 224, "entityLabel": "company"}, {"text": "Jun 2020- Feb2021", "start": 1277, "end": 1294, "token_start": 227, "token_end": 229, "entityLabel": "period"}, {"text": "e-TOUCH INC", "start": 1491, "end": 1502, "token_start": 267, "token_end": 270, "entityLabel": "company"}, {"text": "Jan 2013-May 2020", "start": 1523, "end": 1540, "token_start": 275, "token_end": 277, "entityLabel": "period"}, {"text": "APOLLO EDUCATION GROUP", "start": 2384, "end": 2406, "token_start": 429, "token_end": 432, "entityLabel": "company"}, {"text": "Jul 2011-Dec 2012", "start": 2408, "end": 2425, "token_start": 432, "token_end": 434, "entityLabel": "period"}, {"text": "NTT DATA", "start": 2549, "end": 2557, "token_start": 458, "token_end": 460, "entityLabel": "company"}, {"text": "Oct 2010-Jul 2011", "start": 2559, "end": 2576, "token_start": 460, "token_end": 462, "entityLabel": "period"}, {"text": "TECH MAHINDRA", "start": 2696, "end": 2709, "token_start": 487, "token_end": 489, "entityLabel": "company"}, {"text": "Sep 1992-Oct 2010", "start": 2711, "end": 2728, "token_start": 489, "token_end": 491, "entityLabel": "period"}], "relations": [{"child": 489, "head": 487, "relationLabel": "duration"}, {"child": 460, "head": 458, "relationLabel": "duration"}, {"child": 432, "head": 429, "relationLabel": "duration"}, {"child": 275, "head": 267, "relationLabel": "duration"}, {"child": 227, "head": 224, "relationLabel": "duration"}]}, {"document": "SIDDHANT RASTOGI   MOBILE: +91-8979734934, E-MAIL: thesiddhantrastogi@gmail.com           A challenging position in an organization where I can enhance and strengthen my skills in conjunction with companys objective.     PROFILE OVERVIEW     Currently working with United Health Group (UHG), Noida as Data Analyst.  Technically Sophisticated Professional with 5 years of experience in writing complex SQL queries in Hive/SAS/SQL-Server/Oracle, analyzing data and driving insights, preparing/transforming datasets for data visualization, creating intuitive Tableau dashboards and stories on Tableau Desktop.   I have wide experience of working in an onshore-offshore model for Life Sciences/HealthCare clients.  Quick learner and efficient team player with excellent communication and interpersonal skills ability to resolve complex application issues.  SKILL SETS     Technical Skills:         Development Tools                           :   Tableau 2018.3, Apache Hive, SAS, DB Visualizer, SSRS, Informatica PowerCenter 9.1, Toad (Oracle), Microsoft VS (2013)   Databases                                           :   Microsoft SQL Server, Oracle 10g    Languages                                           :   SQL, T-SQL, PL/SQL, C#, HTML   Operation Systems                            :   Microsoft Windows    Service Management Tool               :  Service Now/Depot  Functional Skills:         Requirement gathering/analysis, design, development and finalization of technical and functional specifications.   Involved in analyzing, extracting & transforming datasets as per business requirement to feed into Tableau Desktop for creating dashboard/stories.  Fine/tuned SQL queries for maximum efficiency and performance.  Hands on experience on Tableau Desktop and Tableau Server for building, publishing customized interactive reports and dashboards.  Create variety of charts like: Bar chart, Pie Chart, Donut Chart, Line graph, Tree Map, Stacked Bars & Maps to drive insights through dashboards and worksheets in Tableau.  Generated Complex calculated field apply action filters and data source filters while handling huge volume of data.  Developed Tableau workbooks from multiple data sources using Data blending.      ORGANIZATIONAL SCAN     Working as Data Analyst with United Health Group (UHG) since Mar-2019  Till Date  Worked as Tableau and SQL Developer with Tata Consultancy Services from Jan-2016  Mar-2019      Projects Overview:        TCS Projects:      Title: Drug Dev Analysis / Employee Management Application(1-Window)  Client: Pharmaceutical Company / Tata Consultancy Services Limited  Domain: Life Science   Role: Developer   Tools and Technologies: SQL, MS-SQL Server, Oracle, Excel, SSRS, Tableau Desktop, Tableau Server   Description: Worked on multiple projects in TCS out of which above 2 are main.   Drug Dev Analysis: This project is a development project to create reports in Tableau Desktop for Drugs analysis and comparison to predict sales and business of drugs.  1-Window: This is an employee management application which track records the employee activities with TCS.  Responsibilities:   Requirement gathering, system analysis, and finalization of technical and functional specifications.  Building dashboard to bring out important insights which impact business and increase sales/profit.     United Health Group (UHG) Projects:   Title: Retention Dashboard  Client: United Health Care (UHC)   Domain: Insurance  Role: Data Analyst  Tools and Technologies: SAS/Apache Hive, Tableau Desktop 2018.3, Tableau Server, Advanced Excel   Description: This dashboard is used by senior management to get insights of our members leaving our plan and moving out of UHC Voluntarily (on their own) / Involuntarily (due to their death).   Responsibilities:   Requirement gathering and discussing KPIs, metrices & demographics which they want in dashboard.  Perform ETL in SAS/Apache Hive to generate final dataset that feeds into Tableau Desktop.  Developed Tableau dashboard with multiple tabs using variety of charts/functionalities available.    Title: Renew Rewards Dashboard  Client: United Health Care (UHC)   Domain: Insurance  Role: Data Analyst  Tools and Technologies: SAS/Apache Hive, Tableau Desktop 2018.3, Tableau Server, Advanced Excel   Description: This is basically a program run by UHC for its members in which UHC provides gift cards to its enrolled members which members can redeem for various purposes.  Responsibilities:   Requirement gathering and discussing KPIs, metrices & demographics which they want in dashboard.  Perform ETL in SAS/Apache Hive to generate final dataset that feeds into Tableau Desktop.  Developed Tableau dashboard with multiple tabs using variety of charts/functionalities available.    Title: Ad-hoc Requests, Report & Maintenance of multiple dashboards  Client: United Health Care (UHC)   Domain: Insurance  Role: Data Analyst  Tools and Technologies: SAS/Apache Hive, Tableau Desktop 2018.3, Tableau Server, Advanced Excel   Description / Responsibilities: These are maintenance and ad-hoc basis tasks done on daily/weekly/monthly schedules which normally includes minor changes to existing dashboard, automate report generation, generate new report as per ad-hoc requirement etc.               Achievements & Activities:      Awarded with Bright Beginner Award for learning and implementing business processes quickly.  Awarded with the Star of the month for implementing a PI in Employee Management Application for mangers to monitor employee on-boarding, off-boarding and security quiz statistics.  Received 2 On the Spot award from Manager for consistent performance and before time deliveries.   Got appreciations from the client as well for as our ODC head and manager for the development of a particular Drug report for Pre, Post and During advertise campaign.    Participation in TECHVYOM (Technical College Fest) for continuous 4 years.  Certificate of Honor for being HOUSE CAPTAIN (2010-2011) in School.  Love playing Snooker and listening to Music during spare time.      ACADEMIC CREDENTIALS    B.TECH with specialization in Computer Science & Engineering from SRMSCET(UPTU), Bareilly in 2015 with 70.03%   12th from Bishop Conrad Senior Secondary School (CBSE) in 2011 with 81%   10th from Bishop Conrad Senior Secondary School (CBSE) in 2009 with 88.4%     PERSONAL DOSSIER    Date of Birth                  : 8th December, 1992   Marital Status                :      Unmarried  Languages know           :       English, Hindi               Declaration:     I declare that the information given above is true to the best of my knowledge.                              (Siddhant Rastogi)", "tokens": [{"text": "United Health Group", "start": 2281, "end": 2300, "token_start": 386, "token_end": 388, "entityLabel": "company"}, {"text": "Mar-2019  Till Date", "start": 2313, "end": 2332, "token_start": 393, "token_end": 396, "entityLabel": "period"}, {"text": "Tata Consultancy Services", "start": 2375, "end": 2400, "token_start": 405, "token_end": 407, "entityLabel": "company"}, {"text": "Jan-2016  Mar-2019", "start": 2406, "end": 2424, "token_start": 409, "token_end": 411, "entityLabel": "period"}], "relations": [{"child": 393, "head": 386, "relationLabel": "duration"}, {"child": 409, "head": 405, "relationLabel": "duration"}]}, {"document": "                                                      Srinivas Choudary P                      +91 9980993967    iamnivas@gmail.com    Bengaluru,IN     SUMMARY  15+ years of IT experience.   DevOps in-depth experience of   managing cloud based   technology & effectively   handling configuration &   deployment of infrastructure &   services. Gained hands on   experience in implementing core   DevOps concepts such as   containerization, virtualization,   version control, cloud computing,   administration, load balancing, etc.   by using a wide variety of   technologies while working with   Kubernetes, Operating Systems &   Automation scripting languages.   Drives excellence in every project   to deliver outstanding results.      KEY SKILLS   Containerization  Version   Control  Load balancing    Configuration Management    Cloud Technology Management    Cloud Computing  Microservices    Storage / Server Maintenance    Infrastructure Monitoring    Security & User Administration    Virtualization Configuration    Deployment &   Provisioning  Database   Management  Cloud Networking    Data-center Management      PROFESSIONAL EXPERIENCE   Infrastructure Specialist, IBM  June '19 - Present   Environment: Docker, GIT, Kubernetes, Openshift, Portworx, Helm charts, Jenkins, Flux,   Istio, Jenkins, load balancing, IBM Cloud, AWS, Maven, Terraform, Apache, MySQL, Linux      Appointed to advise multiple teams on virtualization &   containerization solutions   Designed and Deployed Kubernetes/Openshift service in IBM Public Cloud.   Containerize the monolithic Application to micro services.   Building Docker images for Php/MySQL based applications.   Deployed the Portworx(Software Defined Storage) service in IKS.   Pod/deploy/services/PVC/Secrets..etc deployments using YAML files   Helm chart implementation in IBM IKS.   Designed Backup and DR solution using Portworx services.   Configure ETCD database service for IKS.   CI/CD deployment using Jenkins open source software   Automate and Synchronize version control(git) and K8 Cluster with Flux open Source.   Implemented Istio proxy for the micro-services deployment.   Deployment of Application using Maven build tool.   Monitoring the K8 Cluster and Cloud infra using Prometheus.   Deploy VM's and BareMetal Servers using Terraform in IBM Cloud.   Migrating on-premise VMware infrastructure to AWS.    Ending escalations and support major issues, work on RCA's.            Senior System Administrator, Oracle  June '14 - June '19   OMCS Bengaluru, IN   Environment: O C I,   AWS, Azure, Vmware, Linux, Kubernetes,TSM      Storage & Backup   Worked as a Storage Solution Lead to advise multiple teams on Storage & Backup   solutions   Implemented Integration of AWS public cloud as an offsite backup.    Working with Vendors - in Architectural design planning for Software & Hardware   deployments/Price negotiations/Reviewing & Structuring of annual support process.    LCM of Storage migrations / C-DOT upgrades with Customer, Vendor & coordinating   with Team members.   Responsible for end to end automation for monitoring / performance / health check   for Infrastructure.   Managing SAN(Block/ISCSI) and NAS(NFS/CIFS) protocols.   Designing DR strategies based on RTO and RPO parameters considering the   Customers business needs.        TECHNICAL SKILLS   DBMS: MySQL   Languages: Python   Version Control Systems:   Github, Gitlab   Automation/Build ToolsDocker,   Ansible, Flux, Helm    Storage & Backup: EMC,   Brocade, Cisco, Netbackup,   Arcserve, BackupExec,  TSM  Migrating VM's from VMware to AWS.   Implement Code Build, Code Deploy, Code Pipeline of AWS DevOps    Conceptualized & developed solutions for the Development team to build, deploy,   monitor & test applications   Implemented, monitored & reported solutions across a wide range of environments &   platforms   Deploy Kubernetes in EKS, Dockerfile, container registries,   pod/deployment/service/secrets - yaml files      Virtualisation: VM Ware   ESX/ESXi, Docker, Amazon /   Azure / IBM VM, Vagrant  Orchestration tools: OpenShift,   Kubernetes   Cloud Platforms: AWS, Azure,   IBM, Oracle   Microservices: Servicemesh,   Istio mesh   Platforms: Linux and UNIX, iOS,   Solaris, Windows      WORKSHOPS  Attended the AWS, Azure, Docker,   Hashicorps, Portworx Conferences   Gained exposure to the latest   best practices in the world of   Cloud/DevOps/SDS Storage        Senior Consultant, Capgemini  Feb '13 - June'14                Bengaluru, IN   Environment: EMC, VMware, Netbackup, BackupExec, Arcserve   Provide IT Consulting and Solution Design expertise for various Business IT   requirements.   Design solutions which are highly available, scalable and redundant.   Data-center Migration and Transformation. Cloud Computing Initiatives and Cloud   adaptiveness.   Disaster recovery tests performed on half yearly basis using Snap-mirror technology    Advanced experience of vendor tools for managing, configuring, and troubleshooting   like Oncommand Unified Manager/Data Fabric Manager/On-command   Performance/On-command System Manager.      Configuration of De-duplication, thin provisioning.   Datacenter Management - in UK/Malaysia world class DC's.                      Associate Storage ops, EMC2    May 10  Jan 13                   Backup / Storage Administrator              Engineer  System admin, CSC  Apr 06  May 10       NetBackup Administrator      CERTIFICATIONS  Certified Kubernetes Administrator (CKA)  Feb 2021   Microsoft Azure Fundamentals Certified   VMware certified Professional   Solaris certified   ITIL Certified   NetBackup certified      EDUCATION   MCA , Andhra University    GVP College Vizag   Year of pass: 2005   Score: 71%      SUMMARY  KEY SKILLS  OMCS Bengaluru, IN  TECHNICAL SKILLS  WORKSHOPS  CERTIFICATIONS  EDUCATION  GVP College Vizag  ", "tokens": [{"text": "IBM", "start": 1178, "end": 1181, "token_start": 182, "token_end": 182, "entityLabel": "company"}, {"text": "June '19 - Present", "start": 1183, "end": 1201, "token_start": 184, "token_end": 188, "entityLabel": "period"}, {"text": "Oracle", "start": 2476, "end": 2482, "token_start": 426, "token_end": 426, "entityLabel": "company"}, {"text": "June '14 - June '19", "start": 2484, "end": 2503, "token_start": 428, "token_end": 434, "entityLabel": "period"}, {"text": "Capgemini", "start": 4441, "end": 4450, "token_start": 822, "token_end": 822, "entityLabel": "company"}, {"text": "Feb '13 - June'14", "start": 4452, "end": 4469, "token_start": 824, "token_end": 828, "entityLabel": "period"}, {"text": "EMC2", "start": 5259, "end": 5263, "token_start": 965, "token_end": 965, "entityLabel": "company"}, {"text": "May 10  Jan 13", "start": 5267, "end": 5281, "token_start": 967, "token_end": 971, "entityLabel": "period"}, {"text": "CSC", "start": 5368, "end": 5371, "token_start": 983, "token_end": 983, "entityLabel": "company"}, {"text": "Apr 06  May 10", "start": 5373, "end": 5387, "token_start": 985, "token_end": 989, "entityLabel": "period"}], "relations": [{"child": 985, "head": 983, "relationLabel": "duration"}, {"child": 967, "head": 965, "relationLabel": "duration"}, {"child": 824, "head": 822, "relationLabel": "duration"}, {"child": 428, "head": 426, "relationLabel": "duration"}, {"child": 184, "head": 182, "relationLabel": "duration"}]}, {"document": "                                       Microsoft Word - Sudhir Thangarajan- Resume.docx   SUDHIR THANGARAJAN  |LinkedIn: https://www.linkedin.com/in/sudhir-thangarajan-6441476/  |sudhirthangarajan@gmail.com |+91 9108289844|      1   Key Competencies  P & L Management| Voice| Data Products| Channel Sales| Retail Sales| Mass Market Sales| Franchisee  Management| Enterprise  account Management| Vas selling| Mobile & TV Content Sourcing & Distribution|  Telecom infrastructure Solutions| Sales operations| Own Sales life cycle| CXOs relationship |Team player|  Multi-tasking| Project Management| Crisis management|  Education    Bachelors Degree in Business Management  Bangalore university  Certification & Training    PGDSM (Marketing)  - Symbiosis distance learning Pune.   Diploma in Sales & Marketing - NIS Sparta   The fundamentals of Digital Marketing  Google  March 2021- Credential Id- 2X9 MTR U27   Google Analytics for Beginners  Google  March 2021 to March 2024   Godrej & Boyce ( Modular furniture division) Successfully completed one year of on job training   Bangalore   Synopsis     Accomplished sales professional with over 20 yrs. experience in Telecom, ISP & Banking across diverse  territories.     P&L responsibility for different business verticals under telecommunications.   Specialist in devising calculated and strategic business solutions, supported by market analytics with   detailed planning and execution.   With experience in implementing marketing, sales and commercial programs to maximize revenue and   market share.    Team player with great business acumen, capable of connecting, building and managing CXOS   relationship.   Goal driven, with proven track record of excellence in sales and operations in India, Afghanistan and   Nigeria.  Key Projects     Afghanistan: Led the technology conversion of Analog to digital cable technology, with sourcing of  vendors for hardware, established distribution partnerships with broadcasters likes of Zee, Star TV,  Times Now, NDTV, Viacom, OSN and discovery.    Afghanistan: Piloted additional run through of fiber infrastructure in Kabul from 120 Kms to 330 Kms.   Afghanistan: Steered fiber connectivity for Roshan (TDCA) for their 16 sites in Kabul, establishing   complete eco system. Successfully integrated fiber infrastructure solutions with Huawei managed  solutions.    Afghanistan: Executed 220 kms of fiber infrastructure connectivity for Etisalat Afghanistan.   Afghanistan: Led the launch of  ISP business through network planning, implementation , coordination   and execution of rollout projects.   Afghanistan: Executed VAS projects (CRBT/MOD/Infotainment services across operators) with   upstream revenue generation.   Nigeria: Re-structuring the Mass market channel for Prepaid from a Dealer model to a distribution   based model and restructuring sales team compensation to performance-based-compensation.   Davanagere: Created 360 degree sales ecosystem for reliance infocomm products & driven, established   franchisee model.   Bangalore: Re-structured post-paid retail distribution with setting up of  end to end process for Reliance   Communications.   Bangalore: Established direct sales channel model for Spice communications Ltd with strength of  200   feet on street..    Work Experience  DIRECTOR  MANAGED INFRASTRUCTURE SERVICES - TELECOMS, July 2013 to 31st Oct 2020 - ARIF AZIM  LTD., Kabul, Afghanistan     SUDHIR THANGARAJAN  |LinkedIn: https://www.linkedin.com/in/sudhir-thangarajan-6441476/  |sudhirthangarajan@gmail.com |+91 9108289844|      2    Scaled and driven Fiber infrastructure  and managed solutions business through annual contracts with  telecom operators, ISPs  and enterprise customers, along with network planning, implementation ,  coordination and execution of rollout projects.     Identified buildings-of-interest (BoIs) through extensive research and analysis of data of commercial  and residential occupants for assessment of internet business potential and connectivity planning    Devised end-to-end telecom (voice-data-video) solutions for enterprises, from designing, creating  proposals, bidding, consultative selling and key account servicing.    Established sales channel retail sales for Internet and digital cable Products. Also developed Process  for identification of Market Segments for Digital cable, internet and VAS products.    Source content for VAS products, with upsell to operators, ensure top line revenue targets are  achieved.    Create products for  VAS, digital cable TV and internet business, which translates in to revenue  generation.   REGIONAL MANAGER  HOME LOANS - SALES, Sept. 2010 to April 2012 - ICICI Bank Ltd., Bangalore, India   Established  direct sales channel ecosystem for home loan products.   Recruit, train and deploy feet on street for sourcing of business.   Pioneered the establishment of strategic alliances / tie-ups with A/B/C builders and developers for   business.   Drive & Scaled home loan business revenues for ICIC direct.   Plan and implement innovative business sourcing methods for sales channels.    ZONAL MANAGER  ENTERPRISE SALES, Nov. 2009 to March 2010- Tata Teleservices Ltd., Bangalore, India   Established  direct /dealer sales channel ecosystem for  enterprise products.   Recruit, train and deploy feet on street for sourcing of business.   Drive & Scaled Docomo Post-paid & data products nos for teams.   Plan and implement innovative business sourcing methods for direct/dealer sales channels.    REGIONAL SALES MANAGER  CHANNELS - June 2006 to Sept. 2009 - Multilinks Telkom Limited, Lagos, Nigeria   Direct, monitor and lead the development of  business plan, sales strategies, projects and budget in   line with corporate strategy.    Own and drive market share for voice products.   Pivotal in re-structuring the Mass market channel for Prepaid from a wholesale model to a distribution   based model and restructuring sales team compensation to performance-based-compensation.   Developed the KRAs and KPIs for distributors and implemented a monthly tracker to update the   management on the performance of the regions at a granular level.   POST PAID DISTRIBUTION LEAD, April 2003 to June 2006 - Reliance Infocomm Limited, Bangalore, India    Accomplished launch of RIM Prepaid in Davanagere by way of identifying and appointing channel   infrastructure set-up, manpower mapping, channel mapping, market survey & analysis, pre-launch  activities, distribution, etc.    Instrumental in increasing the Bangalore Post-paid distribution market share by 40%, drove the end- to-end customer life-cycle management including sales, service and collections.   CHANNEL SALES EXECUITIVE, April 2001 to March 2003  Spice Telecom Limited, Bangalore, India    Drive Retail/dealer team to achieve month on month post-paid nos.   Recruit/ train/deploy Feet on street for Direct/dealer team.   Instrumental in increasing the post-paid  market share by 50 % for dealer channel.   CHANNEL SALES OFFICER, June 1999 to Dec 2000  DSS Mobile Communications Limited, Bangalore, India    Drive Retail/dealer team to achieve month on month targets.   Recruit/ train/deploy Feet on street for Direct/dealer team.     PERSONAL DETAILS    |DOB:11.11.1975|Marital Status: Married | Bangalore|    ", "tokens": [{"text": "ARIF AZIM  LTD", "start": 3381, "end": 3395, "token_start": 557, "token_end": 560, "entityLabel": "company"}, {"text": "July 2013 to 31st Oct 2020", "start": 3352, "end": 3378, "token_start": 550, "token_end": 555, "entityLabel": "period"}, {"text": "ICICI Bank Ltd", "start": 4668, "end": 4682, "token_start": 775, "token_end": 778, "entityLabel": "company"}, {"text": "Sept. 2010 to April 2012 ", "start": 4641, "end": 4666, "token_start": 769, "token_end": 775, "entityLabel": "period"}, {"text": "Tata Teleservices Ltd", "start": 5156, "end": 5177, "token_start": 868, "token_end": 871, "entityLabel": "company"}, {"text": "2009 to March 2010", "start": 5136, "end": 5154, "token_start": 864, "token_end": 868, "entityLabel": "period"}, {"text": "Multilinks Telkom Limited", "start": 5567, "end": 5592, "token_start": 945, "token_end": 947, "entityLabel": "company"}, {"text": "June 2006 to Sept. 2009", "start": 5541, "end": 5564, "token_start": 939, "token_end": 943, "entityLabel": "period"}, {"text": "Reliance Infocomm Limited", "start": 6210, "end": 6235, "token_start": 1063, "token_end": 1065, "entityLabel": "company"}, {"text": "April 2003 to June 2006", "start": 6184, "end": 6207, "token_start": 1057, "token_end": 1061, "entityLabel": "period"}, {"text": "Spice Telecom Limited", "start": 6717, "end": 6738, "token_start": 1157, "token_end": 1159, "entityLabel": "company"}, {"text": "April 2001 to March 2003", "start": 6691, "end": 6715, "token_start": 1151, "token_end": 1155, "entityLabel": "period"}, {"text": "DSS Mobile Communications Limited", "start": 7022, "end": 7055, "token_start": 1223, "token_end": 1226, "entityLabel": "company"}, {"text": "June 1999 to Dec 2000", "start": 6999, "end": 7020, "token_start": 1217, "token_end": 1221, "entityLabel": "period"}], "relations": [{"child": 1217, "head": 1223, "relationLabel": "duration"}, {"child": 1151, "head": 1157, "relationLabel": "duration"}, {"child": 1057, "head": 1063, "relationLabel": "duration"}, {"child": 939, "head": 945, "relationLabel": "duration"}, {"child": 864, "head": 868, "relationLabel": "duration"}, {"child": 769, "head": 775, "relationLabel": "duration"}, {"child": 550, "head": 557, "relationLabel": "duration"}]}, {"document": "Sujith G Pillai  +1 647 382 6124 | sujith.gp@gmail.com      HIGHLIGHTS:     PMP, ITIL Certified Professional with 11+ years of experience as Business Systems Analyst and Software engineer, primarily in Telecom, Finance, Life Sciences and Healthcare Industry.  Expert in Application support processes and handling of ITSM tickets.  BI & Data Analyst Informatica, Abinitio, Datameer, Tableau and TIBCO Spotfire tools.  Excellent communicator with exceptional analytical, people management, relationship management and coordination skills.  Conceptually strong, possessing an analytical & innovative approach towards work with an eye for detail along with the ability to quickly learn new concepts & technology.  Proficient in SQL, PL/SQL, ETL Tools like Informatica, Abinitio and Datameer.  Hands on experience on Waterfall, Agile/Scrum project management methodologies, involved in all phases of Software Development Life Cycle from initiation to planning, analysis, design and development, testing, user training, deployment and post Go-live support and maintenance.  Worked with Investment SMEs, Financial Analysts as well as other business teams on requirements gathering to write the Business Requirements Documents.  Involved in smoke, integration, system, data redundancy, security, performance, user acceptance testing, end to end, and ETL workflows testing.  Experience in Investment Operations and data needs through the trading lifecycle.    Functional & Technical Skills:     Tools    Ab-Initio ETL, Informatica, Tableau, TIBCO Spotfire, Datameer  DBMS    Oracle, Teradata, SQL Server, IBM Netezza, Apache Hive (Hadoop)  Programming Languages C#, Java Script, Python Basics  Internet Tools and Packages HTML, XML, MS Office, Visio, SSIS, SSRS, VSS  Methodologies   Agile, Waterfall & SDLC  ITIL    Incident, Problem, Change, Configuration and Release Management    CERTIFICATION:        Project Management Professional (PMP), ITIL V3 Foundation Certified, Accenture Life Sciences and Oracle Certified Associate.    PROFESSIONAL EXPERIENCE:    Senior Consultant Business Systems Analyst, ProjectX Limited.                                          December 2019  Till Date    Client: Bell Canada, is one of the Leading Telecom Industry in Canada.    Led client requirements gathering workshops, documented business, and system requirements.  Participated in the solution design and documented the required design and design changes.  Facilitated system development.  Areas covered: IVR, Speech, Voice Biometrics, Routing, Chat, Email, Click to Talk, Agent Desktop and Outbound Dialer  Led and executed DIT testing and facilitated the defect resolution, by not only performing tests but also by analysis of the solution code, system logs and backend data.  Provided assessment of the system changes, facilitated the costing and planning of the change requests.  Provided 2nd level triage for client production support.      Team Lead, Accenture Solutions Private Limited                                                               June 2009  September 2019  (Accenture is amultinationalprofessional servicescompany that provides services in strategy, consulting, digital, technology and operations)    Client: BMO Financial Group, is a Canadian multinationalinvestment bankandfinancial services. One of theBig Fivebanks in Canada, it is the fourth-largest bank in Canada by market capitalization and assets, as well as one of the ten largest banks in North America.    Gathered requirements through interviews, storyboarding, wireframes and user stories techniques.  Performed GAP Analysis to analyze the difference between the current systems and the to-be system.  Worked with capital market instruments like Derivatives, OTC, Swap, Equity, Equity- borrowing and lending, Options, Futures, Bond/fixed incomes, Regulatory reporting, payment application, Risk Management, Inventory, Repo positions.  Was effective in translating the business requirements into use cases, pseudo code, technical specification, process models and data modelling diagrams so that the technical team can easily understand and develop a quality software.  Proficient in SQL and experienced in using statements to pull data.  Performed data modeling and source system target mapping of data flows  Was successful in performing incident investigation through data collection and data analysis,  Worked collaboratively with the User Experience Analysts and System Analysts to help document custom and system modifications.  Developed Traceability Matrix for maintaining/updating current and new system requirements using MS Excel.    Client: Blue Cross Blue Shield Association is a federation of 36 separate United States health insurance organizations and companies, providing health insurance in the United States to more than 106 million people.    Worked alongside agile development teams to help improve completeness, accuracy and timeliness of datasets   Identified data relevant for the analysis based on the inputs from Domain leads and Technology Leads.  Analyzed and documented the system specifications, Business requirements, documenting, tracking and analyzing the requirements.  Collaborated with the project team to define and create test strategy to test critical enterprise wide data-warehousing applications.  Scripted reusable SQL's for testing Reports & ETLs.  Created inputs for daily status reports and scheduled defect resolution meetings daily during testing phase with development team.  Participated in creating detailed test plan and detailed test cases for each units merger.  Actively participated in scrum meetings, sprint planning meeting, retrospective meeting, reviews and developed test scenarios.    Client: Sanofiis a Frenchmultinationalpharmaceutical companyheadquartered in Paris,France, one of  the world's largest byprescriptionsales.    Defined the project scope and deliverables with Client Analysts and Project Manager  Managed change requests related to design deviations or areas not covered of existing applications  Led the ETL development and testing team to achieve the target deliverables on time.  Responsible for all the production support activities  Worked on the Incident, Change, Problem and Release Management tickets in IT Service Management tool.  Co - ordinated with Onsite team for estimation and project delivery  Co-ordinated with Quality Assurance group and adhere to the quality delivery  Provided cost effective and timely on-boarding for Business Stakeholders to our ITSM systems.    EDUCATION:    M.B.A (Master of Business Administration)  International Business               Pondicherry University, India    B.E (Bachelor of Engineering)              Electrical & Electronics Engineering             Anna University, India  Saudi Aramco: Company General Use      Saudi Aramco: Company General Use", "tokens": [{"text": "Bell", "start": 2192, "end": 2196, "token_start": 390, "token_end": 390, "entityLabel": "company"}, {"text": "December 2019  Till Date ", "start": 2156, "end": 2181, "token_start": 382, "token_end": 388, "entityLabel": "period"}, {"text": "Accenture Solutions Private Limited", "start": 2942, "end": 2977, "token_start": 527, "token_end": 530, "entityLabel": "company"}, {"text": "June 2009  September 2019", "start": 3040, "end": 3065, "token_start": 532, "token_end": 536, "entityLabel": "period"}], "relations": [{"child": 382, "head": 390, "relationLabel": "duration"}, {"child": 532, "head": 527, "relationLabel": "duration"}]}, {"document": "                                        PROJECT MANAGER: Experienced Project Manager with a Postgraduate Certificate in Business Management from XLRI, Jamshedpur.  Adroit, diligent & detail oriented professional with an experience of 20+ years, in Team Management, Coaching, Customer Service Transaction banking, Messaging, High value payments, Clearing, Trade Finance, Loans & Mortgages and Lean Tool across Insurance, Banking & Consumer Service Industry  Relied on as a key advisor in driving global, multibillion-dollar growth; gains in customer loyalty; and record-setting profit improvements.  CERTIFIED DATA SCIENTIST:  Providing data-driven, action-oriented solutions to challenging business  problems  Business-minded data scientist with a demonstrated ability to deliver  valuable insights via data analytics and advanced data-driven methods.  Strong problem solving skills, business acumen, and communication skills.  Experience in driving growth of people and of the firm. Owning client  communication and project delivery.  Proven experience in working with multiple partner groups to drive longer  term strategies within a matrix-based organization  SUMIT KUMAR BEPARI  Education Details:  Project Manager  Service Delivery   CSM |ITIL |SCRUM MASTER  Data Science Manager  Data Scientist | Data Analytics   Personal Details:  +91-96325 33552  sumit.bepari@gmail.com  Bangalore-560048, India  www.linkedin.com/in/ sumitkumarbepari/  Executive Summary:  Technology  Skills:  Programming /   Scripting Python, SQL  Data   Visualization  Seaborn, Matplotlib, Tableau  Database/ Data   Engineering  DB2, SQL, Data pre-processing, Data cleaning,  exploratory data analysis  Machine   Learning  Linear/Logistic regression, Decision Trees, Support  vector machines, Random forest, XG Boost, K-  Nearest Neighbors  Big Data Hadoop, Pig, Hive, Pyspark  Cloud Google Cloud, Dockers, Kubernetes  Statistical   Knowledge   Normal distribution, Binomial Distribution,  Variance, Covariance, Correlation, Probability,  Hypothesis Testing  Z Test, one tail tests, two tail  tests, Regression, Control Charts,  Storyboarding /   Insight   Generation   Have been awarded monetarily for bringing more  business from the existing customer  Certification:  Post Graduate Certificate in  Business Management XLRI, Jamshedpur July 2011 - June 2012  B.E - Mechanical Engineering Utkal University Bhubaneswar, India September 1996 - June 2000  Data Science Course Springboard  July 2019 - May 2020  Certified Scrum Master Scrum Alliance  Functional   Knowledge   Banking  Transaction Banking, Messaging, High Value  Payments, Equities, Derivatives, Custody, Trade Finance,  Loans & Mortgages, Clearing, Cheque Processing  Consumer Services  Promotion & Campaigns, Billing  Global   Exposure India, Germany, Spain & Singapore  Stakeholder   Management  Have managed clients and have been single point of  contact/escalation for the portfolio that I held/managed  Team   Management   Have led teams of team size of 50+ members of various  nationalities working across various geographies and  various shifts    Received the Achievers League Award || January 2020  Achievers League Award given by HCL for getting the highest appraisal rating for more than two consecutive years  Career Journey:  Awards:  Have offshored teams from onsite without delivery slippage and hence have brought down the cost of the project to 30%  30%  Technology Leadership Benchmarks  Have implemented Lean in my project  resulting in optimum distribution of resources across shifts as well as reduction of resources from 30 to 23. Have implemented knowledge management and cross training resulting in better performance of team and reducing cost and attrition  23  Have been in Achievers League in HCL  awarded for getting the highest appraisal rating for more than 2 consecutive years  2 Have received numerous appreciations from the customers for 15+ projects for bug free delivery15  HCL  Technologies Ltd.  Bangalore Project Manager  Jul19 - Current  HCL  Technologies Ltd.  Bangalore Consultant  Jan19  Jun19  HCL  Technologies Ltd.  Stuttgart, Germany Service Delivery   Manager/Consultant  Jan18  Dec18  HCL  Technologies Ltd.  Bangalore Service Delivery Manager  Feb09  Dec17  HCL  Technologies Ltd.  Singapore Project Lead  Jan06  Jan09  HCL  Technologies Ltd.  Chennai Sr. Software Engineer  Mar03  Dec05  Patni  Mumbai Software Engineer  Aug00  Feb03  Projects:  DATA SCIENCE PROJECT 1 - House Price Prediction Oct. 2019 - Dec. 2019  Problem Statement: This project involved the prediction of the house price in the USA based on the 80 odd features such as number of rooms, garage area, location, nearness to schools, quality of material used, type of flooring, type of ceiling, walls, etc.  Tools Used for Data Visualization: Data Visualization techniques such as bar plots, scatter plots, sale price distribution diagram, correlation matrix heat map to find out the features that are highly interrelated to each other. Used Box Cox transformation on highly skewed features to transform the non-normal features to normal  Machine Learning Models Used: Used various machine learning models such as Linear Regression, Ridge Regression & Lasso regression to predict the house price. Compared the root mean squared error (RMSE) to find out the model that fits best. Found Linear Regression to be the best fit based on the its lowest RMSE  DATA SCIENCE PROJECT 2 - Sales Price Prediction for Rossmann Stores Jan. 2020 - Mar. 2020  Problem Statement: Predicting the Sales for Rossman stores that had datasets consisting of around 18 features such as promotion, store type, state holidays and school holidays when the stores are open or closed, competition with other stores, etc.  Tools Used: Used various data visualization techniques such as bar graphs, histograms, scatter plots to find the relationship between various features and sale price. Used correlation matrix heat map to find the relationship between various features. Removed features where they had a correlation more than 0.8. Categorical various were converted to numerical ones  Machine Learning Models Used: For prediction of the Sale price, used 4 machine learning models -- Linear Regression, Decision Tree, Random Forest & Xg boost . In Random Forest and Xg boost, I used Randomized Search CV for hyperparameter fine tuning. Calculated the Root Mean Squared Error (RMSE) of all the above models to arrive at the final prediction. Xg boost model gave the lowest RMSE and hence the prediction using this model was the best among the four models    Previous Professional Experience:  HCL Technologies Ltd. | Bangalore | Consultant  January 2019 - June 2019Key Accountabilities:   Analyzing the customers Mainframe application portfolio to figure out MIPS reduction opportunities   Analyze and identify jobs that has high MSU usage and propose ways to minimize MSU usage  Analysis of various JCLs and propose for ZIIP enablement  Analyzing various JCL and identify Jobs/programs for query parallelism and query optimization  Analyzed some special applications and came up with the interface diagrams for these  applications  Analyzed the complete mainframe application landscape of the customer and came up with an  inventory of Jobs, programs, procs, parmlibs, etc.  MIPS  reduction  Minimize MSU usage  Query  optimization  Analyzing  HCL Technologies Ltd. | Stuttgart, Germany | Service Delivery Manager/Consultant  January 2018 - December 2018Key Accountabilities:   Responsible for the overall project delivery for the client  Coordinating and managing all stakeholders of the project  Business, IT, Senior Management  and SPOC for all parties  Managing Escalations from both client and internal teams  Team Management of cross functional and cross-cultural teams, conflict resolution, goal  setting, appraisal and feedback of the team  Extracting data from DB2 database for data migration. Scheduling jobs for weekly delivery of  data to the customer for UAT  Interacting infrastructure, release management, change management, QA, DBA and application teams  Overall Project  Delivery  Managing  Stakeholders  Team  Managing  Extracting Data  Professional Experience:  HCL Technologies Ltd. | Bangalore | Project Manager  Key Accountabilities:   Driving a bank wide program named Project Sully. Sully program deals with migration of 64 Nostro accounts in 64 countries spread across Americas, Europe, Middle East, Asia & Asia Pacific   Ensuring co-ordination between various stakeholders such IT teams of multiple applications, Bank Operations team in all these countries   Planning UAT for all these branches and ensure seamless UAT for the migration  Regular working group calls and follow up calls to ensure are up to speed  Highlighting risks and issues to the senior management  Planning out the migration activities for the various teams and ensure that those activities are done  Ensuring the migration is done smoothly followed by post migration follow ups  Driving a bank  wide program  Co-ordination  Planning UAT Highlighting   risks & issues  July 2019  Current  HCL Technologies Ltd. | Bangalore | Service Delivery Manager  February 2009 - December 2017Key Accountabilities:   Delivering & implementing the projects as per scheduled milestones; conducting review meetings to ensure timely completion and delivery of the project to the client   Spearheading project planning activities including Schedule Development, Cost Estimate & Budgeting; ensuring maximum efficiency in resource allocation across projects   Carrying out various functions related to resource management, process implementation & improvement and client relationship management, technical support and liaison & co- ordination with different departments   Coordinating with the different project stakeholders including Customers, Senior Management, & other Vendors   Monitoring overall progress of the project, resolving issues and initiating corrective actions if needed   Involved in Application Support & Maintenance (ASM): application of Lean principles to ASM; process & performance improvements and productivity benefits   Identifying improvement areas in incident response and problem management process   Leading, training & monitoring the performance of team members to ensure efficiency in IT Operations  Delivering &  implementing  Spearheading  Project  Carrying out  various   functions  Relationship  Management  ASM Monitoring  Resolving  issues  Team player    HCL Technologies Ltd. | Chennai | Sr. Software Engineer  March 2003 - December 2005Key Accountabilities:   Analyzing the business requirements and transforming the requirements to Functional design specifications   Design, code, unit test, debug of the code and produced high quality deliverables with least number of defects   Development of both online and batch programs with database connectivity like CICS-DB2 and COBOL-DB2   Interacting with Business for getting the requirements for User acceptance testing and have a good rapport with Client partners  Functional  design   specifications  High quality  deliverables  Online and  batch   programs  Interacting  Patni | Mumbai | Software Engineer  August 2000 - February 2003Key Accountabilities:   Analyzing the business requirements and transforming the requirements to Functional design specifications   Design, code, unit test, debug of the code and produced high quality deliverable with least number of defects   Coding of programs with database connectivity like COBOL-IDMS, ADSO & JCL   Providing Support for batch job failures  Coding of  programs  Database  connectivity  All the information mentioned in the resume is correct to the best of my knowledge and belief.  Date:  Place :  Declaration:  HCL Technologies Ltd. | Singapore | Project Lead  January 2006 - January 2009Key Accountabilities:   Analyzing the business requirements and transforming the requirements to Functional design specifications   Design, code, unit test, debug of the code and produced high quality deliverables with least number of defects   Development of both online and batch programs with database connectivity like CICS-DB2 and COBOL-DB2   Interacting with Business for getting the requirements for User acceptance testing and have a good rapport with Client partners  Delivering &  implementing  Spearheading  Project   ", "tokens": [{"text": "HCL  Technologies Ltd", "start": 3951, "end": 3972, "token_start": 712, "token_end": 716, "entityLabel": "company"}, {"text": "Jul19 - Current", "start": 4002, "end": 4017, "token_start": 721, "token_end": 723, "entityLabel": "period"}, {"text": "HCL  Technologies Ltd", "start": 4019, "end": 4040, "token_start": 725, "token_end": 729, "entityLabel": "company"}, {"text": "Jan19  Jun19", "start": 4065, "end": 4077, "token_start": 733, "token_end": 735, "entityLabel": "period"}, {"text": "HCL  Technologies Ltd", "start": 4079, "end": 4100, "token_start": 737, "token_end": 741, "entityLabel": "company"}, {"text": "Jan18  Dec18", "start": 4161, "end": 4173, "token_start": 752, "token_end": 754, "entityLabel": "period"}, {"text": "HCL  Technologies Ltd.", "start": 4175, "end": 4197, "token_start": 756, "token_end": 759, "entityLabel": "company"}, {"text": "Feb09  Dec17", "start": 4235, "end": 4247, "token_start": 766, "token_end": 768, "entityLabel": "period"}, {"text": "HCL  Technologies Ltd", "start": 4249, "end": 4270, "token_start": 770, "token_end": 774, "entityLabel": "company"}, {"text": "Jan06  Jan09", "start": 4297, "end": 4309, "token_start": 779, "token_end": 781, "entityLabel": "period"}, {"text": "HCL  Technologies Ltd", "start": 4311, "end": 4332, "token_start": 783, "token_end": 787, "entityLabel": "company"}, {"text": "Mar03  Dec05 ", "start": 4366, "end": 4379, "token_start": 794, "token_end": 798, "entityLabel": "period"}, {"text": "Patni", "start": 4380, "end": 4385, "token_start": 798, "token_end": 798, "entityLabel": "company"}, {"text": "Aug00  Feb03", "start": 4413, "end": 4425, "token_start": 804, "token_end": 806, "entityLabel": "period"}], "relations": [{"child": 721, "head": 712, "relationLabel": "duration"}, {"child": 733, "head": 725, "relationLabel": "duration"}, {"child": 752, "head": 737, "relationLabel": "duration"}, {"child": 766, "head": 756, "relationLabel": "duration"}, {"child": 779, "head": 770, "relationLabel": "duration"}, {"child": 794, "head": 783, "relationLabel": "duration"}, {"child": 804, "head": 798, "relationLabel": "duration"}]}, {"document": "TANMAYA MISRA  Current  Address -  834, Ashirwad block, eldeco-2, sector-3, lucknow-226025  Mobile number-8861671402  Email id-misratanmay25@gmail.com                                                                     Professional Summary  Having almost 6 years' experience in IT industry in which 4 years' experience in Big data technologies.  Working as Technical Lead in HCL Technologies, Bangalore  Worked as Senior Software Engineer in Mobileum India Pvt Ltd., Bangalore from September 2017-August 2020.  Worked as Software Engineer in Ness Technologies, Bangalore from August 2016- July 2017.  Worked as Project Engineer in CDAC, Bangalore from September 2014-August 2016  Hands on Experience in Java, Spark Streaming, Spark Core, Spark Sql, Spring, Hibernate.  Experience in schedule Hadoop jobs using Oozie  Experience in writing code to process large amount of structured, semi structured data.  Experience in Hive queries and integrating Hive with Spark.  Experience in Oracle RDMS.  Worked on Production support for fine tuning spark jobs running in production server.  Hands on experience in Python.  Experience in migrating Java and RDMS based code to Spark and Hive so that large data can be processed easily.  Experience in processing file in csv and orc format.                                                                          Project Details    Project Name: Network Security Passive    Project Detail: Mobile Network Security, Detection module has been designed for mobile operators to monitor and investigate unwanted or malicious SS7 traffic that may affect their network.    Role: Team member (Developer)    Responsibilities:  Transforming data using Spark Sql and loading into Hive table.  Developed UDF in Spark to identify fraud patterns.  Monitor the job and rectify the issue in case of any failure work on the site.  Writing Oozie workflow for scheduling jobs on hourly basis.  Writing Python script for logging and calling spark job.    Technologies: Java, Hive, Oozie, Hadoop, Spark Sql, Python      Project Name: Network Traffic Redirection    Project Detail: Network Traffic Redirection is application that processes telecom data and calculates various kpi like amount of data used by customer for particular network etc.    Role: Team member (Developer)    Responsibilities:  Transforming data using Spark Sql and loading into Hive table.  Monitor the job and rectify the issue in case of any failure work on the site.  Migration of Java and Oracle based code to Spark-Hive.  Writing Oozie workflow for scheduling jobs on hourly basis.  Writing Python script for logging and calling spark job.  Co-ordinating with doc team for documenting configuration.  Writing Spark sql for calculating unique imsi on hourly basis.  Writing Spark sql  for aggregating hourly data on daily basis.    Technologies: Java, Hive, Oozie, Hadoop, Spark Sql, Python        Project Name: Excaliber    Project Detail: Excaliber is an application that enables CTFS to manage by keeping records of the transaction for their customers and building model on that and recommending ads and detecting possible defaulters.    Role: Team member (Developer)    Responsibilities:  Writing Spark Sql  to pull data from hdfs and push data to hive.  Writing Hive queries for transferring data from temporary table to external table.  Writing Spark Streaming code to process real time data.    Technologies: Hive, Hadoop, Java, Spark Sql, Spark Streaming, Oozie.          Project Name: eGAP-Forum    Project Detail: eGAP-Forum was forum on which students, teachers can discuss on certain topics and student can post queries that can be answered by teachers or fellow students.    Role: Team member (Developer)    Responsibilities:  Writing Java code that reads data in csv format or from oracle database which can be displayed at GUI side.  Writing Java code to push data to oracle database     Technologies: Java, Spring MVC, Hibernate, Oracle.", "tokens": [{"text": "CDAC", "start": 631, "end": 635, "token_start": 98, "token_end": 98, "entityLabel": "company"}, {"text": "September 2014-August 2016", "start": 652, "end": 678, "token_start": 102, "token_end": 104, "entityLabel": "period"}, {"text": "Ness Technologies", "start": 542, "end": 559, "token_start": 82, "token_end": 83, "entityLabel": "company"}, {"text": "August 2016- July 2017", "start": 576, "end": 598, "token_start": 87, "token_end": 90, "entityLabel": "period"}, {"text": "Mobileum India Pvt Ltd", "start": 442, "end": 464, "token_start": 65, "token_end": 69, "entityLabel": "company"}, {"text": "September 2017-August 2020", "start": 482, "end": 508, "token_start": 72, "token_end": 74, "entityLabel": "period"}], "relations": [{"child": 72, "head": 65, "relationLabel": "duration"}, {"child": 87, "head": 82, "relationLabel": "duration"}, {"child": 102, "head": 98, "relationLabel": "duration"}]}, {"document": "                                               Vinoth Kumar V   Hadoop Developer     A collaborative engineering professional with substantial knowledge and experience in analysis, design, development,   manage and implement of large Data-warehouse and Big Data systems by creating an intuitive architecture that helps   organizations effectively capture, store, process and analyze huge volume of structured, semi structured, unstructured   and stream of heterogeneous data set.        vinothmail20@gmail.com     9790768638      github.com/vinothv-cloud/Spark       https://scuttle1.s3.ap-south-1.amazonaws.com/Resume/Vinoth_Hadoop.pdf   PROFESSIONAL SUMMARY       Having 5+ years of Professional experience, involved in Developing, Implementing, configuring, testing  Hadoop Ecosystem   components and maintenance.       Around 2 years of experience as Hadoop Developer with sound knowledge in Hadoop Ecosystem technologies.      Hands on experience in Python, Scala, and Spark programming.      Experienced in working with Apache Hadoop components like Spark, HDFS, Oozie, HBase, Yarn, Hive and  Impala.       Experience in Linux shell scripting      Developed data pipeline using PySpark and with Cloud Storage - AWS technologies such as S3 storage service,  Glue, ETL, trigger, , RDS and Athena       I have exposure Retail, HealthCare insurances with sound knowledge in handling streams as Batch, In-memory  and Real-time Streaming etc.       Ability to perform at a high level, meet deadlines and adaptable to ever changing priorities.             WORK EXPERIENCE            Hadoop Developer          HTC Global Service, Chennai, TN            Mar 2020  Present             Systems Centralized Analytics (SCA) - State Farm      Systems Centralized Analytics (SCA) is a unified, scalable, log     management and analytic solution. It is a collection of the Hadoop   infrastructure, SCA log management and Analytic platform and the   solution the SCA clients have built on them to gain insight from the   log data. Log data is brought into the platform from numerous log   sources and available to enable users to explore, navigate, analyze   and visualize data.           Environments/ Technologies:            Hadoop Cloudera, Hadoop Stack (Oozie, Spark, HBase, Hive, Impala, HDFS), AWS, Scala and Linux                          mailto:vinothmail20@gmail.com https://github.com/vinothv-cloud/Spark         Responsibilities:       Gathered business requirements from Business partners and subject matter experts.      Implemented solutions to enable logs streaming from sources using logstash and elastic search including  UDP, TCP, Http and JDBC input plugins and filters.       Implemented solutions to enrich and index logs such as VPC flow logs using S3 Elastic search in AWS.      Developed ETL jobs in spark scala and python for log data parsing and enrichment from various log file  formats such as text, json, xml and csv.       Developed programs in Spark Scala to process log data in HDFS for various operations and store them in  Hadoop compatible file formats like Avro, parquet and json.       Designed and developed shell scripts and Oozie pipelines for log data process scheduling.      Did performance tuning, benchmarking, followed best practices in the development of spark jobs such as  memory management cache/ persist , Partitioning, broadcast variables, UI monitoring etc.,       Handled target systems such as Cloud Storage (Amazon s3 bucket), NoSQLs, Hive, RDBMS, serialized  compressed datasets.       Processed Spark data write into s3 bucket as objects, and created tables in amazon Athena for data analysis.      Support and monitor various Hadoop applications and pipelines.      Supporting clients with business analysis, documentation and data modelling.      Contributed to internal activities for overall process improvements, efficiencies and innovation.      Developed, implemented, supported and maintained data analytics protocols, standards and documentation.        Associate Consultant   Virtusa Consulting Service Pvt ltd, Chennai, TN    Mar 2019  Mar 2020   Health First Insurance   Responsibilities:       Importing and Exporting data into Hive using sqoop      Created and worked sqoop jobs with incremental load to populate hive external tables.      Direct mode, split by, Boundary query and batch mode have been used to improvise the tuning options  using sqoop.       Developed hive queries for user requirement to perform adhoc analysis including analytics queries such as  row number, rank, explode etc.       Developed UDFs in scala as and when necessary to use in hive queries.      Improvising the tuning options using HIVE functions such as Index, Partitioning and Bucketing.      Performance optimization in hive can be achieved by using Optimized joins, Parallel exec and  Vectorization.                  Loaded the data into spark RDD and do in memory data computation to generate the output response  optimizing in Hadoop using spark Context, Spark SQL , Data frames and pair RDDs.       Using Spark handled different sources such as web services, Cloud (amazon s3 bucket), message queues,  file systems, NoSQLs, DBs, Applied transformation logics such as join, lookup, enrichments, aggregation,   filtering, conversions, concatenations etc.       Fetching data from server in real time using NIFI and placing it in Kafka with appropriate replication and  partition.       Created Automated coordinator, workflow and action involving various ecosystems using OOZIE Possess  strong client facing skills, delivery focused and delivering great customer Experience and proficient in   managing multiple cross-functional teams /projects       Created and published different visualizations and dashboards in Kibana by loading continuous and batch  data into Elastic search using ES storage handlers.         Developer          SCIO Health Analytics, Chennai, TN                  May 2017  Feb 2019                      SCIO Mine - Humana Health insurance              Responsibilities:       Hands on Experienced in Database programming using Oracle PL/SQL.      Involved in writing PL/SQL Stored Procedures, Functions, Cursors, Triggers and Package.      Written complex SQL (Sub queries and Join conditions), in PL/SQL      Troubleshoot and debug system and/or data errors.      Proficient knowledge in performance tuning and knowledge to work according to the      Vast Experience in Oracle advance SQL Programming Using Analytical functions, Sub Queries, indexes  and Set Operators.       Managed performance and tuning of SQL queries and fixed the slow running queries in production      Development of new Reports / Dashboards using Tableau and relevant data source      Experience with the design and development of Tableau visualization solutions      Prepare data sets for various internal and external practices and products - to be used in tableau      Created Worksheet and dashboard to analyze data for business analysts.        Product Specialist   IVY Mobility, Chennai, TN   Oct 2015  May 2017   IVY CPG  Emami India          Responsibilities:    Been part of production support, to work with business, customer service teams, to handle with any data  issues, data analysis, data verification and change request.               Used SSRS to generate formatted production reports, including drill through, drill down, parameterized  reports and sub- reports, with stored procedures and expressions.       Taking application reports using SSRS (Sql reporting service).      Analyze and integrate disparate data and systems using ETL processes and logic to provide timely and  accurate information to our business partners      Wrote SQL stored procedures and SSRS report deployments.       Provide problem-solving, code review, risk assessment, and security recommendations      Closely worked with QA team to verify the result moved to test server from the development servers.      Prepared and updated documentations, business requirement description, procedure documentations file to  ensure all procedures and knowledge are traceable in future cycle.       Developing SQL Scripts, Customer support, bugs fixing and application support.      Trained L1 support resources about project workflows and shared experiences about how to fix issues in  product.        ACCOMPLISHMENT      Participated and Won Big Data Hackathon Certification      Oracle PL/SQL Developer Certified Associate - Oracle Corporation        ADEMIC PROFILE      Master of Computer Application (2011-2014) with 70% aggregate in University Of Madras, Chennai.      BSc in Computer Science (2008-2011) with 67% aggregate in Sathyabama University, Chennai      Higher Secondary: St. Marys Boys Matriculation Higher Secondary School (2006-2008) in Chennai with  56% aggregate.       S.S.L.C: Don Bosco Matriculation Higher Secondary School (2005-2006) in Chennai with 64% aggregate.              PERSONALITY TRITS      Excellent team player with good communication skills      Flexible with excellent co-ordination and presentation skills        DECLARATION     I hereby declare that the above mentioned informations are true to the best of knowledge.            Place:         Date:                                                                                                   (Vinothkumar.V)    ", "tokens": [{"text": "HTC Global Service", "start": 1608, "end": 1626, "token_start": 251, "token_end": 253, "entityLabel": "company"}, {"text": "Mar 2020  Present", "start": 1651, "end": 1668, "token_start": 259, "token_end": 262, "entityLabel": "period"}, {"text": "Virtusa Consulting Service Pvt ltd", "start": 4034, "end": 4068, "token_start": 665, "token_end": 669, "entityLabel": "company"}, {"text": "Mar 2019  Mar 2020", "start": 4085, "end": 4103, "token_start": 675, "token_end": 679, "entityLabel": "period"}, {"text": "SCIO Health Analytics", "start": 5886, "end": 5907, "token_start": 983, "token_end": 985, "entityLabel": "company"}, {"text": "May 2017  Feb 2019", "start": 5938, "end": 5956, "token_start": 991, "token_end": 995, "entityLabel": "period"}, {"text": "IVY Mobility", "start": 7022, "end": 7034, "token_start": 1170, "token_end": 1171, "entityLabel": "company"}, {"text": "Oct 2015  May 2017", "start": 7050, "end": 7068, "token_start": 1177, "token_end": 1181, "entityLabel": "period"}], "relations": [{"child": 1177, "head": 1170, "relationLabel": "duration"}, {"child": 991, "head": 983, "relationLabel": "duration"}, {"child": 675, "head": 665, "relationLabel": "duration"}, {"child": 259, "head": 251, "relationLabel": "duration"}]}, {"document": "                                                   12/8/2020 AXP Internal 1           Vishal Kumar  Senior Lead Analyst      A result-driven data analyst with 4 years of  experience in handling different types of  data in finance and automotive domain.  Seeking a position in a company where I can  leverage my skills to generate interesting  insights from the data and help the  company achieve business goals.          vishal152715@gmail.com        +91 9589307943       Gurugram, Haryana, India        linkedin.com/in/vishal5164        github.com/vishal5164        medium.com/@vishal152715          WORK EXPERIENCE  Senior Lead Analyst - GFE Product Management Team  American Express  09/2019  Present                                                                                                                                Gurgaon, Haryana   Achievements/Tasks    Provided ADs with insights and support on the user data using Field  Tools to gain new customers as well as increase sales with prospective  clients.    Automated manual data extraction and wrangling processes for report  generation using Python, Hive and Apache Spark on ML Studio.    Handled all data quality gaps and partnered with business teams to  resolve any kind of issue.    Handled different data related queries from Sales/AD related to Spend  Leakage in T&E products.    Created attractive Tableau Dashboards related to a particular use case  for stakeholders, product owners and product users to drive efficient  retention of users.    Analyzed and resolved any Account Tagging information of the Sales/Ad  professionals.    Created temporary data extraction pipeline and report for invoice  generation for a company recently acquired by AMEX.    Responsible for creating automated scripts to manage space  utilization of use case area.     EDUCATION  B.Tech in Computer Science  ITM University, Gwalior, Madhya Pradesh  06/2012  05/2016                                                                                           7.84 CGPA   AISSCE  12th  Millia Convent English School, Purnea, Bihar  04/2010  03/2012                                                                                           70%   AISSE  10th  Mithila Public School, Forbesganj, Bihar  04/2008  03/2010                                                                                           9.2 CGPA      ORGANIZATIONS  Internity Foundation, Pune (10/2017  Present)   Interviewed and trained college students on Python and Machine   Learning.    Hosted a series of events as speaker across Pune on Introduction  to Data Science.    Mozilla Community, Gwalior (03/2015  05/2016)   Co-founder of the community in the city. Organized multiple   events across the schools and colleges in city presenting about  various Mozilla projects.           Data Scientist - Vehicle Diagnostics  Tata Consultancy Services  04/2017  09/2019                                                                                                                            Pune, Maharashtra   Achievements/Tasks    Designed preventive measures for clients to prioritize the errors in the  vehicle to reduce the costs incurred by customers while the vehicle is on  warranty using Association and Sequence Mining.    Completed end to end setup of MongoDB to process, import & store  customers vehicle diagnostic data in Python.    TECHNICAL SKILLS                              MongoDB Predictive Analytics   Service Now Apache Spark Tableau   Hive Reporting SQL/NoSQL   NLP Machine Learning Python   mailto:vishal152715@gmail.com file:///C:/Users/vkuma499/Downloads/linkedin.com/in/vishal5164 file:///C:/Users/vkuma499/Downloads/github.com/vishal5164 mailto:medium.com/@vishal152715      12/8/2020 AXP Internal 2            Identified major factors involved in the occurrence of an error in a  particular vehicle using Random Forest and Clustering Algorithms.    Developed predictive maintenance solution for customers using  Machine Learning to predict if an error is going to occur in the vehicle in  the next 60 mins or 24 hours. (Forklift Vehicles).    Developed a workflow using TabPy to generate Machine Learning  models on the data and visualize the predictions in the Tableau using  calculated fields.    Performed regression, classification, bayesian methods, tree-based  learners, SVM, Random Forest, XGBOOST, time-series modelling,  dimensionality reduction, association mining, clustering etc on a regular  basis to provide insights to the customers.    Developed Machine Learning Models using Streaming Data in Azure  using Apache Spark predicting fault occurrence in a vehicle.      CERTIFICATES   Business English Certificate   Passed with merit in Business English Certificate Preliminary  (ESOL Entry Level Certificate) conducted by British Council,  Cambridge University.    Data Science Certifications  Completed online courses on Data Science Foundations, AI  Foundations  Machine Learning, NLP with Python and Machine  Learning and Advanced NoSQL for Data Science using Lynda,   Coursera and Udemy.    Tableau  A-Z (Udemy)  Completed certificate course on Tableau from Udemy           QA Engineer - TCS Embedded Code Analyzer  Tata Consultancy Services  01/2017  04/2017                                                                                                   Pune, Maharashtra   Achievements/Tasks    Developed automated test cases for regression testing for the TCS  owned product using Python, Robot Framework and AutoIT.    Created proper product manuals to be used by customers.             PERSONAL PROJECTS   Sales Trend Analysis (01/2019  01/2019)  Predicting future sales of an organization based on nonseasonal  time series sales data using LSTM (Deep Learning)  and ARIMA models.      Movie Recommendation System  (03/2018  03/2018)  Worked on a use case where data was scraped from IMDB  and based on a particular movie, related movie was  recommended to the user.      College Major Project (01/2016  05/2016)  Extracting Twitter Data related to a particular keyword and  & performing the Sentiment Analysis based on positive and  negative tweets. This project was developed using Hadoop  on Cloudera. The language used was Java.      Other Projects (01/2017  Present)  Apart from the above, I have also worked on multiple  projects on Clustering, Chat-bot, Web Scraping, EDA etc.   COMPUTER PROFICIENCY      Software   Tableau    Power BI    Jupyter Notebook    PyCharm/VS Code    MS Office         Operating System   Windows    Linux        LANGUAGES    English                                                            Native or Bilingual Proficiency       Hindi  Native or Bilingual Proficiency        EXTRA CURRICULAR ACTIVITIES     o Intern at RoughPolish (05/2014  07/2014).   o Intern at ConneXTech (11/2015  01/2016).                      ", "tokens": [{"text": "Tata Consultancy Services", "start": 2850, "end": 2875, "token_start": 432, "token_end": 434, "entityLabel": "company"}, {"text": "04/2017  09/2019", "start": 2877, "end": 2893, "token_start": 436, "token_end": 438, "entityLabel": "period"}], "relations": [{"child": 436, "head": 432, "relationLabel": "duration"}]}, {"document": "William H. Walters404.219.7564Page 1   WILLIAM (Bill) H. WALTERS  Metro Atlanta Area                                     404.219.7564                           bill.walters1281@att.net       DATABASE ENGINEER    Information technology professional with extensive experience including expertise in Oracle and a strong background with Postgres databases. Excellent problem-solving skills by thoroughly troubleshooting issues to identify root causes and prevent recurrence. Ability to maintain multi-terabyte databases with performance tuning, capacity monitoring, and backup and recovery. Work well independently or in a group setting committed to completing projects on time with proven results.    Oracle  Cloud Control  Amazon Web Services  Postgres  Perl  Opsgenie                                                                                     Technical Skills     expert level:   Oracle 9i, 10g, 11g, 12c  CloudFront  SQL Query and Database Performance Tuning  OEM Cloud Control(OEM12 and 13c)  RMAN   Disaster Recovery  RedHat 6 and 7  AIX  Perl  Amazon Web Services RDS (Relational Database Service with Mult-AZ) and EC2 (Elastic Compute Cloud)  AWR   ASH  ADDM  Shell scripting(korn shell)      competent:  Postgres  RAC  Data Guard  MySQL  Opsgenie  AWSCLI  COBOL  SQL Developer    Toad  Python  MongoDB  IBM AIX HA        Professional Work Experience          Jackson Healthcare(Care Logistics), Alpharetta, Georgia March 2019  December 2020     Senior Database Administrator          Provide Oracle and Postgres database support for Hospital Patient tracking systems in AWS and on-premise servers.         Lead AWS database architect for Care Logistics.   Implemented new notification software OpsGenie for the support team.   Migrating current Oracle databases to AWS Postgres RDS.   Building out AWS environments for new hospital clients using Postgres.   Creating a Data Lake in the AWS environment for analysis.   Analysis to determine best fit for hospital reporting between MongoDB(Atlas) and DynamoDB   Created AWS documentation and processes for Care Logistics employees to follow.   Developed Covid-19 reports for hospital clients to send to their respected state health departments.   Maintained Oracle RAC systems for hospital clients.   Upgraded Oracle databases from 11g to 12c to keep in compliance for clients.   Upgraded Postgres databases from 9.5 to 11 to keep in compliance for clients.   Monitored and Alerted for any Oracle and Postgres database issues.   Created an interface to open a trouble ticket for Oracle and Postgres databases.   Daily monitoring of backups on Oracle using RMAN and pgdump for Postgres.   Maintained on-premise and AWS EC2 Redhat Linux servers for clients.   Applied Care Logistic application patches for better operation.   Made suggestions to the development team for improvements in the PL/SQL code on reporting and online systems.   Enhanced database designs on Oracle and Postgres to improve performance.   Created new security database roles to comply with SOX and HIPPA standards.               Veritiv Corporation, Atlanta, Georgia  July 2018  March 2019  Senior Database Administrator/Engineer(Contract)          Provide support for 80 Oracle 10g, 11g & 12c databases ranging from 500 MB to 4 Terabytes        Developed and Implementing Oracle standards for all UNIX(AIX) servers.   Project lead for the Data Warehouse solution to move reporting users to another server to reduce overhead on the current database.   Using Cloud Control, found long running PL/SQL statements provided the solution to the development team to ensure optimal performance of the Data Warehouse database.   Architected a solution to send OEM alerts to Service Now system for automatic ticket creation.   Created Disaster Recovery standards, implemented, and tested to ensure all databases were protected.   Implemented and maintained Data Guard for Data Warehouse database.   Developed packages and procedures for the DBA team for reporting to the group.   Monitored and maintained Backups using RMAN and TDP library system.   Upgraded Oracle databases from 11g to 12c to keep in compliance.   Created new security database roles to comply with SOX standards.                   THE COCA-COLA COMPANY, Atlanta, Georgia  July 2000  October 2017  Database Engineer  Provide support for over 400 Oracle 11g & 12c databases ranging in size from 200 MB to 40 Terabytes.   Architected and implemented the migration of Oracle and MySQL databases to Amazon Web Services RDS (Relational Database Service) and EC2 (Elastic Compute Cloud).   Migrated Oracle databases over to Amazon Web Services.   Implemented Oracle Cloud Control on EC2 (Elastic Compute Cloud) to centralize database monitoring and administration.   Implemented Oracle Internet Directory on EC2(Elastic Compute Cloud), providing support for LDAP (Lightweight Directory Access Protocol).   Served as project lead for upgrading Oracle databases from 11g to 12c. Upgrading to 12c maintains Oracle support and prevented increased license costs.   Converted over 100 Perl scripts to PL/SQL to run on Oracle Cloud Control. Converting the Perl scripts to Cloud Control centralized the maintenance reducing change costs.    Using the tool Splunk, identified unused applications on servers which resulted in cost savings for Oracle database licenses.      Database Administrator lead for implementation of North American Help Desk system software. This system improved efficiency of the help desk by routing tickets quickly which resulted in quicker problem resolutions and better tracking for management.   Senior Oracle Database Administrator (Lead)   Provided support for over 200 Oracle 9i, 10g, 11g & 12c databases ranging in size from 200MB to over one terabyte.   Implemented and provided on-going support for a worldwide web-based purchasing system. The new system centralized and made the purchasing process more efficient to track costs.   Developed, installed, and provided on-going support for a vending machine database, providing the most efficient way for the repairmen to serve customers.     Implemented Oracle Transportation Management System. This new system allowed to decrease freight costs, improve on-time delivery and improve transportation information tracking.   Supervised the Oracle DBA team, ensuring effective assignment of projects and tasks.    Provided input into the hiring process for Oracle DBAs and SQL Server DBAs, making sure to select the most qualified and best fit candidates.   Architected the initial SOX security and compliance process for the audit team. This process is a required government regulation to protect shareholders and the general public from accounting errors and fraudulent practices.   Implemented, maintained, and monitored the Oracle RAC system for the Vitria, Remedy, and OTM databases.   Maintained the financial DB2 databases for the North America Bottlers.                                              Earlier Experience:      CHILDRENS HEALTHCARE OF ATLANTA, Atlanta, Georgia   Oracle DBA/Unix Administrator/SQL Server Administrator  Provided support for six Oracle databases and seven SQL server databases.   Implemented and provided on-going support for Lawson Financial System. This system centralized the multiple aging financial systems into one. The hospital has an interconnected financial system which improved the efficiency of the employees and better management reporting for each department.   Merged all Oracle databases between Scottish Rite and Egleston. Resulted in reducing costs due to duplication between the two hospitals.         PHA/SCOTTISH RITE CHILDRENS HOSPITAL, Atlanta, Georgia   Systems Analyst  Provided software support to 50 users in various departments.   Implemented and provide ongoing support for Patient Billing System for Doctors Offices, allowing for accurate and timely billing and management of patient accounts.        Professional Development / Education       Oracle 12c, 11g, 10g, 9i Administrator   Certified Information Systems Security Professional    Microsoft Azure   Amazon Web Service Cloud   Google Big Data Cloud    Perl Script   Active Directory Boot Camp   Project Management Fundamentals      Associate of Science Degree in Computer Science   DeKalb Community College", "tokens": [{"text": "March 2019  December 2020", "start": 1429, "end": 1454, "token_start": 234, "token_end": 238, "entityLabel": "period"}, {"text": "Veritiv Corporation", "start": 3063, "end": 3082, "token_start": 513, "token_end": 514, "entityLabel": "company"}, {"text": "July 2018  March 2019", "start": 3102, "end": 3123, "token_start": 520, "token_end": 524, "entityLabel": "period"}, {"text": "THE COCA-COLA COMPANY", "start": 4225, "end": 4246, "token_start": 716, "token_end": 720, "entityLabel": "company"}, {"text": "July 2000  October 2017", "start": 4266, "end": 4289, "token_start": 726, "token_end": 730, "entityLabel": "period"}, {"text": "Jackson Healthcare", "start": 1373, "end": 1391, "token_start": 226, "token_end": 228, "entityLabel": "company"}], "relations": [{"child": 726, "head": 716, "relationLabel": "duration"}, {"child": 520, "head": 513, "relationLabel": "duration"}, {"child": 234, "head": 226, "relationLabel": "duration"}]}, {"document": "                                                              PROFILE       An agilest with more than 16 years of experience in technical project / program management. Hands on expertise in    market leading technologies and cloud computing. Ability to translate business needs into technology requirements that  supports the company's business objectives.           EXPERIENCE    XXXX.  Technical Program Manager     Jan20 - Current     Worked with clients to resolve critical issues and suggested cost-effective solutions.    Delivered all the projects within budget and in defined timelines.    Followed Agile / ITIL methods while executing Dev / Ops projects.     Lead the hybrid cloud planning solution.    Hands on experience with market leading technologies Kubernetes, Terraform, ansible and Cloud computing.    Configured JIRA & Jira service desk cloud as per operational / Agile configuration.    Managed projects within defined budget.    Managed all the projects in Jira and knowledgebase in Confluence.       Accenture Solutions Pvt Ltd.  Cloud Project Manager     Feb14  Dec19     Provided AWS / Azure cloud solutions to the client.    Worked with HR in hiring critical resources with niche skills along with negotiation.    Provided risk mitigation plans to the leadership teams.    As a Service delivery manager reduced the customer issues by 98% (saving $3mn +/ month) for the client.    Managed large operational teams to provide continuous support to the client.    Lead implementation of microservices like Kubernetes & docker containers.    Successfully delivered projects within the budget using Agile & DevOps framework.    Managed the financials and forecasts for projects costing more than 10 million.         IBM Global Services  Service Delivery Manager    Jan09  Feb14     Managed the service delivery for the US projects.     Engaged stakeholders throughout the projects life cycle.     Consistently managed offshore delivery in green status.    Single handed delivered projects value ranged from $50 million to $70 million.     Saved more than 30% to the client through service improvement.                  WILSON GADEKAR      [ Tirupati Campus Phase 6, Pune 411015], [Email     Wilson.gadekar@gmail.com     [ Mob -   +91 989 0284 719          Apptix India  Tech Lead    Mar07  Dec08     Provide seamless support on Microsoft tools like Outlook Web Access on mobile devices.    Consistently, achieved 98% resolutions to the issues raised by Apptix customers.     Lead escalated calls for Microsoft applications related issues and provided technical relief to the customers.          Cable & Wireless  BSD Engineer    Mar05  Mar07     Provided first level support to clients related to IT issues.    Worked with market leading Management CRM tools such as Advent-Net ServiceDesk Plus 4.0 and   OpsManager   5.5. Maintaining documentation on processes and their SLAs according to ITIL procedures.          SKILLS       Project/Program Management   Risk Identification / Risk Mitigation    Kubernetes / Docker      Cloud Services (AWS / Azure / Terraform)   Process improvement       Financial Management    Jira / Confluence            CERTIFICATIONS     Certified Kubernetes Admin (May21)    AWS Certified Sys-Ops (Sep19)    AWS Certified Sol Arch (Jan19)    Azure Certified Admin Associate (Jan19)    ITIL (SD, ST, SO, CSI) - 2015    PMP (Dec12)              EDUCATION    MBA (IT)  Jaipur National University    (2015-2017)    PGDBM  MIT College Pune    (2015-2016)    BCS (Computer Science)  Wadia College    (1998-2001)      TESTIMONIALS      I wanted to share the tremendous job Wilson has done in an extremely challenging role. He is stationed at   clients vendor office. He has played an instrumental role in not only controlling vendors false down time   claims which effects the clients Business & commercials, but also played pivotal role in couple of key projects   the client wanted to implement. Retail business has not been a big fan of IT and a positive feedback coming   from them is welcome. Peter at times can be extremely difficult to handle. Appreciation from him is truly   special.    - Associate Director    This is wonderful Wilson. Good job delivered and thrilled to see you getting nominated by the client for an   award.    - Senior Manager      ", "tokens": [{"text": "Accenture Solutions Pvt Ltd", "start": 1022, "end": 1049, "token_start": 161, "token_end": 165, "entityLabel": "company"}, {"text": "Feb14  Dec19", "start": 1078, "end": 1090, "token_start": 170, "token_end": 172, "entityLabel": "period"}, {"text": "IBM", "start": 1735, "end": 1738, "token_start": 286, "token_end": 286, "entityLabel": "company"}, {"text": "Jan09  Feb14", "start": 1784, "end": 1796, "token_start": 294, "token_end": 296, "entityLabel": "period"}, {"text": "Apptix", "start": 2275, "end": 2281, "token_start": 382, "token_end": 382, "entityLabel": "company"}, {"text": "Mar07  Dec08", "start": 2302, "end": 2314, "token_start": 388, "token_end": 390, "entityLabel": "period"}, {"text": "Cable & Wireless", "start": 2614, "end": 2630, "token_start": 439, "token_end": 441, "entityLabel": "company"}, {"text": "Mar05  Mar07", "start": 2648, "end": 2660, "token_start": 446, "token_end": 448, "entityLabel": "period"}], "relations": [{"child": 170, "head": 161, "relationLabel": "duration"}, {"child": 294, "head": 286, "relationLabel": "duration"}, {"child": 388, "head": 382, "relationLabel": "duration"}, {"child": 446, "head": 439, "relationLabel": "duration"}]}, {"document": "Naresh Jain  Contact: +919711400199 ~ E-Mail: engg.nareshjain@gmail.com   LinkedIn: https://www.linkedin.com/in/naresh-jain-b9622926/  IT PROFESSIONAL   ~ANDROID ARCHITECT ~ ANDROID/JAVA DEVELOPER ~ TECHNICAL LEAD ~   PROFESSIONAL SNAPSHOT   Offering 9 years of strong expertise in the areas of:    - Android Architect    - Android/Java Development        - Application Architecture              - Requirement / System Analysis   - Agile Methodology    - Team Management                     - Client Relationship Management   - Effort Estimation    - Release Management    Professional work experience in designing & development of digital solutions across different technology like Android, Kotlin, Java, J2EE & Spring Boot.   Worked in various domains like Retail/E-commerce, Telecommunication, Travel, Instant Messenger & Utility Products    An expert in designing applications from scratch and making products with excellent UX including responsive UI for millions of users.  Comprehensive knowledge of Android Material Design Principles, Jetpack Components, Memory management, Concurrency, Design patterns, MVVM, App performance, Security and backend system (REST  & SOAP Services)  Proactive and result-oriented leader adept in mentoring and motivating the dynamic team to exemplary performance.    Self-starter with good communication and interpersonal skills.   Excellent logical, analytical and observational skills.     TECHNOLOGIES EXPERTISE    Languages /Framework    Libraries / API  Android, Kotlin, Java, J2EE, Spring MVC, Spring Boot    Accessibility API, ADA, GSON, Google API services, Firebase Crashlytics, Junit, Media/Exo Player API    Databases   Room, SQL Lite, MySql, Oracle 10g    Version Control  Build Tool  Other Tools  IDE  Operating Systems  Github, Git, Gitlab, SVN, TFS(Microsoft Visual Studio), SVN  Maven , Gradle  JIRA, Jenkins, Confluence, TFS, Sourcetree, Postman, CharlesProxy  Android Studio, Eclipse , IntelliJ  Mac, Windows, Linux    EMPLOYMENT SCAN  GlobalLogic India Pvt. Ltd., Noida (Technical Lead/Consultant, April 2018  Present)   Mara Social Media, Gurgaon (Sr. Software Engineer, March 2017 - March 2018)  Nimbuzz Technologies India Pvt. Ltd, Gurgaon (Sr. Software Engineer, June 2015 - Feb 2017)  Fareportal India Pvt. Ltd., Gurgaon (Associate Team Lead, August 2011 - May 2015)    NOTABLE ACCOMPLISHMENTS ACROSS THE CAREER  Sun Certified Java Developer (SCJP)   Excellence Award 2019 at GlobalLogic  Employee of the Quarter Awards in 2016 at Nimbuzz  Mentor Awards 2013 at Fareportal  Employee of the Month awards 2012 at Fareportal    ACADEMICS  Master in Computer Application(MCA) from IGNOU, Delhi  O Level (Computer Diploma) from NIELIT, Ministry of Electronics & Information Technology, Government of India  Bachelor degree from Rajasthan University, Jaipur          MAJOR PROJECT EXECUTED    1. Kohls- Online Shopping App  Overview: This is free online shopping app that assures a great shopping experience with a lighter app, faster load time & wide selection across categories. Using this app user can browse and search for their desired products by product name, category or brands.  Get personalized recommendations based on ratings & shopping trends.   Responsibilities:    Responsible for design and implementation for technical user stories  Responsible for building and maintaining product features, addressing bugs in the application.  Explored best practices and applied Kotlin and Android architecture components at Kohls.  Implemented Accessibility feature (ADA compliant)    Incorporated Cert pinning and introduced DI (Daggar2), Retrofit, Room DB in the application.  Developed scan & search feature for products in Android App using Kotlin, RxJava, ViewModels.  Proposed and implemented design changes in Frameworks, which helped in performance, memory consumption.  Managing Android team & drive daily scrum meetings also  Performed code reviews to ensure better code quality    Technologies: Android 4.4 and above, Zxing library (QR Scanner), Android AspectJ,  Sqlite Database, Google Play Service API, Adobe Analytics API, Retrofit Library, Crashlytics, FireBase, JetPack Components, Dagger2, ExoPlayer,ADA  2. Nimbuzz Messenger   Overview: Nimbuzz is a multi platform Instant Messenger, social networking (chatroom) and VOIP product. Nimbuzz Messenger combines the internet and smart phone messenger into one platform and lets you make free video calls, voice calls, send chat message, share files on any mobile device across popular messengers.    Responsibilities:    Responsible for building and maintaining product features, addressing bugs in the application.  Developed and owned modules with high focus on scalable designs and high performance architectural patterns.  Worked on key features for Nimbuzz like- Chat Rooms, Message Queuing, ads and stickers etc.  Performed quick prototypes for new features to show case to the product team.  The KPI were performance, low memory footprint, crash free, highly functional and secure    Technologies: Android2.3 and above, Location API, Sqlite Database, Google Play Service API, Google Analytic Tracking API, Volley Library, Digits API, Facebook API, Google Phone number Library, VideoAd, Crashlytics  3. Holaa! Called ID & Call Block  Overview: Holaa is a free Caller ID application which lets you know the callers name, photo and their location during an incoming call.   Holaa also helps you block spam calls from pesky spammers or other unwanted callers. Its available on play store and also supports multiple language- English, Arabic, Persian, Spanish, French & Portuguese  Responsibilities:    Design and Developed   and owned modules with high focus on scalable designs and high performance architectural patterns.  Development and Designing the complete app from scratch   Involved in architectural discussion for all new features  Worked on Key modules  for app like Name search, Call Log , Backup and restore   Implemented Push Notification,  Sorting and Filtering & Crittercism Bug tracking tool  Performed quick prototypes for new features to show case to the product team.     Technologies: Android2.3 and above, Location API, Sqlite Database, Google Play Service API, Google Analytic Tracking API, Volley Library, Digits API, Facebook API, Google Phone number Library, VideoAd, Crashlytics    4. Cheapoair Flight, Hotel and Car Search  Overview: The CheapOair is a free travel category application which you can book the Flights, Hotel and Car. Book flight from 450+ airlines, find great deals on hotels and cars, all from one easy-to-use app. Its available on play store and also supports multiple currencies.   Responsibilities:    Developed and owned modules with high focus on scalable designs and high performance architectural patterns.  Development and Designing the complete app from scratch   Involved in architectural discussion for all new features  Worked on Key modules  for app like Flight Search & Booking, Hotel Search & Booking, Check My Booking  Implemented Push Notification,  Sorting and Filtering & Crittercism Bug tracking tool  Performed quick prototypes for new features to show case to the product team.       Technologies: Android2.2 and above, Location API, Sqlite Database, Google Play Service API, Google Analytic Tracking API, Google Cloud Messaging    PERSONAL DETAILS  Fathers Name  :  Padam Kumar Jain  Linguistic Proficiency : English, Hindi  Marital Status  :   Married  Hobbies   : Reading books, Badminton, Listening Music  Sex   : Male", "tokens": [{"text": "GlobalLogic India Pvt. Ltd", "start": 1992, "end": 2018, "token_start": 354, "token_end": 359, "entityLabel": "company"}, {"text": "April 2018  Present", "start": 2055, "end": 2074, "token_start": 367, "token_end": 370, "entityLabel": "period"}, {"text": "Mara Social Media", "start": 2078, "end": 2095, "token_start": 373, "token_end": 375, "entityLabel": "company"}, {"text": "March 2017 - March 2018", "start": 2129, "end": 2152, "token_start": 384, "token_end": 388, "entityLabel": "period"}, {"text": "Nimbuzz Technologies India Pvt. Ltd", "start": 2155, "end": 2190, "token_start": 391, "token_end": 396, "entityLabel": "company"}, {"text": "June 2015 - Feb 2017", "start": 2224, "end": 2244, "token_start": 405, "token_end": 409, "entityLabel": "period"}, {"text": "Fareportal India Pvt. Ltd", "start": 2247, "end": 2272, "token_start": 412, "token_end": 417, "entityLabel": "company"}, {"text": "2011 - May 2015", "start": 2312, "end": 2327, "token_start": 425, "token_end": 428, "entityLabel": "period"}], "relations": [{"child": 367, "head": 354, "relationLabel": "duration"}, {"child": 384, "head": 373, "relationLabel": "duration"}, {"child": 405, "head": 391, "relationLabel": "duration"}, {"child": 425, "head": 412, "relationLabel": "duration"}]}, {"document": "                                                 Muhammed Nazeem Rafiq Data Analyst  Data and Reporting Analyst with more than a year of experience in International Business and IT with exposure to industry-leading tools & technologies. Strong knowledge in Tableau Desktop, Tableau Server and Tableau Viewer. End-to-end experience in designing and deploying data visualizations using Tableau for various product domains.  muhammednazeemrafiq@gm ail.com  +91 8714381318  Calicut, India  linkedin.com/in/nazeemrafiq  SKILLS  Tableau Oracle SQL  Microsoft Office  Python Power BI  Data Visualization  LANGUAGES English Full Professional Proficiency  Hindi Full Professional Proficiency  INTERESTS  Football Traveling  Gaming  DATA ANALYST  Lulu International Exchange MIS & Analytics 02/2020 - Present, Kochi, India  Analyzed business data to establish Key Performance Indicators, built dashboards and  generated financial reports to visualize important trends and measure effectiveness of business  decisions.  Generated advanced Tableau dashboards with quick/context/global filters, parameters and  calculated fields that allow to track and improve customer units KPI by 10%  Analyzed past performance data and conducted feasibility studies on business processes such  as staff mix ratio analysis, CR Rate, Churn rate analysis, CLT Value , CAGR etc.. driving up sales  25%.  Developed CEO Dashboard that is relied upon top managements to take decisions across all 9  countries operations.  Deployed over 40+ analytics project for various departments using Tableau.  SME for Tableau for operations extending up to nine countries in GCC & APAC region.  Conducted multiple in-house training sessions for Tableau and also coached associates in the  team.  Create workbooks and dashboards using parameters and calculated metrics for different  visualization requirements on full range of Tableau platform technologies for senior  management on a periodic basis.  Work with large data sets from disparate data sources to understand linkages and to develop  use cases for business needs.  Managing and designing the reporting environment, including data source real time monitoring,  security and meta data.  Supported the development and modification of tableau views in tableau Desktop, while  preparing documentation and training users from respective units.  CERTIFICATION Data Analysis with Python IBM  Data Visualization for Data Analysis and Analytics LinkedIn Learning  ONLINE COURSES Microsoft Professional Program in Data Analytics EdX - Learning Platform  Building Modern Python Applications on AWS AWS - Coursera  EDUCATION  Bachelor of Technology in Computer Science APJ Abdul Kalam Technological University 08/2015 - 05/2019, Mdit Institute of Technology  Key Achievements:  Key Responsibilities:  mailto:muhammednazeemrafiq@gmail.com https://linkedin.com/in/nazeemrafiq  ", "tokens": [{"text": "Lulu International Exchange", "start": 737, "end": 764, "token_start": 114, "token_end": 116, "entityLabel": "company"}, {"text": "02/2020 - Present", "start": 781, "end": 798, "token_start": 120, "token_end": 122, "entityLabel": "period"}], "relations": [{"child": 120, "head": 114, "relationLabel": "duration"}]}, {"document": "                                                           SAROJADEVI PALANI   Result-oriented individual with strong aptitude for learning, interpersonal skills & an enthusiastic   Data Scientist with dynamic 13+ years of experience in Data Science, Analytics & strong database  background. Highly successful in intense & demanding environments, providing decisive team  leadership with a proven track record of stakeholder & client relationship management. Seeking to  leverage my technical & professional expertise to grow in the role of Data Scientist at your company.           Work Experience      Personal   Phone number  +917299923325   Email  saroojadevi@gmail.com   LinkedIn  https://www.linkedin.com/in/reachsaro      Skills   Oracle, Python, NLTK, SQL, MYSQL, Hive,   Hadoop   Machine Learning, NLP, Deep Learning,   Statistics   AWS, IBM & Azure Cloud, Exadata, Teradata    Regression, Classification & Clustering      Data Visualization (Tabulae, Power BI)       Confluence, GitHub, Snow  Tool, Atlassian Jira      Certifications     Sentiment Analysis with Deep Learning    using BERT (Coursera).    Certified Hadoop Administrator (Dezyre)   Oracle Certified Professional (Oracle9i)   Teradata Certified Professional V2R5   NSE Certification in Financial Markets (NCFM).      Achievements      \"Outstanding Performer Award\" as an   individual contribution in critical Oracle reports       Migration  Cognizant (CTS).      Bronze Awards (2) & 1 Silver Award       successful implementation of Capacity    planning & assessments in Bank of America.      Interests    Women Empowerment in STEM,   Music, Badminton, Yoga & Swimming                     Research Assistant (Datascience) Aug 2020 - Feb 2021  Indian Institute of Technology (IIT), Madras, Chennai   Build & develop a NLP framework to integrate LDA & BERT.   Unsupervised clustering ML model using NLP for contextual topics.  Unsupervised sentiments classification model for social media using transformer-based NLP   algorithms.   Senior Data Analyst / Operations Team Lead Oct 2017 - Mar 2019  Maersk Line UK Ltd, Maidenhead, Maidenhead, UK   Provided data insights and proposed recommendations for Maersk C&CC database   operations stability. Architected the Business requirements with customers.  Predictive Maintenance: Individually contributed in reporting & analyzing the capacity for   business-critical databases & successfully saved $1M cost to Maersk as part of Storage reclaim.   Given advice & provided solutions to the Vendor Managed Services in both Private Cloud (IBM)   and Public Cloud (Azure) Platforms. Involved & Contributed to Exadata - hybrid on-premise   migration.   Technology Consultant  Aug 2015 - Apr 2017  HPE/DXC Technology, Chennai   Owned & Managed team in establishing database management on critical production systems &   implement oracle best practices.  Involved in building the team capability in installing Oracle 11G/12C, RAC, Configure Backup &   Recovery, Data guard, Patching, Capacity planning & performance tuning in critical production   environments.   Senior Analyst  Oct 2010 - Oct 2014  Bank of America, BA Continuum India Private Limited, Chennai   Involved in critical production support BAU activities, Oracle 10/11G, RAC installation &   configuration, Backup & Recovery, Data guard, DR Activity, standby rebuild, Migration, Upgrade   & patching, Capacity planning, performance tuning.     IT Analyst  May 2006 - Sep 2010  Tata Consultancy Services, Chennai   Key project leader responsible for delivering a critical migration project for AXA Bank - Belgium.   Global BAU & 24/7 production support for Investment Banking & Insurance (USAA), British  Telecommunications (BT) & Motorola.      Education   Masters in Data Science  Jun 2019 - Feb 2021  Liverpool John Moores University (LJMU), UK, UK     Post Graduate Diploma in Datascience  Jun 2019 - Jun 2020  IIIT-Bangalore, Bangalore     Bachelor of Engineering - Electrical & Electronics  Jun 1999 - May 2003  Periyar University, Salem   https://www.linkedin.com/in/reachsaro/  ", "tokens": [{"text": "Maersk Line UK Ltd", "start": 2069, "end": 2087, "token_start": 355, "token_end": 358, "entityLabel": "company"}, {"text": "Oct 2017 - Mar 2019", "start": 2048, "end": 2067, "token_start": 349, "token_end": 353, "entityLabel": "period"}, {"text": "HPE/DXC Technology", "start": 2709, "end": 2727, "token_start": 470, "token_end": 473, "entityLabel": "company"}, {"text": "Aug 2015 - Apr 2017", "start": 2688, "end": 2707, "token_start": 464, "token_end": 468, "entityLabel": "period"}, {"text": "BA Continuum India Private Limited", "start": 3128, "end": 3162, "token_start": 546, "token_end": 550, "entityLabel": "company"}, {"text": "Oct 2010 - Oct 2014", "start": 3090, "end": 3109, "token_start": 536, "token_end": 540, "entityLabel": "period"}, {"text": "Tata Consultancy Services", "start": 3451, "end": 3476, "token_start": 608, "token_end": 610, "entityLabel": "company"}, {"text": "May 2006 - Sep 2010", "start": 3430, "end": 3449, "token_start": 602, "token_end": 606, "entityLabel": "period"}, {"text": "Indian Institute of Technology (IIT), Madras", "start": 1717, "end": 1761, "token_start": 290, "token_end": 298, "entityLabel": "company"}, {"text": "Aug 2020 - Feb 2021", "start": 1696, "end": 1715, "token_start": 284, "token_end": 288, "entityLabel": "period"}], "relations": [{"child": 602, "head": 608, "relationLabel": "duration"}, {"child": 536, "head": 546, "relationLabel": "duration"}, {"child": 464, "head": 470, "relationLabel": "duration"}, {"child": 349, "head": 355, "relationLabel": "duration"}, {"child": 284, "head": 290, "relationLabel": "duration"}]}, {"document": "                                              CV     +91-9444578480      vnapasupathi@gmail.com   Page 1 of 4   LinkedIn: https://www.linkedin.com/in/pasupathinarayanan   .                                                  PASUPATHI NARAYANAN                SEEKING CHALLENGING OPPORTUNITIES IN INNOVATIVE DIGITAL SOLUTIONS   Since May15 Cognizant Technologies, Chennai - India   Proficiency    Multiple years of US work experience handling Complex   Projects for North American Retail and Insurance clients    15 + years varied experience in Architecting, Retail Solutions   using Visualization Techniques, Business Analytics Tools    Track Record: In less than two years, brought out a non-  performing account with negative margins to a top   performance producing 12 M USD YoY  Using accurate   predictions in Profit Recovery and Revenue Leakages    Proven Client relations building skill: Business team once   quoted that they never had flawless on-time delivery in their   20 years past history    Technical solution & Competency building: Resulted in 3+ M   USD direct business savings    Data visualization techniques & inferential statistics for   a large north America retailer for Assortment & Optimal   warehouse capacity predictions resulting in increased sales    Vast interest towards learning, understanding and solving   real-time business problems using Analytical skills    Produced a feasibility study & implementation approaches on   migrating multiple on premise applications into Azure cloud   for large North American Retailer   Education & Certifications    Business Analytics (Data/AI/ML) - IIM,   Indore  2019-20    Business Analytics - IIMB  2014    Azure Cloud Certified (Data & Analytics)   MCA-1996 | PMP-2008 | CSM-2016   Analytics Competencies    Hands on in Exploratory Data Analysis -   Data patterns linear, non-linear, Coefficient   correlation & Feature Engineering    Data visualization using Excel, Tableau, Data   Analysis and Techniques in Python    Experience in Supervised & Unsupervised   & Ensemble models - Poisson, Polynomial,   Time series, K-Means, Random Forrest etc.,    Regression and Classification ML models    Hands on in Python .Net (C#) & MySQL     Strong exposure to Cloud, Tensorflow,   React JS and React Native    Expert skills in Machine Learning, &   Artificial Intelligence    Strong experience in Deep learning,   Neural Networks & Natural lang. (NLP)   Professional Overview   Technical Skills:  Expertise in Data Science, Analytics, Cloud and Automations. For past 5+ years, have been primarily focusing   on Automations, Data Visualization, and Analytics using AI/ML & Cloud (Azure / GCP).       Expertized skills in Exploratory Data Analysis, Statistical Modelling & Machine Learning models.    Data Profiling and Feature Engineering | Outlier Treatment, IQR and Data Transformations    Hands-on in Python 3.x, SQL, ML algorithms for Retail & Healthcare    Development skill in Tableau, Hadoop, Power BI, Tensorflow, Kubernettes, Anaconda 4.5+, CosMos DB,   Kafka and Docker    Hands-on in .Net, Mainframes, Azure (AI + ML Cognitive stacks, Text Analytics, Video Streaming,   Computer vision and Bots), AWS (Build POC and Feasibility studies using EKS, S3, EC2,   Sagemaker, Comprehend, Lex, Polly, Fraud detector for ML stacks) and MySQL    SME in Retail, Insurance & Healthcare domains      Technical contributions:   Produced & implemented handful of ML models applying Time series, Poisson distribution, K-Means++ segmentation   for Warehouse Management Systems and Stores applications. Identify and produce CoE groups in technology.   Modernization & Digital Transformations (Cloud migration / MF reductions)      Innovations / Initiatives:  Problem solving using AI / ML modelling and mentoring technologies to teams. Building Conversational UI (Azure Chatbot)  and AR/VR tools - To reduce effort, post production defect density, increase productivity, and build efficiency      Client Centric:  Engage customer, Provide potential savings area, Agree upon opportunities to improve, Work with team to align for  customers Technical & Business teams goals. Employ feasibility / POC for potential business value add      Delivery Excellence:  Steering Delivery Management of multiple North American Retail projects; Adopt and Implement CMMi best   practices. Provide technology and business insights to customers for maximizing ROI. Create repository and share   re-usable frameworks. Improve productivity. Optimize Effort building automated tools & accelerators resulted in   multi-year engagements for CI/CD pipeline implementation and DevOps automation   mailto:vnapasupathi@gmail.com https://www.linkedin.com/in/pasupathinarayanan https://www.linkedin.com/in/pasupathinarayanan     +91-9444578480      vnapasupathi@gmail.com   Page 2 of 4   LinkedIn: https://www.linkedin.com/in/pasupathinarayanan     I) Revenue Maximization for Walmart Stores (November 2019  Q2 - 2021)   Problem Statement: Predicting revenue loss that occurs due to items at aisle in a store is not getting re-filled on-  time if ran out in a middle of a day.  Stores supervisor does not have an alerting mechanism that predicts and   communicates before an item would be running out of Aisle is aware of such sold outs. Potential opportunity for USD   20-65 K revenue per day per store. USD 20-60 M revenue potential      Solution approach: Python, Scikit.Learn, Time Series  ARMA / ARIMA, Matplotlib, Goodness of fit, ACF/PACF, Kafka   & React JS    Business case identification using sales data analysis of few high volume stores (US).     Extensive EDA to prepare for ML modelling    Create prediction engine using Time series ML modelling which provides items running out of Aisle in advance    React JS front end to facilitate the store manager to take appropriate action (Triggers Order-Fillers a   replenishment move)      II) Election Data Analytics  Intern (August  2019)   Problem statement: Given data has details of all the state elections and contestant across Indian states Business   case is to identify:    If son is contesting on same seat as father    Candidate name discrepancies like first name + last name considered different than First + Middle + Last    Find Repeat candidates    List of other insights from data such as candidates are related to each other, Analyzing coefficient of variation   within given variables   Solution approach (EDA, Python, Dataframes, and K-Means++):    Data had 1.5 lakhs unstructured records. Following are the steps implemented using Python 3:    ROBUST null value treatment for Age using mean and Data clean up, Bit value creation for Winner contestants    Transformation using LabelEncoder for having normal distribution of data    Scaler transformation (Sklearn)    Elbow method / KneeLocater to define cluster size    Segmented using K-Means for Significant clusters for Winners, Age groups, Self-occupation and State etc.,    Good ness of fit using Silhouette score   Outcome: Cluster Segmentation of Winner group using K-Means. Cluster segmentation to figure out related group   of Age and Year for a specific state (State wise). With a given data, what is most influencing for Winning?      III) Retail Segmentation - Capstone at IIM Indore (June  2019)   Problem Statement: One of the house hold cleaning supplies brand has captured the sales data of their 3 major   customers. Using which, they would like to find out the price elasticity of demand as:     Demand -varying over the time.    Sales impact due to the price or the product style or the competitors pricing model to key customers   Based on the understanding of these constructs, they would like to determine optimal pricing that could help increase   the sales / revenue.      Solution Approach (Python, Scikit.Learn, Matplotlib, Logistic / Linear regression, ARIMA, K-means & Random Forest):    Data Treatment: Data profiling using Panda | Null value, Negative value and Outlier Treatment    Log transformations to asses Linearity of data for Linear regression model to solve Analytical Opportunities    Decide optimal ML algorithm such as Polynomial, K Means Clustering and Random Forest    Find out the price elasticity by customer segment over time period      IV) Customer Service Analytics  Cognizant (July  2019)   Problem Statement: Trustar Hypermarket is a retail giant expanding its business globally. Their eCom business is   becoming popular though, 87% of their revenue is from B&M. Part of a B&M expansion, and they have a new store   being launched. One of their organic-produce arm is gearing up to test new promotional offer (In-Store-Bidding).   Management is worried about attempting to test In-Store-Bidding and hence looking for solution(s) to predict the   success rate of the In-store bidding sales via the Social media.      Solution approach (Python, Scikit.Learn, Matplotlib, K-Means):    Marketing campaign effectiveness on promotional offers and in-store bidding using Sentiment Analysis    Sales increase due to in-store bidding using Linear Regression    Predict In-store bidding launch time using Linear Regression    Customer Segmentation based on millennial buyers visiting the store using K-Means Clustering      V) Poisson distribution for unveiling DevOps Inflow  Walmart (May -2019)  Problem Statement / Solution: Adding up more products to support perspective, team has raised a concern for   increased bandwidth. Solution deals with what is involved in per FTE productivity and brings out No. of forecasted   tickets for upcoming seasonality (Hypercare support during peak season) and optimal FTEs to be placed for support   / resolutions. Model used was Poisson distribution for visualizing the problem statement and helping for right decision   making.   mailto:vnapasupathi@gmail.com https://www.linkedin.com/in/pasupathinarayanan https://www.linkedin.com/in/pasupathinarayanan     +91-9444578480      vnapasupathi@gmail.com   Page 3 of 4   LinkedIn: https://www.linkedin.com/in/pasupathinarayanan      VI) Intuitive Chatbot using Conversational UI  Walmart (January  2019)  Problem & Solution: Designed a Chatbot for one of the team (Conversational UI with Speech recognition). This is   used for grooming new comer in a project to facilitate KT / SME training in an intuitive way. Also a reference material   where in the bot can answer the queries. This can be leveraged as a productivity tool for Walmart DC users / Walmart   management team to get specifics reports pertaining to daily transaction / performance.     DC Users  may use this tool for communicating / reporting a defective item while item-picking to the DC   supervisor with an image and facilitate for further chase-trips.    Piloted for rollout teams and can be leveraged to multiple such use cases further      VII) BP Prediction using Arima  Cognizant (October -2018)   Problem Statement: This is a case study for utilizing Arima predictions in order to define Blood pressure of a patient   (or any healthy persons as well) at a given future interval.      Solution approach (Python, Scikit.Learn, Matplotlib, and ARIMA):    Data collection; Clean up data (Data massaging) - Data treatment done for Outliers, Null readings    Model building : Create a linear model and checking upon the essential outcome parameters, Fit the model   using ARIM univariate Time Series regression      Achievement (Focused on automation / Innovation):   Primary delivery owner driving delivery of 6 different key projects of Walmart Logistics system with 14+ scrum   teams (Peak size team 140+ offshore)    Driven multiple initiatives using Data & ML (Logistics and Linear Regression) based algorithms for predictions   and better understanding of the data spread (Data Visualization)    Implemented DevOps solution & Simplification for NextGen product, resulted in USD 3 Million savings    Implemented several automation initiatives in CI/CD, Automation Testing, and Automated build & pre-  validations. Promotes outage less production rollouts    Tech. Modernization, DevOps, Automation and Feasibility on Azure Cloud migrations    Create POC for adopting new methodologies and AR/VR solutions. Mobile, Azure Cloud computing, React JS UI   technology adoption    Accountable for scope finalization, planning, tracking deliverables, incident management, invoicing and resource   management including the appraisals    Automations resulted in 1+ USD million savings every year   Technologies: Core Java, Spring Boot, Web services, React JS and Angular JS. Azure Cloud. Python ML,   Containerization, Docker and CICS, IMS DB/DC DB2.         Since May14 till Apr'15 MAP Technologies, India (Self owned organization)   Primary area of work:    Produce Distributed and Cloud-based healthcare product for Business Analytics    Business development, Client management and Pricing    Ensure on-time and under budget deliveries as committed to the Client's Business    Consulting and Enterprise Architecture (Cloud, SOA)    HIPAA Compliance, ICD 9   Achievements:    Managing client relations | Delivered 3.85 man years of work to a healthcare client    Closed two new deals in less than 8 months   Technologies: .Net, MySQL and Python. SQL Server, Windows 10.      Since Feb10 till Apr'14 Software Paradigms Infotech Pvt., Ltd, India - (CognizantSoftvision)  (Mysore, Atlanta & Canada)   Achievements:    Achieved revenue growth of 25% through quality delivery, account mining from existing clients and identifying   cross-sell/ up-sell opportunities    Overachieved targets; facilitating 15% growth in gross margin targets through continuous implementation of   improvement practices    Won proposals for mainframe migration/ elimination solutions for a large retail client (Estimated Full cycle   delivery cost 10+ M USD / Year) and turning IT from cost center to a profit center to keep ahead of competitors    Maintained 100% compliance in delivery SLAs and quality standards with Level 2 and 3 critical supports while   ensuring CMMi 5 standards and processes are met     Identified, developed and coordinated with various vendors/ resource providers to ensure delivery as per   contractual agreements within time & cost constraints     Started Informatica (BI) CoE practice, building large team of CoE based on Teradata methodology which helped   in increasing billed utilization from 25% to 85%    Technologies: Informatica, Core Java, SQL Server, Dot Net (C#), Windows 10, SOA      Since Mar07 till Feb'10 Mahindra Satyam Computer Services Ltd, India   Bangalore, Chennai & Hyderabad   Achievements:   mailto:vnapasupathi@gmail.com https://www.linkedin.com/in/pasupathinarayanan https://www.linkedin.com/in/pasupathinarayanan     +91-9444578480      vnapasupathi@gmail.com   Page 4 of 4   LinkedIn: https://www.linkedin.com/in/pasupathinarayanan    Instrumental in architecting a perfect solution for DHL, Singapore. Presented the proposal to the customer   stakeholders at their Singapore premises.    Pivotal in giving the presentation to a key client Nestle on L2 and L3 support.    Writing white papers in emerging technologies and helped 100s of people thru associate enrichment program    Received 70K Cash Award for excellence in handling on-time delivery of projects & achieve zero attrition during   critical times.     Successfully initiated Defense Proposals & Proposal Presentations to Nestle customer on L2 and L3 support.    Instrumental in winning multimillion deals from an Australian based public services organization.    Successfully completed Solution architecting for SOA architecture POC.    Saved 70 man/ day by implementing automation testing and delivery excellence initiatives across the assigned   portfolio projects in a short span of 8 months.     Conducive in ensuring timely Solution architecting for EMRI - 108 (Emergency service)    Ensured knowledge grooming & dissemination of SOA architecture and emerging technologies for more than   200+ associates.   Technologies: SQL Analytics Services, C#, CICS Web services, Core Java, Spring, Java Web services, Hybernate, DB2,   SL Server and Windows      Mar00 - Feb07 at HCL Technologies Ltd, India and USA   I. Insurance Publishing & Model Office Automation AIG AG, USA - Dec03  Feb07, HCLA   This system aims at tracking of a legacy application data with the help of core document templates.     Successfully conducted User Acceptance Testing for all Microsoft technology based Office Applications.    Worked extensively on C#, Dot net ODBC, Sybase Client, VSAM, CICS, Sybase Gateway Server, Windows    Received Excellent Rating from the customer for successfully implementing the IPS System      II. Talent Management & HR Solutions, Brassring, USA (Later called Kenexa)   Jet stream - a flagship product of BrassRing Inc. (now called Kenexa) - is a web-based HR solution    Successfully handled 30 members team involving development and unit testing. Studied core business   requirements of BrassRing and proposed suitable application designs.    Implemented Internationalization (Unicode) for Eastern European Languages and web-based tool for   maintaining production bug log, development test cases.    Worked extensively on ASP, ASP.Net, C#, IIS 5.0, SQL Server 2000, Com+, XML, XSL, Test director, Visual   source safe, Rational Rose and Windows 2000.    Received excellent feedback from the customer for successful production implementation.    Received 26K Cash Award from the customer for excellent delivery and large team handling.      III. Purchase Order & Inventory Control for VXL Technologies, USA   The application administers purchase order, RFP & RFQ data that are collected from end users. It has combination of   sixty data collection screens and reports. Technologies: VB, SQL Server 2000, Far point Controls using Windows 2000.      IV. eCommerce (Private Brands)   An e-commerce system used to track product promotions, catalogs, product definitions and user transactions/ profile.       Technologies: Commerce Server 2000, BizTalk Server, ASP, VB 6.0, SQL Server 2000, C#, Dot net ODBC, Sybase   Client, VSAM, CICS, Sybase Gateway Server, Windows 2000, Winrunner, Proterm and DB2.     Mar98  Mar00 at Zenith Global Consultants Ltd, India (BA to TCS)   Highlights:    Received Delivery Excellence Reward while working for TCS.     Got Onetime Delivery Reward from Earnst & Young Customer while working for TCS.    Some of major clients worked include Allied Signal  USA, USAA  USA, Tata International  India, Bilpa    Turkey     Instrumental in devising numerous automation tools to do Automated Year 2000 conversions which have helped   in saving manpower by almost 2 hr. /day per person in a 4 months project for about 60 member team   mailto:vnapasupathi@gmail.com https://www.linkedin.com/in/pasupathinarayanan https://www.linkedin.com/in/pasupathinarayanan  ", "tokens": [{"text": "MAP Technologies", "start": 12636, "end": 12652, "token_start": 2269, "token_end": 2270, "entityLabel": "company"}, {"text": "May14 till Apr'15", "start": 12618, "end": 12635, "token_start": 2266, "token_end": 2268, "entityLabel": "period"}, {"text": "Paradigms Infotech Pvt., Ltd", "start": 13263, "end": 13291, "token_start": 2385, "token_end": 2390, "entityLabel": "company"}, {"text": "Feb10 till Apr'14", "start": 13236, "end": 13253, "token_start": 2381, "token_end": 2383, "entityLabel": "period"}, {"text": "Mahindra Satyam Computer Services Ltd", "start": 14479, "end": 14516, "token_start": 2605, "token_end": 2609, "entityLabel": "company"}, {"text": "Mar07 till Feb'10", "start": 14461, "end": 14478, "token_start": 2602, "token_end": 2604, "entityLabel": "period"}, {"text": "HCL Technologies Ltd", "start": 16124, "end": 16144, "token_start": 2863, "token_end": 2865, "entityLabel": "company"}, {"text": "Mar00 - Feb07", "start": 16107, "end": 16120, "token_start": 2859, "token_end": 2861, "entityLabel": "period"}], "relations": [{"child": 2266, "head": 2269, "relationLabel": "duration"}, {"child": 2381, "head": 2385, "relationLabel": "duration"}, {"child": 2602, "head": 2605, "relationLabel": "duration"}, {"child": 2859, "head": 2863, "relationLabel": "duration"}]}, {"document": "                                                                                           POOJA MISHRA   CONTACT DETAILS:   Address:              C-130, Sector 61, Noida   Phone No.:  +91 8946823634    Email:   poojamishra0239@gmail.com    LinkedIn:             https://www.linkedin.com/in/pooja-mishra-09428b117/    PERSONAL PROFILE:   Machine Learning Engineer with 3+ years of in-depth experience in executing complete life cycle of   data science projects for companies across IT which includes insurance analytics (Repair), e-commerce   and healthcare. Highly enthusiastic to wrestle with complex dataset and innovate new methodologies   to resolve structured and unstructured business problems.    SKILLS:    Primary Skills:  Python programming language, Machine learning, Artificial Intelligence,    Deep learning, Natural Language Processing (NLP), Computer Vision,   Recommendation System    Data Analysis:  Data Wrangling, Data Acquisition, Data Modelling, Data Visualization    Deep Learning:  Deep neural networks (DNN), Convolutional neural networks (CNN), LSTM   Long-Short Term Memory, RNN    Other Skills:  Machine learning algorithms (Linear Regression, Nave Bayes, Support   Vector Machine (SVM), Random Forest, Decision Tree), Words   Embeddings, Tableau(basic), SQL (basic)   Social Skills:  Problem Solving, Client Handling, Creative Presentation and Time   Management, Teamwork, Leadership, Adaptability, Anchor          EDUCATION:    Bachelor of technology in    Computer Science and    Engineering     Swami Keshvanand Institute of   Technology, Management and   Gramothan, Jaipur   B.Tech(Honors) in Computer   Science, 2017   Internship Program Indian Institute of Technology, Delhi   (IITD)   Internship in Data Science,2016   Machine Learning   Certification    Udemy Certification in Data Science,   Python           https://www.linkedin.com/in/pooja-mishra-09428b117/ https://www.linkedin.com/in/namansahore/   EXPERIENCE & PROJECTS:   Infogain India Private Limited, since July 2017     Project:    Smart Recommender    Role:      Data Scientist    Summary:    Developed an artificial intelligence solution based innovative recommender   engine, which is capable of automating manual tasks and enabling Customers to make better decisions   and accelerating the vehicle repair and Claims Management process.    Responsibilities:     Serving as a key member responsible for creating new models to enhance the accuracy of the  system.     Weekly Client Interaction, requirement gathering, analysis and effort estimation     Design and developed the accuracy matrix for the recommender engine      Implemented Regression model technique for Labor Hour recommendation, which give an   uplift of 10% in overall accuracy     Optimized the existing python script with 10x times faster efficiency.     Written multiple testing and automation scripts for recommender engine.    Environment:          Python, Amazon Web Services, MS-SQL and RedHat.    Project:    Repair Procedure Classifier    Role:      Data Scientist    Summary:    Separating repair manuals into Collision Repair and Non-Collision Repair    documents. Automating the classification of Collision Repair manuals using Natural Language   Processing to FastTrack the documents under multiple labels.    Responsibilities:     Performed extensive Data Pre-processing using cutting edge NLP techniques.     Classify whether a document is Collision Repair or non-collision repair.     Designed and developed Machine Learning models to map documents to their   corresponding Categories and Sub-Categories.     Derived the overall accuracy of 92%.     Weekly interaction with Client to get latest requirements and work depending on it.     Environment:         Python, Amazon Web Services, RedHat, Beautiful Soup.    Project:    Synthetic Peer   Role:      Data Scientist    Summary:    In this project we are comparing and measuring performances of various   Insurance carriers and providing unbiased apple to apple comparisons. It involves Statistical Modeling   to generate predictions with high response time and ensure high accuracy.   Responsibilities:     I was the only AI/ML resource working on this project. Involved in daily client meetings  across the globe.      Understood business problem and devised advance analytics solutions to obtain        satisfactory results.    Performed Data processing and Data Visualization afterwards.    To perform exploratory data analysis (EDA) for analyzing data sets to summarize their  main characteristics and present them with visual method.   Implemented Desired algorithm to obtain the result.    Performed Data Validation to check the required output.      Environment:              Python with Pandas and NumPy, Statistical modeling, Hadoop Cluster, and   SQL.   Project:    Predictive Analysis for Quality   Role:      Data Scientist    Summary:                    Huge number of testcases run regardless of their failure status, which takes a   lot of time and resource to deploy the product. So, by using Machine learning technology we have   optimized the whole testing process which has saved a lot of time and resources to deploy the product.   Responsibilities:     Understood business problem and devised advance analytics solutions to obtain satisfactory   results     Pulled Data from Database using Python connection    Performed exploratory data analysis for analyzing data to summarize their main   characteristics and present them with visual methods     Prepared desired Data in order to apply Machine Learning algorithm (Nave Bayes).     Trained Model using Convolutional Neural Networks in order to shoot up the accuracy.    Performed testing in order to check the model output.      Environment:          Python with Pandas and NumPy, CNN, Deep Learning, Hadoop   cluster and SQL      Project:    Smart Triage   Role:      Data Scientist Intern   Summary:                    Developed an Artificial Intelligence solution i.e. Smart Questionnaire which is   capable of automating manual task and enable customers to make better decisions and helps in   accelerating vehicle repair and claim management process.   Responsibilities:     Only AI/ML resource working on this project.     Understood business problem and devised advance analytics solutions to obtain satisfactory   results    Responsibilities include Requirement gathering, Analysis and effort estimation.    Using Machine Learning and Deep Learning developed model for the POC        Environment:   Python with Pandas and NumPy, Hadoop Cluster, CNN, Correlation, Decision tree   and SQL                  CERTIFICATIONS:    Python for Data Science & Machine Learning Bootcamp    Deep Learning A-Z, Udemy    Python for Data Science and Machine Learning A-Z, Udemy      AWARDS & ACHIEVEMENT:    Value Creator for the quarter 2020, at Infogain India Pvt. Ltd.     Budding Engineer award in 2019, at Infogain India Pvt. Ltd.     Outstanding Performance Award for the quarter 2019, at Infogain India Pvt. Ltd.           ", "tokens": [{"text": "Infogain India Private Limited", "start": 1967, "end": 1997, "token_start": 324, "token_end": 327, "entityLabel": "company"}, {"text": "July 2017", "start": 2005, "end": 2014, "token_start": 330, "token_end": 331, "entityLabel": "period"}], "relations": [{"child": 330, "head": 324, "relationLabel": "duration"}]}, {"document": "Pradyumna Rana                                                                       Email: pradyumnaranaml@gmail.com                                                                      Total Exp: 10+ years                                                                            Mob: 9663602742    Profile Summary  Having around 10+ years of experience in Software Development and 4+ year as Data Engineer, Hadoop/Spark, Big Data Development.  Having experience on designing and implementing complete end-to-end bigdata application using Hadoop ecosystem tools such as HDFS, MR2, Hive, Impala, Hue, Sqoop, Spark etc.  Strong knowledge/working skill in Oracle Database (12c, 11g), Data Warehouse.  Good in python programming language, Unix Shell Scripting, SQL  Extensive programming experience in writing packages, procedures, functions, triggers, cursors, exception handling, collections and bulk operation.  Extensive experience in Oracle performance tuning in Oracle 11g.  Excellent troubleshooting techniques and debugging.  Have experience in requirement gathering, analysis, development, testing and postproduction support cycles in SDLC.  Have strong experience working in Snowflake cloud data warehouse (SaaS DWH).   Have experience working with different AWS cloud services, have strong experience in Data Analytics services  Having experience working on multiple domains such as banking, health science, insurance etc.  Hardworking and quick learner always open to new ideas and suggestions.      Professional Skills:   Big Data and   Cloud Technologies:            MapReduce, HDFS, Hive, Impala, Sqoop, YARN, Spark, Snowflake, AWS, Redshift, Athena, Docker, Kubernetes, IBM Cloud, IBM DB2                                                  (Basic Knowledge  Spark streaming, Apache Kafka)   Languages:                           Python, Unix Shell (Basic Knowledge  Scala)  Database:                              Oracle (SQL, PLSQL)    IDE:                                        Hue, Spyder, Jupyter, Toad, WinSCP, PyCharm  Operating system:                Windows, Unix  Other:                                     SVN, AutoSys, Clear Case, TFS, Jira, Airflow, Git    Work Experience Summary  1  Worked as a Software Developer from Jan 11 to Apr 12 in Bajaj Allianz Life Insurance co. ltd.  2  Worked as a Consultant from Jun 12 to Nov 14 in Capgemini India Pvt Ltd.  3  Worked as a Senior engineer from Nov 14 to Feb 21 in Harman Connected Services (Bangalore)  4   Working as a Senior Advisory Technical Services Specialist from Feb 21 to till date in IBM    Professional Experience:    Company  IBM  Client  IBM  Project Name  EPM  Duration  Feb 2021 to May 2021  Tools and technology  Pyspark, Python, Git, Docker, Kubernetes, Airflow, IBM Cloud, IBM DB2  Project  Summary  This is an ETL pipeline using Pyspark to load data into data lake from various source system and then applying the necessary transformation, using data frame API and then finally loading it to the target as IBM DB2.   Responsibility    Some of the key responsibilities while executing this project are as listed below.    Analysing and understanding the requirement.    Coding and developing the modules in PySpark, Python.    Deploying the code in cluster orchestrator as Kubernetes.            Company  Harman Connected Services  Client  PRA Health Science  Project Name  ETL pipeline for CVPP/CVPI/Novartis/Eversana deliverables.  Duration  Feb 2018 to Jan 2020  Tools and technology  Pyspark, Python, Hadoop, HDFS, Hive, AWS, Unix scripting, Oracle12C, SQL, Snowflake, Autosys  Project  Summary  This is an ETL pipeline to build a data mart into Snowflake DWH which enables reporting for different deliverables. Data has been consumed by BI teams from Snowflake DWH for creating dash boards and reports.  Responsibility    Some of the key responsibilities while executing this project are as listed below     Analysing and understanding the requirement.    Coding and developing the modules (batch processing) in Pyspark and other big data tools.    Modelling Snowflake DWH, writing procedures to process data in Snowflake, loading data into/unloading data from Snowflake.      Reviewing code of the team members and ensuring the quality and standards of deliverables.  Teamwork and communication  Working with 6-8-member team to plan and execute the work.  Coordinating directly with client to gather the requirement.    Client  PRA Health Science  Project Name  Mediastat Extract  Duration  Jan 2015 till Nov 2017  Tools and technology  Pyspark, Python, Hadoop, HDFS, Hive, AWS, Unix scripting, Oracle12C, SQL, Snowflake, Autosys  Project  Summary  Mediastat is analytics application provides insights into patient behaviour by following patients over time and examining behaviours of interest, such as Adherence and Source of Business. Persistency/Switching, Compliance, Medication Possession Ratio, Length of Therapy, One & Done, and Consumption.  Responsibility    Some of the key responsibilities while executing this project are as listed below   Redesigning the Mediastat extract application using spark and big data technologies, completing POC on the same and presenting to client.  Coding and developing the project in Pyspark.  Delegating the task among team members. Leading and mentoring the team.  Reviewing code of the team members and ensuring the quality and standards of deliverables.  Teamwork and communication  Working with 6-8-member team to plan and execute the work.  Coordinating directly with client to gather the requirement.    Company  Capgemini  Project Name  Sola Horizon 2.0  Duration  Jun 12 till Nov 2014  Tools and technology  Oracle (PL/SQL, SQL), Unix shell scripting, TFS, SQL Developer, Toad, Putty  Project Review    SOLA Horizon is a planning and forecasting application developed to handle the supply chain management of Sony Electronics in Latin America.  Operations in Sola Horizon include sales plan management, purchase order management, sales forecast, warehouse and dealer allocation, logistics management, inter country sales etc. This project is an advanced and extended version of the project Horizon 2.0  Responsibility        Gather Requirement and prepared Technical Document.    Involve in coding, unit testing for making Test Cases.    Involve in debugging PL/SQL objects    SQL Performance tuning by using Index and Explain plan    Involve in using advance features in Oracle 11g.    Coordination for UAT and handling deployment activities.    Experience in Unix Shell Scripting.  Teamwork and communication  Worked with 30-member team to plan and execute the work along with POC.    Company  Bajaj Allianz  Project Name  Policy Servicing and Cashiering  Duration  Jan 11 till Apr 12  Tools and technology  Oracle (PL/SQL, SQL), Unix shell scripting, Clear Case, SQL Developer, Toad, Putty  Project Review  Bajaj Allianz Life Insurance is a union between Allianz SE, one of the largest Insurance Company and Bajaj Finserv. Cashiering module used throughout PAN INDIA for collection of premiums from policy holder under cashiering different product specific validations was configured as per the terms and condition of the products. The customers pay policy amount for new premium, renewal premium etc for which we configure different types of accounting entries for different types of transaction and pay modes checking entries for receipts generated through cashiering module.  Responsibility    Developed modules using data base objects like (Cursor, Function, Procedure, Package, Collections, Triggers) and oracle Forms.    Involve in coding, unit testing for making Test Cases.    Involve in debugging PL/SQL objects    SQL Performance tuning by using Index and Explain plan    Experience in Unix Shell Scripting  Teamwork and communication  Worked with 10-member team to plan and execute the work along with POC.      Achievements and Awards:  Have received twice Spot Award appreciation from Sony client.  Have brief Onsite Experience to Mexico for UAT and GO-LIVE activities.      PERSONAL SKILLS  Have competent communication skill.  Have the ability to deal with different kinds of people and handling difficult situations calmly.  Believe in group work rather than individualism.  Always think positive which helps to deal all kind of situations in life.  Have the interest to work on different technologies and quick learner.        PERSONAL DETAILS  Name                                                        Pradyumna Rana  Sex                                                      Male  Date of Birth                                          29th May, 1986  Marital Status  Married  Nationality                                        Indian  Languages Known                           English, Hindi, Oriya", "tokens": [{"text": " Bajaj Allianz Life Insurance co. ltd", "start": 2264, "end": 2301, "token_start": 376, "token_end": 382, "entityLabel": "company"}, {"text": "from Jan 11 to Apr 12", "start": 2240, "end": 2261, "token_start": 370, "token_end": 375, "entityLabel": "period"}, {"text": "Capgemini India Pvt Ltd", "start": 2355, "end": 2378, "token_start": 398, "token_end": 402, "entityLabel": "company"}, {"text": "from Jun 12 to Nov 14", "start": 2330, "end": 2351, "token_start": 391, "token_end": 396, "entityLabel": "period"}, {"text": "Harman Connected Services", "start": 2437, "end": 2462, "token_start": 417, "token_end": 419, "entityLabel": "company"}, {"text": "from Nov 14 to Feb 21", "start": 2412, "end": 2433, "token_start": 410, "token_end": 415, "entityLabel": "period"}, {"text": "IBM", "start": 2567, "end": 2570, "token_start": 441, "token_end": 441, "entityLabel": "company"}, {"text": "Feb 21 to till date", "start": 2544, "end": 2563, "token_start": 435, "token_end": 439, "entityLabel": "period"}], "relations": [{"child": 370, "head": 376, "relationLabel": "duration"}, {"child": 391, "head": 398, "relationLabel": "duration"}, {"child": 410, "head": 417, "relationLabel": "duration"}, {"child": 435, "head": 441, "relationLabel": "duration"}]}, {"document": "                                                    Solutions architect CV template                                   AREAS OF EXPERTISE                      Analytics     Solution Architecture   Big Data     Artificial Intelligence        Chatbots        Machine Learning    IoT    SaaS    Digital Transformation    Project Management        Agile Methodologies            PERSONAL SKILLS            Business Acumen   Client Focused     Solutions driven        Creativity & innovation         Decision making               PERSONAL DETAILS               Rajib Bhattacharya     Flat 10A, Tower 1            Diamond City West     18 Ho-Chi-Minh Sarani    Kolkata  700061 India              M: +91 90510 71500          E: rajib.bhattacharya@gmail.com          DOB: 07/03/1980      Nationality: Indian     Passport: P8475531     Valid Till: March, 2027                                       Rajib Bhattacharya    Global Leader - Data Integration      PERSONAL SUMMARY           Highly accomplished and motivated professional with over 15 years of contribution   in individual and leadership roles involving enterprise software and presales and   consulting services, catering to a variety of different areas across many industry   verticals, including Business Intelligence, Advanced Analytics, AI and IoT.        ACADEMIC QUALIFICATIONS             Post Graduate Program in Artificial Intelligence and Machine Learning    University of Texas at Austin and GreatLearning (Great Lakes Executive   Learning) in 2020     MCA from West Bengal University of Technology in 2005      B.Sc. from University of Calcutta in 2002          TECHNICAL AND FUNCTIONAL SKILLS           Reporting Tools: Cognos Analytics, Tableau, Power BI     Databases: SQL Server, DB2, Oracle, Netezza      Big Data Tools: Cloudera, Hortonworks    Data Science: R, Python, SPSS     Cloud: IBM Cloud, AWS     AI: IBM Watson      Financial Performance Management: IBM Cognos TM1, Planning Analytics     Languages: Python, SQL, Java, XML      Application Server: Websphere       Domains: Telecom, BFSI, Retail, Supply Chain, Public Sector         CERTIFICATIONS           Certified Scrum Master      Prince 2 Practitioner      Introduction to R Programming  Microsoft      International Scrum Master Foundation  Scrum.as      IBM Certified Specialist  Cognos 8 BI      IBM Certified Administrator  Cognos 10 BI      HDP Operations: Hadoop Administration  Hortonworks     Partnerworks Accredited Technical Sales Professional - Hortonworks    Automation Anywhere Certified Advanced RPA Professional        PATENTS    Filed patent Method of generating relevant usage tips based on user and device   context (US20160203005) in Jan 2015. Granted (Patent #10061598) in Aug 2018.           WORK EXPERIENCE                     1. Working as Global Leader  Data Integration Management at Cargill (Mar 2020  Till Date):        Leading worldwide Analytics teams based in NA, LATAM, APAC, EMEA     Working with different vendors to implement different projects     Leading creation of data lake for Cargill Business Operations and Supply Chain organization     Complete ownership of development and support for Analytics      Working as a worldwide SME for Analytics in different projects      2. Worked for Gulf Business Machines, Bahrain as Pre-sales Consultant - Analytics (Apr 2018    Jan 2020) Job Profile:      Handling all Analytics, Big Data, AI and IoT software and services technical sales for GBM in Bahrain     Designing solutions on IBM Cloud, AWS     Responsible for RFI, RFP, RFQ responses, PoCs and demos on Analytics, AI, Big Data and IoT solutions (both   onpremise and cloud)     Engaging with Services teams for streamlining delivery     Handling Business Partners and Clients in multiple domains           3. Worked for IBM India Pvt. Ltd., Kolkata as Solution Architect (May 2008  Mar 2018)  Job   Profile (I have worked in the following roles during my tenure at IBM):      Solution Architect / Technical Project Manager  IBM Global Business Services  Cognitive Business Decision Support     Defining Solution Architecture for Analytics, Big Data Solutions (both on-premise and cloud)     Responsible for consulting and delivery of Analytics, Data Science and Big Data engagements     Technical Project Management of Analytics, Data Science and Big Data engagements     Analyzing new business requirements and changes to existing functionalities     Working with Sales teams for RFI, RFP responses, PoCs and product demos for Analytics, Data Science, Big Data      Responsible for Technical Project Management      Team handling and mentoring junior developers      Designing and delivering Cognos Analytics, Big Data, Data Science and Artificial Intelligence trainings to Clients and   Business Partners      Working as Analytics SME for different projects             Solution Architect  IBM SWG Solutions Group  IBM Watson Supply Chain     Defining Solution Architecture for the Analytics (BI, Big Data, FPM) Solutions (both on-premise and cloud)     Responsible for consulting and delivery of Analytics, Data Science and Big Data engagements     Technical Project Management of Analytics, Data Science and Big Data engagements     Analyzing new business requirements and changes to existing functionalities      Working with Sales teams for RFI, RFP responses, PoCs and product demos for Analytics, Data Science, Big Data      Responsible for Technical Project Management      Team handling and mentoring junior developers      Designing and Delivering Cognos Analytics, Big Data trainings to Clients and Business Partners      Working as Analytics SME for different projects         Sr. Consultant, Business Analytics Professional Services - IBM Software Labs - IBM Business Analytics      Responsible for consulting and delivery of Analytics (BI, FPM) engagements     Analyzing new business requirements and changes to existing functionalities      Assisting Pre-Sales / Field teams in Analytics (BI, Big Data) PoCs and demos      Team handling and mentoring junior developers      Working as Analytics SME for different projects               4. Worked for Cognos, an IBM Company, Pune as Technical Support Engineer (Jan 2008  Apr   2008)          Job Profile:      Enterprise Support for IBM Cognos BI 8.x and 10.x (Framework Manager, Report Studio, Query Studio, Analysis   Studio, Transformer etc.) and IBM Cognos Reportnet       Solving report development and data modeling issues raised by enterprise level clients      Logging bugs and enhancements for the product and preparing test cases for development team.      Point of contact between customers and development team      Writing technical articles on Cognos         5. Worked for Symantec Software India Pvt. Ltd., Pune as Technical Support Engineer (Sep 2006    Jan 2008)          Job Profile:      Team handling and mentoring junior team members      Resolving Database and Unix File System related issues of production databases      Enterprise Support for Indepth for Oracle (Oracle DBA Tool)      Enterprise Support for Indepth for SQL Server (SQL Server DBA Tool)      Point of contact between customers and development team      Authoring Technical Articles as per requirement         6. Worked for Amdocs Business Services Pvt. Ltd., Pune as Production Support Analyst      (Apr 2006  Aug 2006)           Job Profile:      Handling Data Center Operations      Maintenance and support for Data Warehouse      Resolving Database and Unix File System related issues      Performance Tuning of Production Databases      Generation of various reports for billing cycles        7. Worked for IBM Global Services, New Delhi as Systems Executive (Jun 2005  Mar 2006) [Contract Employee - on roles of   Intec Infocom Pvt. Ltd. and TeamLease Services Pvt. Ltd.]          Job Profile:      DBA for application databases (Oracle and SQL Server)      Maintenance and support for Data Warehouse      Development and support of various web applications (Core Java,JSP,Oracle 9i)      PL/SQL programming       Handling Telecom Operations         CLIENTS SUPPORTED    Celcom (Malaysia), Airtel, Idea, Vodafone, Deutsche Telekom (Germany), Bank ABC (Bahrain), Royal Bank of    Canada (Canada), TD Bank (Canada), British Petroleum (UK), Nationwide (USA), Unilever, American Express,   Bombay Stock Exchange, Sandvik (Sweden), Henkel (Germany), Cipla, DanOne (UK)          ACHIEVEMENTS            Awards    A++ Excellence , Symantec - 2007      Excellence in Customer Support, IBM - 2009      Appreciation certificate from IBM ISL for blogging on IBM Technologies, IBM - 2010      IBM ISL Delivery Excellence and Leadership Award, IBM - 2013      Quality Software Engineering Award, IBM - 2015      University Relations Award, IBM  - 2014, 2016, 2017     IBM SWG Lab Services Award, IBM - 2017     IBM ISL BU Award, IBM - 2017     Managers Choice award in 2015, 2017             Speaker at Conferences     Speaker on Business Intelligence, Techgig.com webinar - January 2012      Speaker on Business Intelligence at IMTC/ISTC Conference, IBM India  2012, 2013, 2014, 2017      Speaker on Big Data and Business Intelligence at IOD / Insight Conference, IBM US  2012, 2013, 2014,   2015     Speaker on Big Data and Business Intelligence at IBM Regional Technical Conference, IBM India  2012,   2015     Speaker on Blockchain at BlockOn Conference, GBM Bahrain - 2018               Author and Reviewer    Authored Building Cognitive Applications with IBM Watson Services: Volume 1 published as a IBM   Redbook:    http://www.redbooks.ibm.com/redbooks.nsf/redbookabstracts/sg248387.html?Open         Authored Getting started with Data Warehousing book published under IBM Big Data University   program.     https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/   page/FREE+ebook+-+Getting+Started+with+Data+Warehousing         Worked as a Technical Reviewer for the book Mobile Artificial Intelligence Projects published by Packt   Publishing:    https://www.packtpub.com/big-data-and-business-intelligence/mobile-artificial-intelligence-projects         Worked as a Technical Reviewer for the book Blockchain Quick Reference published by Packt   Publishing:    https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference               Training Delivery for IBM Programs          Delivered 50+ trainings and workshops on Business Analytics, Big Data, Cognitive Computing and   Data Science at top Business schools and Engineering colleges in India as a part of IBM Career    Education and IBM University Relations programs between 2010-2018     http://www.redbooks.ibm.com/redbooks.nsf/redbookabstracts/sg248387.html?Open http://www.redbooks.ibm.com/redbooks.nsf/redbookabstracts/sg248387.html?Open https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.ibm.com/developerworks/mydeveloperworks/wikis/home/wiki/Big+Data+University/page/FREE+ebook+-+Getting+Started+with+Data+Warehousing https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference https://www.packtpub.com/big-data-and-business-intelligence/blockchain-quick-reference  ", "tokens": [{"text": "Cargill", "start": 2842, "end": 2849, "token_start": 436, "token_end": 436, "entityLabel": "company"}, {"text": "Mar 2020  Till Date", "start": 2851, "end": 2870, "token_start": 438, "token_end": 442, "entityLabel": "period"}, {"text": "Gulf Business Machines", "start": 3259, "end": 3281, "token_start": 507, "token_end": 509, "entityLabel": "company"}, {"text": "Apr 2018    Jan 2020", "start": 3328, "end": 3348, "token_start": 520, "token_end": 524, "entityLabel": "period"}, {"text": "IBM", "start": 3792, "end": 3795, "token_start": 609, "token_end": 609, "entityLabel": "company"}, {"text": "May 2008  Mar 2018", "start": 3844, "end": 3862, "token_start": 620, "token_end": 624, "entityLabel": "period"}, {"text": "Cognos", "start": 6138, "end": 6144, "token_start": 998, "token_end": 998, "entityLabel": "company"}, {"text": "Symantec Software India Pvt. Ltd", "start": 6724, "end": 6756, "token_start": 1102, "token_end": 1108, "entityLabel": "company"}, {"text": "Sep 2006    Jan 2008", "start": 6795, "end": 6815, "token_start": 1115, "token_end": 1119, "entityLabel": "period"}, {"text": "Amdocs Business Services Pvt. Ltd", "start": 7250, "end": 7283, "token_start": 1192, "token_end": 1198, "entityLabel": "company"}, {"text": "Apr 2006  Aug 2006", "start": 7327, "end": 7345, "token_start": 1206, "token_end": 1210, "entityLabel": "period"}, {"text": "IBM", "start": 7638, "end": 7641, "token_start": 1256, "token_end": 1256, "entityLabel": "company"}, {"text": "Jun 2005  Mar 2006", "start": 7691, "end": 7709, "token_start": 1266, "token_end": 1270, "entityLabel": "period"}, {"text": "Jan 2008  Apr   2008", "start": 6198, "end": 6218, "token_start": 1010, "token_end": 1015, "entityLabel": "period"}], "relations": [{"child": 438, "head": 436, "relationLabel": "duration"}, {"child": 520, "head": 507, "relationLabel": "duration"}, {"child": 620, "head": 609, "relationLabel": "duration"}, {"child": 1010, "head": 998, "relationLabel": "duration"}, {"child": 1115, "head": 1102, "relationLabel": "duration"}, {"child": 1206, "head": 1192, "relationLabel": "duration"}, {"child": 1266, "head": 1256, "relationLabel": "duration"}]}, {"document": "ALS Ravi Teja  als.a.raviteja@gmail.com  Contact: +91 9885878868      JOB OBJECTIVE:  Seeking a role in a company where I can use my Business Intelligence knowledge and experience to benefit the   organization and improve myself.      PROFESSIONAL SUMMARY:  Around 8 years of experience in Analysis, Design, Development, Implementation, Testing and Support of   Data Warehousing using IBM Infosphere Information Server DataStage 9.1 and DataStage 11.7  Have experience in Teradata, DB2 and Biq-Query.  Proficiency in developing SQL with various relational databases like Oracle, DB2, Teradata. Biq-Query  Have extensively worked in developing ETL program for supporting Data Extraction, transformations and   loading using DataStage.  Used the IBM WebSphere DataStage designer to develop jobs for extracting, cleansing, transforming,   integrating, and loading data into data warehouse database.  Independently perform complex troubleshooting, root-cause analysis and solution development.  Retrieved the source data from of DB2 tables, Sequential files, web services  Handled errors using Exception Handling extensively for the ease of debugging and displaying the error   messages in the application.  Used the DataStage Director and its run-time engine for testing and debugging its components, and   monitoring the resulting executable versions  Developed number of DataStage ETL jobs based on business requirements using various DataStage Stages as Copy, Modify, Aggregator, Filter, Funnel, Join, Lookup, Merge, Datasets, Sequential files and Transformer etc  Provided production support and performed enhancement on existing multiple projects.  Expert in Data Warehousing techniques for Slowly Changing Dimension phenomenon (SCD), Surrogate   Key assignment and CDC (Change Data Capture).  Ability to meet deadlines and handle multiple tasks, decisive with strong leadership qualities, flexible in work schedules and possess good communication skills.  Team player, Motivated, able to grasp things quickly with analytical and problem-solving skills.  Comprehensive technical, oral, written and communicational skills.  Hands on expertise with CI/CD tools like Git, Jenkins, G3, Maven.  Have experience with different bug tracking tools like JIRA, quality centre.    TECHNICAL SKILL SET:  ETL: IBM Infosphere Information Server DataStage 11.7  Database: SQL Server 2008, Oracle 10g, DB2, Teradata, Biq-Query  Platforms:  Windows, UNIX, Python  Others: MS Word, MS Excel, Outlook, PowerPoint, Spyder  Scheduling Tool: Control-M, Juniper Scheduler      Academic Profile:  MCA from Anil Neerukonda Institute of Technology & Sciences affiliated to Andhra University in year 2013.    Working Experience:  Currently working at HSBC as Senior Software Engineer from Aug 2018 -Till Date in HYDERABAD.  Worked at Capgemini India Pvt.Ltd from June 2013-Aug 2018 as Consultant.    PROFESSIONAL EXPERIENCE:  The details of the various projects that I have handled are listed here.    Title-1    HSBC   Team                                  IHUB  Methodology                   Agile  Client      HSBC North American Holdings  Location  HSBC, Offshore  Team Size  10  Duration  Aug 2018  Present      About Client                     HSBC Bank USA, National Association, an American subsidiary of UK-based HSBC Holdings plc, is a bank with its operational head office in New York City and its nominal head office in McLean, Virginia (as designated on its charter). HSBC Bank USA, N.A. is a national bank chartered under the National Bank Act, and thus is regulated by the Office of the Comptroller of the Currency (OCC), a part of the U.S. Department of the Treasury.    Project Description         Intelligence Hub (IHUB) is the desired solution to unlock the prize of data as a critical and valuable asset, providing customer experience improvement, better decision making and commercial value following Global Architecture and Operating Design based on Agile                                               methodology. Intelligence Hub is an enduring capability to enable personal and relevant                                               customer experiences, improved operational effectiveness, improved business                   decisioning based upon data and insight and will support the regulatory agenda.       Roles & Responsibilities:      Team Lead/Sr.Developer    Uploaded 53 sources systems projects onto cloud.  Hands on expertise with Juniper tool to get the data from On-prem to GCS Bucket.  Worked on Python Scripts to get the data from GCS buckets to Google Biq Query Tables.  Used Control-M for scheduling the jobs on Cloud platform.  Migration of On-Prem jobs DataStage jobs to Cloud Supported using Spyder.  Hands on expertise with CI/CD tools like Git, Jenkins, G3, Maven.  Working with JIRA to track project defects and tasks.  Used GIT source control systems to manage code.    Environment: GCP, Juniper Scheduler, Cloud Storage, Biq-Query, Spyder, Web-ui, Control-M, SDK, DataStage,                            CI/CD    Title-2    HSBC   Team                                 HDW (Household Data Warehouse)  Methodology                   Agile  Client      HSBC North American Holdings  Location  Cap Gemini, Offshore  Team Size  18  Duration  Feb 2016  Aug 2018      About Client                     HSBC Bank USA, National Association, an American subsidiary of UK-based HSBC Holdings plc, is a bank with its operational head office in New York City and its nominal head office in McLean, Virginia (as designated on its charter). HSBC Bank USA, N.A. is a national bank chartered under the National Bank Act, and thus is regulated by the Office of the Comptroller of the Currency (OCC), a part of the U.S. Department of the Treasury.    Project Description         The objective of HDW is basically to have an in-house management of customer data to   support the Banks Revenue and Deposit Growth strategy. Create a Central accessible   data store in HSBC for deposit account and transaction data, with supporting customer-  level data, to improve our informational capabilities in terms of more frequent, more   detailed information to ultimately drive deposit growth. This is envisioned as Central   Enterprise Data warehouse for HBUS PFS Data.    Roles & Responsibilities:      Team Member/Developer/Support    Monitor all data stage jobs and provide production support to all.  Used CONTROL-M Scheduler to schedule the DataStage jobs to run the daily jobs and weekly jobs  Written UNIX shell script to handle automation of daily batch   Monitor all data stage jobs and provide production support to all.  Working on DataStage Designer - for Extracting Source Transactional Database and transforming and then load into Target Database.  Used Sequential file, Oracle Enterprise, Join, Lookup, transformer, file set, datasets, change capture, filter, and remove duplicate stage for designing the jobs in the DataStage Designer.  Used Data Stage Director for running, monitoring and scheduling the jobs  Involved in Performance Tuning Jobs process, identifying and resolving performance Issues.  Worked with DataStage Designer for Export & Import Jobs  Worked on various Stages like Transformer, Aggregator, Lookup, Join, Merge, Remove duplicates,   Funnel, Filter stages etc.  Automated the process of data stage jobs.  Implementation of all jobs in production for all newly changed jobs to improve performance.    Environment: Data stage, UNIX, Control-M, WINSCP, DB2, Teradata      Title-3   Capital One Financial - CRS Data warehouse  Client      Capital One  Methodology                   Agile  Location  Capgemini, Offshore  Team Size  22  Duration  Nov 2014 - Feb 2016      About Client                    Capital One Financial Corporation is a U.S.-based bank holding company specializing in credit cards, home loans, auto loans, banking and savings products. When measured in terms of total assets and deposits, Capital One is the eighth largest bank holding company in                                                the United States.         Project DescriptionThe Project is of team - 22 @ offshore. This Project involves in creating the feeds for Customer Account, Customer Suppression and Internal Fraud analytics data COF Marketing   decisions and reporting. It majorly involves in Integrating the existing Cards and Retail   services for North America Data Warehouse Applications into existing Capital One CDW   and PDW environment apart from migrating the data from HSBC Owned servers to COF   Owned servers which involved some changes to the existing system. It is also used to   interpret claims processing and to respond to customer inquiries regarding claims   processing.       Roles & Responsibilities:      Team Member/Developer/SupportEnhancements     Resolving abends and updating the resolution in the repository  Meeting the SLA standards  Executing adhoc tasks if assigned  Worked with the Business process analyst tothoroughly understand the different business processes and          requirements.  Used Data Stage Director for running, view, monitoring and scheduling the jobs.  Worked on Information Analyser for column analysis, primary key analysis and foreign key analysis and   developed detailed data quality assessment (DQA) reports  Worked used DataStage Designer to develop customized rule sets for standardizing fields like individual name, organization names, address and phone numbers  Extensively used DataStage Designer to design and develop jobs for extracting, cleansing, transforming,  integrating, and loading data using various stages Remove Duplicate, Surrogate Key, Aggregator, Funnel, Join, Merge, Lookup, Change Capture, Change Apply and Copy.  Extensively worked on migrating DataStage jobs from development to test and to production   environments.  Performed Unit Testing, Regression Testing, and User Acceptance Testing (UAT) for every code change   and enhancement.  Sending consolidated reports and logs to client daily  Working on different weekly and monthly reports  Provided operational support to all production practices on holidays and weekends.    Environment: Data Stage, UNIX, Db2 Control-M.      Title-4   Capital One Financial FDWC CARDS  Client      Capital One  Methodology                   Agile  Location  Capgemini, Offshore  Team Size  22  Duration  June 2013 - Oct 2014      About Client                    Capital One Financial Corporation is a U.S.-based bank holding company specializing in credit cards, home loans, auto loans, banking and savings products. When measured in terms of total assets and deposits, Capital One is the eighth largest bank holding company in the United States.         Project Description        The Company is part of a diversified financial services organization offering a spectrum of   financial advisory. The project main purpose is gets daily data from different source   systems and validate the bad guys data and generate reports for end users and the   project is to track all the credit card transaction and load it in to the target tables for the end users. The framework of the project was to design, develop and maintain a data   warehouse for the clients to access their day to day, monthly and yearly activities.         Roles & Responsibilities:      Team Member/Developer/SupportEnhancements     Resolving abends and updating the resolution in the repository  Worked used DataStage Designer to develop customized rule sets for standardizing fields like individual name, organization names, address and phone numbers.  Extensively used DataStage Designer to design and develop jobs for extracting, cleansing, transforming, integrating, and loading data using various stages like Standardize, Match Frequency, and Reference Match, UN duplicate Match, Remove Duplicate, Surrogate Key, Aggregator, Funnel, Join, Merge, Lookup,   Change Capture, Change Apply and Copy.  Extensively worked on migrating DataStage jobs from development to test and to production   environments.  Performed Unit Testing, Regression Testing, and User Acceptance Testing (UAT) for every code change   and enhancement.  Used CONTROL-M Scheduler to schedule the DataStage jobs to run the daily jobs and weekly jobs    Environment: Data Stage, UNIX, Db2 Control-M.      Award and Accolades Received    Got RAISING STAR award in Banking BU Quarterly Award - Q3 (2014).   Received appreciations from the Client Manager during TRANSISITION of our project.  Awarded with the Pat on Back for independently maintaining three projects simultaneously.  Top rated for the annual appraisal consecutively for the years 2018-2019 & 2019-2020.  Receiving STAR PERFORMED from HSBC for the year 2019.        Certifications    Completed Google Cloud  Associate Cloud Engineer Certification.  Completed IBM DB2 certification.  Completed IBM DataStage 9.1 certification.      |INTERNAL|  |INTERNAL|", "tokens": [{"text": "HSBC", "start": 2726, "end": 2730, "token_start": 492, "token_end": 492, "entityLabel": "company"}, {"text": "Aug 2018 -Till Date", "start": 2764, "end": 2783, "token_start": 498, "token_end": 501, "entityLabel": "period"}, {"text": "Capgemini India Pvt.Ltd", "start": 2809, "end": 2832, "token_start": 508, "token_end": 512, "entityLabel": "company"}, {"text": "June 2013-Aug 2018", "start": 2838, "end": 2856, "token_start": 514, "token_end": 516, "entityLabel": "period"}], "relations": [{"child": 498, "head": 492, "relationLabel": "duration"}, {"child": 514, "head": 508, "relationLabel": "duration"}]}, {"document": "RESUME                                                                                                                                                                                            RaviKumar KRS    +91-8105629857    hadoopravi2020@gmail.com    ravikumar.kodiy@hcl.com            Over 10+ years experience, seeking a challenging role in Software/Artificial Intelligence/Machine Learning. Have worked cross-functionally with big data engineering, marketing, analytics, product and brand teams.      Certification           :  Stanford University MACHINE LEARNING certification By Prof Andrew NG  Verification Link  :  coursera.org/verify/9Q7MZMFNM2F5    Certification           :  Stanford University NEURAL NETWORKS and DEEP LEARNING certification By                                               Prof Andrew NG  Verification Link  :  coursera.org/verify/CG59LHHJ2AA9    Certification           :  Stanford University DEEP LEARNING Specialization Certification By                                               Prof Andrew NG  Verification Link  :  coursera.org/verify/specialization/LEF6DUR8LJG5      Github              : Active contributor in github machine learning  Verification Link  : https://github.com/ravithilak/Machinelearning        Technical Skill Set    Programming Language  Python, Pig Latin, Scala, Hive QL   Tools  Putty, VMware, WinScp, Eclipde IDE, IntelliJ IDE,    Libraries  NumPy, Pandas, Scikit-Learn, Keras, Tensorflow, NLTK   AI/Cognitive  Machine Learning  Classification, Regression, Clustering  NLP  Text mining, Voice to text   Deep Learning  ANN, CNN, RNN Neural Network   Frameworks   HADOOP (Map-Reduce), Spark, Kafka  DB  MySQL,  Mongo DB  Hadoop Distributions  Apache Hadoop, Cloudera  Cloud Technologies  AmazonWebService AWS, Microsoft Azure    Data Layer Skill Set    Data Sourcing  ADLS, ADF Azure Data Factory   Data Injestion  SparkSql   Data Landing   HDFS, S3  Data Visualization  PowerBI  Database  HBase, RDBMS, RDS  Data Access  Pig, Hive  Data Scheduling  Oozie  Data Processing  Spark, Hadoop (Map-Reduce I and II ) YARN      Career progression      Research Scientist  at SAMEER ,  (June 2010 to March 2015)  Machine Learning Engineer  at Wipro ,  (April 2015 to March 2019)  Technical Lead (Data Science and  Big Data)  at Price WaterHouse Coopers (March 2019 to August 2019)  Lead Data Analyst(Data Science)  at Mindtree ,  (Jan 2020 to October 2020)  Technical Lead (Data Science) at HCL, (Oct 2020 to Present)      Education Graph    M.E  ComSys with 70% from Thiagarajar College Engineering, Madurai. (2010) (Top 5 Institution in TN)  B.E  ECE with 72% from Jayaram College of Engineering and Technology, Trichy. (2008)  12th Higher Secondary with 80% from Bishop Heber Higher Secondary School, Trichy. (2004)  10th Secondary with 89% from Bishop Heber Higher Secondary School, Trichy. (2002)    Expertise  10+ Years of extensive experience in IT including Data/Pattern Analysis, Prediction model building, Business process improvements, Visualization, Process implementation, Big Data Ecosystem, SPARK ecosystem and Hadoop ecosystem.   Data Analysis, provide insights and provide necessary recommendations.   Exploratory data analysis, feature selection   Building prediction models to predict the business objectives there by assessing the risk of not meeting the business goals   Machine learning algorithms such as Decision tree, Nave Bayes, k-NN, SVM and Neural Networks.  Time series analysis and building models based on the scenario to forecast the business interests.  Text mining and sentiment analysis.  Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and vice-versa.  Hands on experience in application, RDBMS, and Linux shell scripting.   Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and vice-versa.  End to end architecture using Kafka, Spark and mongoDB     Projects Executed  Project 1:    TeseCase Optimization  Mindtree    Problem Statement:  This project describes the observations and challenges faced during the task of clustering the count of defects that would occur across functionalities for each release by analyzing the similarities of the instances of the defect.  Increase in number of testing, requires optimization for reducing the clusters into similar groups. According to the actions or errors, similar groups are categorized using Kmeans clustering algorithm and creating the number of topics. NLP and Tfidf vectorizers are used to preprocess and vectorize the errors in text format.      AI/Machine learning techniques used:  Clustering mechanism is used to detect group the different test severities  Tools/libraries used:  Azure, Jupyter notebook, K means, NLP,  Machine Learning  Project 2: Price Waterhouse Coopers    DAMS: (Digital Asset Management System)     Problem Statement:  Increase in number of digital assets, solution to monitor and processes logs from client machines. These logs can be from application servers, Devices, Routers, Database servers, which are streamed into our infrastructure. Unauthorized entries and password entries monitoring is necessary for asset management system.    .    Applied Solution:   Big data cluster provides Highly available and scalable solution. HDFS is able to replicate data across the cluster for faster and failsafe. Kafka and flume are used to stream data from client environments into their message queues and store them in HDFS. Spark is used to submit python jobs which process this data and apply machine learning algorithms in ITSM.        AI/Machine learning techniques used:  Classification mechanism is used to detect unauthorized login attempts from end users and successive wrong entries using NLP (TFIDF)    Tools/libraries used:  Python/ NLTK/SCIKIT/KAFKA/    Project 2:  MARKETING TO FREQUENT FLIERS  Tools: Python  Techniques:  Cluster Analysis (hierarchical and K-means clustering)  Objective: The goal is to try to identify clusters of passengers for eminent Airlines that have similar characteristics for the purpose of targeting different segments for different types of mileage offers.   Analysis: Applied the hierarchical clustering (Euclidean distance) and plotted the dendrogram. Identified the 3 cluster- High spending, medium spending and Low spending - with help of dendrogram and compared the results with k-means clustering. By making use of this data, airlines can announce various offers to various segments  Project 3:  TEXT ANALYTICS USING MACHINE LEARNING NLP  Tools: Python  Techniques:  Nave Bayes Machine Learning algorithm  Objective:  To classify the text records which are logged by users of a large application of a Health Insurance firm in to different classes and route them to respective teams to act on requests.  Users log their requests related to enquiries, issues they face and any clarification they needed. Also third party log the requests related to claims, clarifications etc. Currently classification is being done manually by domain experts.  Considered the data in billing division, defined and standardized the classes definitions. Got training data set coded manually with the standardized definitions. Divided the data set in to training and validation sets in 80: 20 ratios respectively. Data pre- processing techniques included removing stop words, extra white spaces, numbers, stemming the words. Tokenization was done using DocumentTermMatrix and sparse matrix was built using Natural Language Processing NLP . Built the Nave Bayes model and validated the model using validation set. Accuracy of the model was improved by changing the Laplace constant. Finally, model accuracy was at 83%   POC 1   Query based Recommendation System    Technology: Python: standard NLP packages, NLTK, Stanford Core NLP, Numpy, Gensim (word2Vec model), Mallet, Classifiers, Sentiment analysis    Description: The recommendation engine responds to unstructured queries like best pizza place in Pune. The response is in the form of a definite recommendation of restaurants.     Responsibilities:   Sentiment analysis and nave Bayes classifier to tag reviews.    Implementation of word2vec model in python.       SAMEER -  A Join Project with CDAC , Chennai    PROJECT 1 :    Team Size: 5  Environment: Cloudera, AWS EC2    Brief:  Refine and Visualize Server Log Data.  The HTTP log analyzer provides a fast and accurate data analysis for public web pages. Each day after midnight, a daily log file of all interactions for the previous 24 hours is pushed from an external source to a spooling directory. The directory structure for the log file will be /YYYY/MM/DD.  Responsibilities:  Involved in installing, configuring and managing Hadoop Ecosystem components like HDFS, Hive, Pig, Sqoop and Flume  Worked on Linux shell scripts for business processes and with loading the data from different systems to the HDFS.  Used Pig as ETL tool to do Transformations and some pre-aggregations before storing the data onto HDFS.  Developed scripts to automate the creation Sqoop jobs for various workflows.  Involved in generating analytics data using MapReduce programs written in core java.  Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries.  Configured and designed Pig Latin scripts to process the data into a universal data model.  Involved in creating Hive internal and external tables, loaded them with data and writing hive queries which requires multiple join scenarios.  Created partitioned and bucketed tables in Hive based on the hierarchy of the dataset.  Used Kafka for Log aggregation to collect physical log files from servers and puts them in the HDFS for further processing.  Configured deployed and maintained multi-node Dev and Test hadoopClusters.  To analyze data migrated to HDFS, used Hive data warehouse tool and developed Hive queries.  Implemented Spark using Scala and SparkSQL for faster testing and processing of data.  Designed and developed MapReduce programs for data lineage.  Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data.  Developed workflow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.  Created ETL (Informatica) jobs to generate and distribute reports from MySQL database  Responsible for troubleshooting MapReduce jobs by reviewing the log files.  Experienced in loading data from Unix file system to HDFS.  Environment: Hadoop, Map Reduce, Hive, PIG, Sqoop, Kafka, Spark, Core java, Oracle, ETL, Linux, Unix, Shell Scripting.    Responsibilities:    Analyze Log data to understand the contents and structure.  Define flume configuration to gather streaming log data from spooling directory into HDFS. Verify the correctness.  Develop PIG queries for user specific query.  Heavy traffic analysis per hour.  Search term analysis.  Peak Time analysis.  Store the result back to HDFS.    PROJECT 2 :    Team Size: 4  Environment: Apache Hadoop, AWS EC2    Brief:  Arrange product Application                               The APA is the customization of the key cat attributes of the products. The key cat attributes are the key attributes of a product like size, wt etc which are derived from the central database. Some additional attributes are also defined over the existing key set attributes. For each of the client a separate subset central database is maintained so that each of the clients has it own copy of the master central database.     Responsibilities:    Installing Hadoop and Setting up Hadoop cluster.  Define flume configuration to gather streaming log data from spooling directory into HDFS. Verify the correctness.  Develop PIG queries for user specific query.  Working as HDFS and hadoop map reduce admin  Loading files to HDFS and writing map reduce jobs to mine the data.  Understanding and tacking backup of Data node, Name node, Job Tracker, Secondary Name node, Task Tracker.  Troubleshoot map reduce jobs, PIG scripts and HIVE queries.  Involved in Commissioning and decommissioning the Name node  Hadoop Shell commands, Writing Map reduce Programs, Verifying the Hadoop Log Files  Well understanding of HDFS and Map Reduce framework and Hadoop's ecosystem   Developing Application on Map Reduce Programming Model of Hadoop   Writing pig Scripts & loading data to Hive tables.  Exposure on Distributed database like Hbase  Interacted closely with business users, providing end to end support on APA.  Created Technical design documents based on business process requirements.  Involved in the debugging of the coding.          Personal Particulars    Fathers Name   :   Mr. R. Shanmugam  Nationality    :   Indian    Contact Details      Mobile     :   +91-8105629857    Personal Email ID  :  hadoopravi2020@gmail.com      Declaration: I hereby declare that all the information furnished above is correct and true to the best of my knowledge. Will provide necessary reference details based on request.                                                                                                                                                                                K.R.S.RAVIKUMAR       HCL Technologies India               Page 1 of 6", "tokens": [{"text": "SAMEER", "start": 2132, "end": 2138, "token_start": 331, "token_end": 331, "entityLabel": "company"}, {"text": "June 2010 to March 2015", "start": 2143, "end": 2166, "token_start": 335, "token_end": 339, "entityLabel": "period"}, {"text": "Wipro", "start": 2199, "end": 2204, "token_start": 347, "token_end": 347, "entityLabel": "company"}, {"text": "April 2015 to March 2019", "start": 2209, "end": 2233, "token_start": 351, "token_end": 355, "entityLabel": "period"}, {"text": "Price WaterHouse Coopers", "start": 2284, "end": 2308, "token_start": 370, "token_end": 372, "entityLabel": "company"}, {"text": "March 2019 to August 2019", "start": 2310, "end": 2335, "token_start": 374, "token_end": 378, "entityLabel": "period"}, {"text": "Mindtree", "start": 2374, "end": 2382, "token_start": 388, "token_end": 388, "entityLabel": "company"}, {"text": "Jan 2020 to October 2020", "start": 2387, "end": 2411, "token_start": 392, "token_end": 396, "entityLabel": "period"}, {"text": "HCL", "start": 2447, "end": 2450, "token_start": 406, "token_end": 406, "entityLabel": "company"}, {"text": "Oct 2020 to Present", "start": 2453, "end": 2472, "token_start": 409, "token_end": 412, "entityLabel": "period"}], "relations": [{"child": 335, "head": 331, "relationLabel": "duration"}, {"child": 351, "head": 347, "relationLabel": "duration"}, {"child": 374, "head": 370, "relationLabel": "duration"}, {"child": 392, "head": 388, "relationLabel": "duration"}, {"child": 409, "head": 406, "relationLabel": "duration"}]}, {"document": "                                                              SABARI ARULNAGARAJAN Mail arulnagarajan.sabari@gmail.com Mobile No +91 9885678027 Linked In https://www.linkedin.com/in/sabari-arulnagarajan/ DOB 3rd Aug 1991  PROFESSIONAL SUMMARY 7+ years of experience in Data science/ Machine learning and Big Data. Having industry exposure of working with key global banking/manufacturing leaders in Financial, Banking and Industrial Process Automation domains. Currently building a framework that uses AI to classify customer sensitive banking information using Predictive and NLP based models. Experience in developing data models applying machine and deep learning algorithms. Delivered projects handling huge volume(TB) of data using Big Data and its subsystem like Hadoop and Spark. Have sound mathematical knowledge and understanding of machine learning.  SKILLS AND COMPETENCIES Technical Machine Learning, Deep Learning, Python, Image Processing, Natural Language Processing, SQL,Decision Tree,  RandomForest,SVM,TensorFlow,Keras,Pandas,NumPy,Seaborn,Matplotlib,Big Data, Hadoop, Spark, Hive, Sqoop, Shell Scripting.  Domain Banking and Industrial Process Automation(Manufacturing) Certifications Applied Machine Learning - AppliedAI, Neural networks and deep learning - Coursera  Digital Skill Tag as Big Data Professional, Certified Big Data, Python, Agile Developer - Infosys Big Data and Machine Learning Foundation- Google GCP, ISTQB Foundation Certified - ISTQB  PROFESSIONAL EXPERIENCE Technology Analyst  Infosys Ltd. Dec2017Present Leading US Bank (Data Protection Solutions | Machine Learning and Big Data Developer)  Skills - Machine Learning, Deep Learning, Python, Big data, Hadoop, Spark, Hive, Sqoop, Shell Scripting.  In DNA team, working on an AI and Big Data based platform, which behind the scene uses AI to classify the customer  sensitive data and uses Big Data to sanitize sensitive data in Hadoop distributed file system.  Involved in Capture, Classification, Cleansing, Sanitization, Preseeding and Seeding of large and different type of data  sets (TB+s Range).  Built ensemble Machine learning model for classification of sensitive(NPPI) data on a highly skewed meta-data information  of various banking applications.  Used Natural Language Processing models for identifying NPPI data in RDBMS free form text columns (length more  than 80 Char), logs, files etc.  Performed exploratory Data analysis, preprocessing, Feature Engineering on banking data for fine tuning the model  performance.  Developed automation framework for parallel data sanitization using python and shell script.  Implemented Synthetic Data Generation work-flow that gives an fake output which is similar and 10 times of input data.  Responsible for maintaining Big Data environment by handling files, tables and logs.  Involved in project estimations ,resource management and mentoring team members.  Awarded Best Project DNA Award for on time delivery of product milestone.  Associate  Cognizant Technology Solutions Apr 2013Dec 2017 Schneider Electric (Historian and System Log | Big Data Developer)  Skills - Python, Big data, Hadoop, Hive, Sqoop, Shell Scripting.  In Industrial automation R&D team, worked as an Individual contributor for a Big Data based application.  Developed the application that ingests historical and system logs of devices in industries to Hadoop distributed file system and process using hive to monitor critical device failures.   Responsible to capture and process raw data (System Logs and Historian Data) in TB range coming from different sources like controller and devices from industrial plants using Sqoop and Hive.   Expertise in ingesting and Seeding of huge volume (TBs range) and different formats of data.  Worked on Input validation and Stress test automation to validate the new field device/controller performance.  Appreciated and awarded with Quarterly award Daraya-E-Noor consecutively thrice for delivery excellence and  stakeholder satisfaction. Also, received multiple spot awards for the quality solutions delivered.  EDUCATION  Thanthai Periyar Government Institute of Technology, India Aug2008-June2012 Bachelor of Engineering, Electronic and Communication (CGPA - 8.4) Lions Matriculation and Higher Secondary School, India Jun 2006- Mar 2008 Higher Secondary Education (Percentage - 93.1)  mailto:***************@gmail.com https://www.linkedin.com/in/sabari-arulnagarajan/   PROFESSIONAL SUMMARY  7+ years of experience in Data science/ Machine le  Experience in developing data models applying mach  SKILLS AND COMPETENCIES  PROFESSIONAL EXPERIENCE  EDUCATION  Thanthai Periyar Government Institute of Technolog  ", "tokens": [{"text": "Infosys Ltd", "start": 1520, "end": 1531, "token_start": 233, "token_end": 235, "entityLabel": "company"}, {"text": "Dec2017Present", "start": 1533, "end": 1547, "token_start": 235, "token_end": 235, "entityLabel": "period"}, {"text": "Cognizant Technology Solutions", "start": 2994, "end": 3024, "token_start": 505, "token_end": 507, "entityLabel": "company"}, {"text": "Apr 2013Dec 2017", "start": 3025, "end": 3041, "token_start": 508, "token_end": 510, "entityLabel": "period"}], "relations": [{"child": 508, "head": 505, "relationLabel": "duration"}, {"child": 235, "head": 233, "relationLabel": "duration"}]}, {"document": "SAGARIKA DASH  Lead Engineer IIHCL Technologies  sagarika_d@hcl.com    PROFESSIONAL SUMMARY    Data Science :Having5.8years of IT Experience    Having2+Years of Experience inDataScience and 3+years of ITexperiencewith Manual Testing and Automation usingSelenium(Web Applications).  Hands on experience in both Supervised Algorithmsand Unsupervised algorithms.  Hands on experience in Decision Tree, Random Forest,Gradient Boosting,SVM, K-NN Algorithms.  Hands on Experience in Linear Regression and Logistic Regression.  Good Knowledge Clustering algorithms like inKMeansandDbscan.  Hands on Experience on Dimensionality Reduction techniques:PCA  Worked with Handling Missing values and Correlation.  Good knowledge in Tensor Flow and keras.  Good Knowledge in Optimisers (SGD, Adam,GD)  Good Knowledge in Activation Function(Sigmoid,Tanh,Relu and Leaky Relu).  Good  knowledge on Neural Network(CNN, ANN,VGG 16,LeNet)    EDUCATION    Degree  Specialization  University  B.Tech  EIE  JagannathInstitute of Technology and Management(BPUT)    Technical SkillSet  Machine Learning  Decision Tree, Random Forest, SVM, K-NN Algorithms  Deep Learning  ANN,CNN,Lenet,VGG16  Deployment  GCP  Manual And Automation Testing  Python , Selenium  Database  MySQL  Certifications  Data Science-LearnBay,ISTQB  Tools/Utilities  Jira,Pycharm,JupyterNotebook      PROFESSIONAL EXPERIENCE:    HCL Technologies:    Role  Project  Client  Tenure  Lead Engineer  GoogleHome Automation  Google  Sept2018Present      Responsibilities:  Data Preprocessing, Data Cleaning and EDA(Univariate,Bivariateand Multivariate analysis)  Missing data Handling, Outliers Detection  Drawing Insights and applyingHypothesis Testing.  Applying different algorithms andDetermine accuracy of the Model using Confusion Matrix, AUC ROC Score.  Data Visualization by usingMatplotlib,Seabornand Tableau  Hands-on experience inPytest.      CenduitIndia ServicesPvt.Ltd.    Role  Project  Client  Tenure  SeniorTest Engineer  Clinical Trials  Novartis  March2015Aug2018    Project description:  Certification  Projects  C.I.R.T will provide absolute control of clinical trials. Whether trials are developing a vaccine or a biotech product, or running an early-phase or late-phase study, C.I.R.T will design a smart process and a smart system tailored to the needs of clinical trial.  The unique combination of clinical operations and clinical supplies expertise and our study-specific approach to project management will help you ensure seamless patient enrollment, randomization and drug supply management and patient safety management.    Responsibilities:  To convert manual test cases into Automation Scripts using SeleniumWebDriver, Python.  Complete knowledge on the testing life cycle(STLC) and SDLC  Functional testing, System testing, Integration testing and Regression testing of the product.  Involved in the Debugging of Automation scripts developed using Selenium Web driver.  Analyzing and Publishing Automation Test script Results.    PERSONAL PROJECTS:    Image classification using VGG-16  Mall Customer Segmentation", "tokens": [{"text": "HCL Technologies", "start": 1375, "end": 1391, "token_start": 246, "token_end": 247, "entityLabel": "company"}, {"text": "Sept2018Present", "start": 1473, "end": 1488, "token_start": 266, "token_end": 266, "entityLabel": "period"}, {"text": "CenduitIndia ServicesPvt.Ltd.", "start": 1894, "end": 1923, "token_start": 330, "token_end": 334, "entityLabel": "company"}, {"text": "March2015Aug2018", "start": 2006, "end": 2022, "token_start": 352, "token_end": 352, "entityLabel": "period"}], "relations": [{"child": 266, "head": 246, "relationLabel": "duration"}, {"child": 352, "head": 330, "relationLabel": "duration"}]}, {"document": "                                          Microsoft Word - SaiDheepaS_Resume   SAI DHEEPA S                       Mobile: +91 8754211791 | E-Mail: saidheepa04@gmail.com      https://www.linkedin.com/in/sai-dheepa-s-310a6338/         6 + Years of experience in Amazon as Senior Catalog Associate with experience in Retail Business services in Amazon with strong defect  elimination and process simplification knowledge. Possess strong knowledge on the process being followed in other market places in  Amazon.  Received rewards and recognition for delivering best results and for going extra mile in process.  Knowledge in Python and SQL languages for automating process.       Excellent understanding on the processes being followed in Retail Business services in Amazon across marketplaces.    Interacting and coordinating with vendors/buyers to accurately reflect the Amazon catalog for price, product details and other product- related information.    Experienced in defect elimination and vendor management skills.   . Played the role of SME and handled Canada marketplace.    Works closely with stake holders to understand the repetitive issues and providing solutions for process improvement.    Knowledge in Python and SQL languages and have analysed data.    Established strong vendor relationships to maintain and support the business.    Completed Python certification from XXXX..    Mentored team on data analysis in SIDV project.                                                    ACADEMIC DETAILS     Master of Computer Application  MCA | Thiyagarajar School of management |Madurai| 2011-2014     Bachelor of Science  Computer Science | Sourashtra College | Madurai| 2008-2011.          CAREER DETAILS    Professional Experience:    Working as Senior Catalog Associate in Amazon Development Centre, Chennai from June 2019 to Till Date.  Catalog Associate in Amazon Development Centre, Chennai from October 2014 to June 2019.    Domain: Retail Business Services             Professional Skills        Operating system: Linux, Windows, Unix   Technologies: Python and SQL   Skills: Effective Problem Solver and highly organized in process improvement initiatives.   Database Technologies: MySQL, Oracle.   Build Tools: Ant, Maven.   Mark-up Languages, HTML   Certified Python Developer from XXX   Experienced in writing ETL queries.   Proficient in MS Office          Awards and Achievements:    Recognized and rewarded as First among Equals several times for being consistent in achieving 95% Quality and 1.3X  productivity and 95% SLA.  Rewarded with Extra Mile Award for taking ownership and identifying the NULL binding values and fixing the template.   Rewarded with Extra Mile Award for taking ownership and reducing the dependency and TAT of our team by 0.2 days.  Took responsibility in managing the CA MP single handed and maintained its productivity, Quality and TAT and thus  maintaining the Metrics in Green for all Quarters.   Loaned out to several teams to provide my expertise in defect reduction.         June 2019  Till Date: Amazon Development Center | Senior Catalog Associate     Project           - No PO  Null Mapping  Domain           -     E Commerce    Description:    In Q4, 2018 the open queue of FBA ISS reached 10k due to backlog, I took ownership to analyse the open queue and identify if there were  any unusual spike in any of the CTIs. I found that tickets moved to manual queue from automations for No PO  No PS has increased due  to higher inflow.    Roles and Responsibilities:      Built an ETL query to fetch all the tickets moved to manual queue for this use-case and then developed an automated script to  perform a bulk action by adding FnSku(s) to FBACI and providing resolution on those tickets.     Helped in reducing manual effort on 200 TTs/month, thereby saving 0.5 HC.   Ability to deal with ambiguity  use discretion and judgment to take decisions on critical business tasks based on available   information.   Investigated and identified Root Cause Analysis' for a recurring problem.   Excellent written and oral communication skills.     October 2014  May 2019: Amazon Development Center | Catalog Associate     Project           - TAM Untombstone Dependency reduction  Domain           -     E Commerce    Description:    This SOP change idea which I proposed was in order to reduce our teams (FBA Ops US) dependency with Seller support Team. I have  analysed 50 % of our total inflow tickets which we have flipped to TAM team (dependent team) to make our ASIN active for buy box. I  proposed an SOP change to reduce the dependency with the team with resolutions so that the ticket raised can be resolved in First touch  and thus by reducing our teams overall TAT by 0.2 days.    Roles and Responsibilities:      Identified the ASINs had no binding value contributed (NULL binding attribute) and I analyzed the 3P template format for  3PASINs in seller account and it was evident that, when seller tries to list a new product, there are no mandatory fields set for  updating the binding attributes.    Proposed an idea of fixing the 3P template thus by eliminating the root cause defect of the issue.   Template fix change helped in maximum reduction of binding update cases for CA MP thus reducing the overall inflow of the   tickets by 7%.     Declaration:    I hereby confirm that all the above-mentioned information is up-to-date and true to the best of my knowledge, with the understanding  that this application is being submitted for employment.      Date:             (S.SAI DHEEPA)    ", "tokens": [{"text": "Amazon Development Centre", "start": 1785, "end": 1810, "token_start": 268, "token_end": 270, "entityLabel": "company"}, {"text": "from June 2019 to Till Date", "start": 1820, "end": 1847, "token_start": 273, "token_end": 278, "entityLabel": "period"}, {"text": "Amazon Development Centre", "start": 1871, "end": 1896, "token_start": 284, "token_end": 286, "entityLabel": "company"}, {"text": "from October 2014 to June 2019", "start": 1906, "end": 1936, "token_start": 289, "token_end": 294, "entityLabel": "period"}], "relations": [{"child": 289, "head": 284, "relationLabel": "duration"}, {"child": 273, "head": 268, "relationLabel": "duration"}]}, {"document": "                                               SAMEER KHAN   NEW DELHI|  +919910565736| |LinkedIN|GitHUB   WEBSITE: https://code-sameerkhan.github.io/portfolio/   Profile  Engineer adept in collecting, analyzing and interpreting large datasets and developing new  forecasting models.   Key skills   Programming Language - Python     Machine learning - Regression/ Cluster  analysis, Time Series Analysis    Data Base management tool - MySQL     Visualization Tools -Tableau, Power BI   Education  Bachelor of Technology  Mechanical Engineering   IAMR College of Engineering, UP | First Division    Developed electronically controlled Fuel  Injection system to increase efficiency in  automobiles  [Link to the project]     Professional Experience  TECHSOL ENGINEERING SERVICES PVT LTD   Business Analyst (2019  Present)    Built Linear regression based machine learning model to predict failure in mechanical  components.    Deploying algorithm to forecast monthly spare usage thus reducing the losses and helping  make better business decision.    Been part of the team targeted towards reducing the cost effective loses, successfully modeled  a gain of 13% in profits.    Awarded employee of the month 3 times on separate occasion for team working and problem  solving skills demonstrated multiple times on different level.    Data Science projects:    Artificial Neural Network based Classification   [Project Link]    Model developed to accurately classify data  in the form of images into proper classes.    To employ the usage of Artificial Neural  Network based model for prediction    Deploying TensorFlows Keras dense  Network and achieved an accuracy of  99.26%   Revenue generation forecasting [Project Link]    Revenue generation prediction when a  customer visit a website page    Data visualization using Seaborn/  Matplotlib libraries of revenue generation.    Deployed algorithm such as KNN to predict,  reached an accuracy of 85.915%.     Segregation by Facial Recognition [Project Link]    Created a tool that classifies different  sportsperson based on their image.    Determined visuals features using HAAR  enabled cascade classifier.    Performed wavelet transformation,  deployed GridSearchCV to determine best  model with best parameters.    Using Logistic regression, achieved 80%  accuracy in classification.     https://www.linkedin.com/in/sameer-khan-engg15/ https://github.com/Code-SameerKhan/ https://code-sameerkhan.github.io/portfolio/ https://www.youtube.com/watch?v=Q0Bm8RnBtjU&ab_channel=SameerKhan https://github.com/Code-SameerKhan/Handwritten-digits-classification https://github.com/Code-SameerKhan/Revenue-generation-prediction https://github.com/Code-SameerKhan/Image-classifier  ", "tokens": [{"text": "TECHSOL ENGINEERING SERVICES PVT LTD", "start": 748, "end": 784, "token_start": 111, "token_end": 115, "entityLabel": "company"}, {"text": "2019  Present", "start": 805, "end": 818, "token_start": 120, "token_end": 122, "entityLabel": "period"}], "relations": [{"child": 120, "head": 111, "relationLabel": "duration"}]}, {"document": "                                             MALAVIYA NATIONAL INSTITUTE OF TECHNOLOGY JAIPUR   Name: Sandeep isKumar Gupta               Degree:             Phd* M.Tech,   B.E.        Branch: Computer Science & Engg   Date of Birth:   April 29, 1986        E-Mail:sandeepmbm@gmail.com              Contact No:     +91-7230960333        Communication Address:  F-1101, nri Residency, greater noida, India-201310.   OBJECTIVE    To make innovative efforts to sharpen the intellect and inculcate innovative skills in order to improve constantly research   methodology, quality of innovation.   EDUCATION    YEAR QUALIFICATION UNIVERSITY / BOARD % / CGPA   2016-2021 Ph.D.- AI  National Institute of Technology, Jaipur Submitted   2010-2012 M.Tech (CSE)-  AI National Institute of Technology, Jaipur 6.89/10   2003-2007      B. Tech. (IT) National Institute of Technology, Jaipur 63.43   E2002 XII (PCM) Rajasthan Board of Secondary Education   Ajmer   65.54   2000 X (Math) Rajasthan Board of Secondary Education   Ajmer   80.33   EXPERIENCE   Organization Roles/Responsibility Duration   Reason of   leaving    Coforge Ltd.  Tech Analyst (AI-Architect) Oct 2020- onwards Better opportunity   Jekson Vision, Ahmadabad Tech Lead(Data Science)- Design, HLD,   Development     From June 2018    April 2020.    Project Over   Redpine Signals Inc.,   Hyderabad   Research Scientist-(Data Science)- Design,   HLD, Development     Sep 2017 Jan 2018 Better Opportunity   MNIT Jaipur  Research Project Staff (Machine Learning-  - R&D)-Object Detection and Image   Attribute      Jan 2017-July 2017 Project over    JECRC University Jaipur Data scientist- AP (Machine Learning-   R&D)- Design, code Development,   Principal investigator, Agile     Oct-2015-Sep 2016 For better   opportunity   The ICFAI University Jaipur Faculty (Machine Learning)- Design and   Mentoring on project of AI, Principal   investigator     July 2014 Jan 2015 Project over   BMIT Jaipur Data scientist/ Asst. Professor-(Image   Analytics -R&D)-PI of sponsored indutrial   projects    Jan,2013  June,2014 Challenging Role    Sharda University, Greater   Noida    Data scientist/Asst. Professor-(Image   Analytics -R&D) PI(Projects),   Development of sponsored kiprojects     June,2012-Jan,2013 Better location for   managing family   NIT Jaipur Research Scholar -TA-(Image Analytics-  R&D)- Development of industrial projects     July2010-June 2012 Completion of   project/tenure   JaganNath University Jaipur Research AP-( Image Analytics-R&D),   Development of industrial projects         July,2009-June,2010 For higher   education   NIT, Jaipur Faculty - (Image Analytics)- Development   of industrial projects     Aug.,2008-Dec,2008 For  better position   IIIM, Jaipur Lecturer-( Image Analytics)- Development   of industrial projects     Dec,2007-Aug.,2008 Better Project and   learning new tech      mailto:sandeepmbm@gmail.com   Note: During the above position as Asst. professor, I was involved in R&D division of premium universities for   development of funded project s of industry rather than teaching and I have lead the projects in area of machine learning,   AI and computer vision.   TOTAL EXPERIENCE:  11+ Years in Research and Development Industry in area of Machine Learning, Artificial   Intelligence, Data Science, computer vision, Image Processing, video analytics, predictive analytics, statistical   modelling, NLP, RPA,designing new applications from scratch and features engineering for complex problem, product   Architects, recruitment talent and attract as centre of excellence, mentoring, technology selection, training, build teams   of AI, POC and lead of overall development, research, designing and innovation of new state of art applications and   features engineering, product road map, Program Scheduling of different parallel product line. Knowledge updating for   technology and feature advancement of state of art product, advising to business leadership for new area, product in AI.   (Architecture, design Responsibility of product and self contribution of coding for model building and overall product   for business enhancement.)   Notice Period:  30 days   CERTIFICATIONS COURSES   (1) Management Development Programme from Indian Institute of Management, Kozhikode.   (2) Professional Development Programme from Indian Institute of Technology, GandhiNagar.   (3) Qualified GATE 2016, GATE 2010.   (4) Certified IBM DB2 Database Associate Certification.   RESEARCH PUBLICATIONS / WHITE PAPER   Conference/Journal/ No of papers Remarks   International Conference Proceedings 22 List is attached   National Conference Proceedings 2 List is attached   International Journal 5 List is attached   Number of Courses Participated National 5 List is attached   Number of Courses participated International 5 List is attached   Paper Presented in International Conference (IEEE, ACM, Springer) 20 List is attached   Excluding Above one Research Papers are in IAPR approved international conference and one papers are communicated   in SCI Indexed International Journal. One patent is going to file.   RESEARCH AREA   (1) Artificial Intelligence (2) Machine Learning, Deep learning (3) Computer Vision (4) Image Processing, Medical   Imaging (5) Algorithm-Image and Signal Processing (6) Data Science-NLP, RPA (7) Speech Technology (8) Industrial   Cameras and Lens (9) Analytics, Math, Statistics (10) GPU-Cuda Computing   TECHNICAL SKILLS   (1) Tools: Python, pytorch, MATLAB, C++, Deep Learning-Tensor Flow, theano, Keras libraray, Scikit-learn, panda,   numpy, MATLAB: 7 years, Python: 5 years, OpenCV: 5 years, C/C++: 5 years. AWS, S3, lambda, EC2,  EMR, Azure,   NLTK, HMM, Kaldi, Festival, GPU-Cuda, QT, QML, Flask, Restful API, Django, PostgreSQL, MongoDB.   (2) Object detection, Facial Recognition, anomaly detection, predictive analytics, forecasting, biometric recognition,   information extraction from document, document classification, fraud analytics.   (3) Applied Data Science for classification and regression on image, audio, video, numerical, categorical, financial data   and time series data, biomedical datasets as ultrasound and CT data, Text, sensor data, biometrics data.   (4) Machine learning and computer vision projects developed on GPU, and deployed on ARM architecture machine   (IOT device), cloud based AI solution, feature engineering, prototype, AI model design for complex problem and   integrate innovation, state of art solutions, High Level Design, Low Level Design for solutions of product design,   benchmarking of product, Client-server Architecture of AI product, SVM, boosting, K-means, KNN, HMM, GMM.   NLP, NER. Strong knowledge and experience of machine learning techniques: deep-learning: CNN, All state   of art  model: VGG, VGG-19, Inception, RESNET, R-CNN variant, Yolo, Mask-RCNN, CapsNET, LSTM,     GAN, BIRT and own custom model for classification, parallel architecture of multi attribute identification,   regression problem and Naive-Bays, linear and logistic regression, decision trees, probability networks,   Bayesian, association rules, clustering, Association rule mining and neural networks: back-propagation,   adaboost.   (5) AI Cloud Services, AWS, Azure, CI/CD, K8s, git-hub, SVN, Operating System: Linux, Unix, Windows.   (6) Statistical analysis of data: z-test, t-test, chi-square test, ANOVA, ANCOVA, Regression.   (7) Patent/publication level work: Some publication are in pipeline. Lead of 24 member across different team on   multiple projects and supervisor whole life cycle of product with individual contributor.   (8)  statistical analyses, identify bottlenecks, and provide solutions for critical business problems. Sop for data engineering to model   designing, performance evaluation of proposed methodologies, deployment. Short term and long term milestone setting with team   and execute using project plan. Drive the team for common set goals.   (9) Product Benchmarking, Article, white papers and patent design. Strong and influential Leader, Carries spirit of an   Entrepreneur in technology and technical management. Peer review, risk management and mitigation with engineering   management.   (10) Project management Under agile,      Agile tools: Jira,     Documentation Tools: Confluence.   PROJECTS and CONSULTANCY   S   n   Title of Project Year Tool/ Language   Used   Duratio  n   Publicat  ion   Client Status    Information retrieval and automation   decision from email data for insurance   domain   2020-  Python, NLTK,   pandas, BIRT,    3   Months   NA Confidenti  al   In progress    Email classification and categorization   for matching of sla breach    2020- Python, NLTK,   pandas, BIRT   3 M NA Confidenti  al   First phase   is   completed   1 Multi Object and Multi Attribute   recognition on face   2019 Python,   tensorflow,   Keras, Opencv,   scikit learn   1Y In   progress   Phd   Project   In progress   2 Customer Age, gender and identity   using face.    2017 Python,C++,open  cv, Python, keras,   tensorflow   4 Month NA In house   Product   Completed   3 Customer satisfaction evolution using   face gestures.   2017 Python, OpenCV,   Python   3 Month IEEE Redpine POC   4 Facial Gender Recognition 2017 MATLAB 8 Month Springer MNIT Delivered   5 Moving Object Detection 2016 MATLAB 3 Month IEEE MNIT Delivered  6 Facial Expression Recognition (from   scratch in 2010 and optimization in   2016 )   2010,  2016   MATLAB 6 Month IEEE MNIT Delivered   7 Auto Blister Inspection System on  pharmaceutical packaging line   2018- 19   Python, C++,  OpenCV,   tensorflow, Keras   1Y In- Process   Jekon  Vision   Completed   8 Auto Tablet Inspection System on   pharmaceutical packaging line   2018 Python, C++,   OpenCV,   tensorflow, Keras   6M In-  Process   Jekon   Vision   Completed   9 Print quality checking of 2D code as   par GS1 compliance for packaging   Line.   2019 C++, OpenCV,   Halcon   3M --- Jekon   Vision   Completed   10 Data matrix decoding and Print quality  check according to ISO-15415, ISO-  16022 GS1 compliance.    2019  C++, OpenCV,  tensorflow, Keras   6M In- Process   Jekon  Vision   In-Process   11 Bar code Pharma code Reader      2018 Hacon Library,   C, C++, OpenCV   6M NA Jekon   Vision   Completed   12 OCR Reader for pharma packaging   line and validation   2019 Python, OpenCV,    TOCR   6M In-  Process   Jekon   Vision   Completed     Keras,   tensorflow,    13 Bulk Aggregation on pharma   packaging line   2018 Hacon Library,   C, C++, OpenCV   6M NA Jekon   Vision   Completed   14 Industrial Motor anomaly detection 2017 C++,OpenCV,   Python   3 Month - Redpine Delivered   15 Heart anomaly  detection 2017 C++,OpenCV,   Python   3Month - Redpine Delivered   16 Child Tracking System 2017 C++, Opencv,   Python   3Month - Redpine Delivered   17 Image Fusion 2016 MATLAB 3 Month IEEE MLR Lab Delivered  18 Urdu Corpus Development and   Handwritten Text Recognition   2016 MATLAB 3 Month  DST   Rajasthan   Delivered   19 Person Identification using Footprint 2015 MATLAB 3 Month IEEE AA-MNIT Delivered  20 Facial Recognition 2015 MATLAB 4 Month IEEE MLR Delivered  21 Detection of select forwarding attack   on WSN   2015 MATLAB    NS-2    3Month IEEE freelancer Delivered   22 Detection of Sink hole attack on WSN 2015 NS-2 3M IEEE freelancer Delivered  23 Medical Image Compression  2014 MATLAB 6 Month IEEE MLR Delivered  24 Image Watermarking 2014 MATLAB 6 Month IEEE MLR  Delivered  25 Association Rule Mining 2014 MATLAB 6 M  Freelance   project   Delivered   26 Facial Expression Recognition -  Version 2.0   2013 MATLAB 5 Month IEEE MNIT Delivered   27 IRIS Biometric Recognition 2013 MATLAB 7 Month Springer UIDAI Delivered  28 Ear Biometrics Recognition System 2012 MATLAB 10M IEEE DST Delivered  29 Fast and Optimized K-Means   Clustering   2011 MATLAB 4 Month  Freelance  project   Delivered   30 Image Steganography 2009 MATLAB 9 Month  MNIT Delivered  31 Auto Performance Measurement of    Network   2008 C++, Python NS-  3   11M  MNIT Delivered   32 Plaque Identification and  Plaque   Quantification in Ultrasound images   (POC)   2016 MATLAB 6M  MNIT Delivered   33 BLAS & Machine learning Library for   Data Science on ARM architecture.   2017 C 4M  Redpine Delivered      Management Skills   1. Project Management of multiple projects simultaneously and timely deliver with quality.   2. Project Lead, Project Planning, time management, process management, Risk management and risk mitigation,   knowledge, team skill- asset building.   3. Future Business Aspects and lead in business technology competitor.   4. Project Management, Resource and people management, product life cycle using Agile.   EXPERIENCE OF ALLIED RESEARCH RESPONSIBILITY   (1) Senior Member of IEEE computer society.   (2) Member in Conference Program Committee of 13th IEEE International Conference SITIS 2017.   (3) Session Chair, Reviewer of IEEE, ACN and Springer international conferences.   (4) Machine Learning Expert Speaker in Conferences and Workshops at national and international level.   NATIONAL & INTERNATIONAL PARTICIPATION   (1) Participated in International Courses on Machine Learning in Biometrics conducted by IIIT-Delhi.     (2) Presented 20 research paper in International Conferences and 2 papers in national conference.   (3) Participated in MHRD sponsored Short Term Programme: Pattern Recognition, in MNIT Jaipur.   (4) Participated in National Seminar on Advanced Data Computing: Emerging Trends and Issues.   (5) Participated in national workshop on Concepts & Techniques of Evaluation & Measurement.                                 Research Quality   Citation Count of published white paper internationally as per Google Scholar = 220, H Factor=7,  I-10 factor=6.   Link: https://scholar.google.co.in/citations?user=O3DmcsgAAAAJ&hl=en   HOBBIES/INTEREST    Writing Script, Technology Articles, Poems & Reading Novels, Articles.   References:    1. Prof. (Dr.) M.S. Gour, Director, IIT Jammu  email id: gaurms@mnit.ac.in    2. Dr. Neeta Nain, Professor, CSE, NIT, Jaipur  email id: nnain.cse@mnit.ac.in     3. Prasad R.V. CTO, Jekson Vision- email id: Prasadr_v@yahoo.com    4. Murali Nair, Delivery Head, Jekson Vision     5. Dr. Anil K. Jain -           LIST OF PUBLICATION   International Conference proceeding Publication   1. Gupta, Sandeep K., ShubhLakshmi Agrwal, Yogesh K. Meena, and Neeta Nain. \"A hybrid method of feature  extraction for facial expression recognition.\" In Signal-Image Technology and Internet-Based Systems (SITIS), 2011   Seventh International Conference on, pp. 422-425. IEEE, 2011. ISBN: 978-0-7695-4635-3 Nov.  2. Soni, Karuna, Sandeep K. Gupta, Umesh Kumar, and Shubh L. Agrwal. \"A new Gabor wavelet transform feature   extraction technique for ear biometric recognition.\" In Power India International Conference (PIICON), 2014 6th   IEEE, pp. 1-3. IEEE, 2014. ISBN: 9781479960439  3. Dosodia, Priya, Amarjeet Poonia, Sandeep K. Gupta, and Shubh Lakshmi Agrwal. \"New Gabor-DCT feature   extraction technique for facial expression recognition.\" In Communication Systems and Network Technologies  (CSNT), 2015 Fifth International Conference on, pp. 546-549. IEEE, 2015. ISBN: 9781479917983  april    4. Kumari, Neelu, Sandeep Kumar Gupta, Rajni Choudhary, and Shubh Lakshmi Agrwal. \"New performance analysis  of AODV, DSDV and OLSR routing protocol for MANET.\" In Computing for Sustainable Global Development   (INDIACom), 2016 3rd International Conference on, pp. 33-35. IEEE, 2016. ISSN: 0973-7529     ISBN: 978-93-80544-  19-9   5. ShubhLakshmi Agrwal, Anita Yadav, Umesh Kumar, Sandeep Kumar Gupta, Improved Invisible Watermarking   Technique Using IWT-DCT, 5th International Conference on Reliability, Infocom Technologies and   Optimization(ICRITO), IEEE, 2016. pp. 283-285. ISBN 9781509014897 1509014896 9781509014903 150901490X   6. Sandeep et al., Analysis of Detection Algorithm of Sink Hole Attack & QoS on AODV for MANET, 2nd IEEE  International Conference NGCT'2016, IEEE, 2016. pp. 839-842.  ISBN: 9781509032563   7. Sandeep et al., Improved Image Compression Technique Using IWT-DCT Transformation, 2nd IEEE International  Conference Next Generation Computing Technology, IEEE, 2016. pp. 683-686. ISBN: 9781509032563   8. Sandeep et al., Performance Analysis of Detection Technique for Select Forwarding Attack on WSN, International  Conference on Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC), IEEE, 2016.    9. Sandeep et al., Image Fusion Technique Using IWT-DCT Transformation, International Conference On  Quality, Productivity, Reliability, Optimization and Modeling, IEEE, 2017.    10. Sandeep et al., 4x4 Block DWT-DCT Image Steganography for Gray Scale and Colored Image, International  Conference On Quality, Productivity, Reliability, Optimization and Modeling, IEEE, 2017.    11. Sandeep et al.,Efficient Feature Extraction Technique for Face Recognition, International Conference On  Quality, Productivity, Reliability, Optimization and Modeling, IEEE, 2017.    12. Sandeep et al., Performance Analysis of Frequency Domain based Feature Extraction  Techniques for Facial Expression Recognition, International Conference on CONFLUENCE, IEEE, 2017.    https://scholar.google.co.in/citations?user=O3DmcsgAAAAJ&hl=en mailto:nnain.cse@mnit.ac.in https://www.tib.eu/en/search/?tx_tibsearch_search%5Bquery%5D=isbn%3A%289781479917983%29   13. Sandeep et al., Hybrid Object Detection using Improved Three Frame Differencing and Background Subtraction,  International Conference on CONFLUENCE, IEEE, 2017.   14. Sandeep et al., Performance of Image Fusion Technique Using 4x4 Block Wavelet Cosine Transformation,  International Conference on CONFLUENCE, IEEE, 2017.    15. Sandeep et al., Person Identification on the Basis of Footprint Geometry, International Conference on Signal Image  Technology and Internet Based Systems, Italy, 2017.    16. Sandeep et al., Gabor Filter meanPCA Feature Extraction for Gender Recognition, The Second International  Conference on Computer Vision & Image Processing (CVIP-2017) , Springer,  2017.    17. Sandeep et al.,Optimized Dynamic Background Subtraction Technique for Moving Object Detection and Tracking,  2nd International Conference on Telecommunication and Networks TEL-NET 2017, IEEE, 2017.    18. Sandeep et al., Performance Analysis of Gabor 2D PCA Feature Extraction for Gender Identification using Face ,  2nd International Conference on Telecommunication and Networks TEL-NET 2017, IEEE, 2017.    19. Sandeep et al., Gabor-Max-DCT Feature Extraction Techniques for Facial Gesture Recognition, 2nd International  Conference on Recent Advancements in Computer, Communication and Computational Sciences (RACCCS- 2017),   Springer, 2017.    20. Sandeep et al., Facial Expression Recognition Using Gabor-Mean-DWT Feature Extraction Technique, 5th  International Conference on Signal Processing and Integrated Networks, IEEE, 2018.   21. Sharma, Archana Kumari, ShubhLakshmi Agrwal, Sandeep K. Gupta, and Umesh Kumar. \"Gesture Recognition Using  Gabor-MeanLBP-PCA Feature Extraction.\" In 2018 International Conference on Advances in Computing,  Communication Control and Networking (ICACCCN), pp. 986-990. IEEE, 2018.   22. Sandeep et al., Gender Recognition for Juvenile Unconstrained Faces Using Gabor-MeanPool-DCT Feature Model  and SVM-Kernel Optimization In Signal-Image Technology and Internet-Based Systems (SITIS), 2019, 15th   International Conference on , , IEEE, Itely, 2019.  Excluding above two papers is communicated in IAPR approved international conference and two paper in SCI Index   Journal, IEEE transaction.   National Conference Presentation   1. Sandeep et al., Performance analysis of feature extraction technique for face recognition, in national conference on  Emerging Technologies in Computer Engineering (NCETCE), 2014.   2. Sandeep et al., Performance analysis of feature extraction technique for iris recognition, in national conference on  Emerging Technologies in Computer Engineering (NCETCE), 2015.   International Journal Publication   Sandeep, Neet Nain, Single attribute and multi attribute facial gender and age estimation, international journal on   multi media tools and applications, springer, 2020.    Sandeep, Neeta Nain, multi attribute facial gender and age estimation for unconstraint data and juvenile faces,   international journal on multi media tools and applications, springer, 2020.  (Communicated)      1. Sandeep et al. Optimized Average Gabor-DWT-DCT Feature Extraction for Gender Recognition, Journal on  multimedia tools and application, Springier, 2019. (Communicated-SCI Index Journal)    2. Agrwal, Shubh Lakshmi, Ayushi Jhanwar, Kuldeep Goswami, Sandeep K. Gupta, and Vibhor Kant. \"Facial gender  recognition using Gabor-DCT feature extraction.\" Journal of Statistics and Management Systems 22, no. 4 (2019):   719-728.   3. Kumar, Umesh, Neha Gopaliya, Uma Sharma, and Sandeep Gupta. \"Discrete Transform Based Image Fusion: A  Review.\" International Journal of Multimedia Data Engineering and Management (IJMDEM) 8, no. 2 (2017): 43-49.   4. Neha, Pratistha Mathur and Sandeep Kumar Gupta. Performance Analysis of Feature Extraction Techniques for Facial  Expression Recognition. International Journal of Computer Applications 166(1):1-3, May 2017.  (ISSN: 0975-8887)   5. Bhanu Vadhwani, Vineet Khanna, Sandeep Kumar Gupta and Shubhlakshmi Agarwal. Performance of Gabor mean  Feature Extraction Techniques for Ear Biometrics Recognition System. International Journal of Computer   Applications 168(12):1-2, June 2017. (ISSN: 0975-8887).   6. Tambi, Preksha, Savita Shiwani, and Sandeep K. Gupta. \"Performance Analysis Of K-Means Clustering And K-Means  Clustering With Improved Initial Means.\" In  Journal of Engineering Research and Technology, vol. 2, no. 4, pp.128-  131, ESRSA Publications, 2013.( ISSN:2278-0181)      Presentation/Invited Speaker in International Conference:   1. Springer International Conference on Recent Cognizance in Wireless Communication & Image Processing 2015.  2. IEEE International Conference PIICON 2015.   3. IEEE International Conference CSNT 2015.   4. IEEE International Conference ICRITO 2016.   5. IEEE International Conference NGCT 2016.  6. IEEE International Conference ICQPROM 2017.  7. IEEE 7th International Conference Confluence 2017.  8. IEEE 7th International Conference ISBA  2017.     9. Springer International Conference on Recent Advancements in Computer, Communication and Computational  Sciences, 2017.   10. IEEE International Conference SPIN 2017.  11. Springer CVIP-2017, CVIP- 2019.  12. IEEE SITIS 2019.      Sandeep Kumar Gupta                                                                                                 Place:  Jaipur    ", "tokens": [{"text": "Coforge Ltd.", "start": 1110, "end": 1122, "token_start": 198, "token_end": 199, "entityLabel": "company"}, {"text": "Oct 2020- onwards", "start": 1152, "end": 1169, "token_start": 208, "token_end": 210, "entityLabel": "period"}], "relations": [{"child": 208, "head": 198, "relationLabel": "duration"}]}, {"document": "                                2021-03-10 H.Lowe-Content.Strategist-Resume-2   HECTOR LOWE hector.lowe@gmail.com | North Hollywood, CA | (310) 721-7510  Project manager, creative content developer, writer, and strategist in a broad range of projects delivering  user experiences with a strong consumer focus. Over 10 years experience with recent basis in  instructional design; key expertise in creating instructional material; project management; team  coordination; interactive training media; writing for video and film; delivering voiceover; graphic design  and illustration; game design; and technical writing. Experience in software training and development;  teaching and mentoring; entertainment industry; museum exhibition; and both legal and medical fields.  Certified language instructor; received formal recognition and awards in writing.  TECHNICAL AND PROFESSIONAL SKILLS  Writing and Editorial Skills:  - Content Writer: Writes clearly according to both AP and Chicago style guidelines. Technical  Writing; Legal Writing; Medical Writing; Academic Writing; Creative Writing; Proofing & Editing  Document Production:  - Design: Instructional Design; Graphic Design; Visual Development; Photo Editing; Sketching;  Vector Tracing; Web design; User Interface Optimization; Diagrams; Print Design;  - Written: Playbooks; Manuals; One-pagers; Web-based Tutorials; Typography; Style Guides;  Flowcharts; Newsletters; Treatments; Pitch Packets; Page Layouts; Brochures & Flyers  - Video: Video Production; Interactive Video; Tutorials; Video Editing; Assembly; Visual Effects  - Audio: Voiceover Artist; Audio Editing; Balancing; Audio Effects  Software Proficiency:  - Adobe Creative Suite:  - Video: Premiere; Animate; Camtasia; Snagit; ToonBoom; Articulate (Rise)  - Design: Photoshop; In Design; Illustrator; Dreamweaver; Unity; Lucidchart, Visio  - Audio: Audition; Audacity; Audiomulch  - Project Management: Asana; Leankit  - Additional: HTML & CSS; Google Suite; Microsoft Office; Wiki Pages; Social Media  Managerial:  - Project Management: GAP Analysis; ADDIE Development; Agile Development; Leading Scrum &  Sprint Meetings; Workflow Optimization;  - Team Management: Onboarding & Training; Interdepartmental Coordination; Teaching &  Mentoring; Mediation & Conflict Resolution; Public Presentation; Client Advocacy    EXPERIENCE  CONTENT DEVELOPER   HCL @FACEBOOK  2/2020 - Present  Project manager, leading the end-to-end production and delivery of many simultaneous,  high-impact projects. Led daily scrums and used agile development to create a series of  multimedia training packets for Facebook employees in global order promising, Oracle Fusion,  order to cash processes, and more.   Created interactive video and written training materials for onboarding new Facebook employees and instructing existing employees in new systems and software. These included the  scripting, voiceover, production and editing of video tutorials and interactive instructionals.   Used LucidChart, Visio and the Adobe suite to develop abstract company process flows and itemized workflows into comprehensive visualizations, creating road-maps for effective  personnel instruction in Trade Logistics Fulfillment, Post-Sales Ops, and more.   Oversaw departmental output, monitoring for progress and ensuring the highest quality of products through tireless review and comprehensive feedback. Acted as the teams safeguard  against failure. Using written, design and problem-solving skills, regularly addressed short-fuse  issues and solved problems in high-pressure, short notice circumstances with total success.   Increased productivity by working cross-functionally with subject-matter-experts and clients to satisfy their needs exactly.   Redesigned the UI and iconography for the Facebook Analytics team server to increase output through clear sign-posting files, folders and departments.   Spearheaded the departmental rebranding of the Facebook AR/VR content team into the Facebook Reality Labs Training & Development team, including creating new team branding, and  redesigning and standardizing templates of written documents, logos, and video media into  consistent formats.   Organized and conducted regular presentations, meetings and lunch-and-learn events to publicly inform Facebook employees of new procedures, corporate structure changes, systems  and processes.   Personally drove the redesign of departmental workflow, creating project process templates in Asana and Leankit to maximize team efficiency. Strategized and implemented improvements on  team output, revolutionizing review procedures and ultimately developed project management  guidelines that radically increased productivity.   Acted as team mediator, resolving any unanticipated conflicts with care and alacrity, to every employees satisfaction. Led seminars to improve interdepartmental communication.  INDEPENDENT CONSULTANT  8/2010 - 2/2020  Independent consultant for business and creatives, creating graphic, written and audio-visual  material for various projects.   Provided key insight into mechanical errors and generated effected visual redesign of components in board games, based on effective research and consumer psychology.   Worked closely with producers and creators to generate visual concepts for effective communication of concepts to investors.   Developed and delivered a website for the California Association of Realtors within two weeks.     Wrote brief, persuasive, on-task text for the user manuals and consumer interface.  Created and improved upon animation assets for use in marketing materials.  Consumed content provided by the consumer and other agencies or from own research, and  extract relevant insights to brief and inspire my clients.   Generated and edited audio and video content for informational videos and instructional tutorials.   Worked closely with Product Management, Engineering, Marketing, Customer Support and User Experience to design visuals in advertising and marketing for video games and toys.   Designed and created visual references and character models for concept packages in animated television and childrens literature.  DEMAND WRITER  JAVAHERI & YAHOUDAI  4/2017 - 10/2018  Lead department in developing written materials to maximize efficiency and profit.  Developed and implemented a cross-channel content strategy to translate business goals into  meaningful deliverables.   Improved inter-departmental efficiency and output through a series of workplace and labor analyses and planning and implementing corrective measures.   Measured the impact of work using analytics.  Conducted extensive client research in order to improve output as well as new client uptake and  outreach.   Collaborated with all departments to guide the strategic direction of organizational efforts and established the value of each client as a business asset.  OPERATIONS ASSISTANT  TOURWEST (UK)  12/2009 - 7/2010  Wrote and coded clear, friendly online content  (exhibition overviews, news blog) that supported  users to understand how the company works and how to partner with them.   Measured the impact of work using analytics and updated documents to improve customer engagement.   Consumed content provided by the client, client services, and other companies or from own research, and extract relevant insights to brief and inspire management.   Provided feedback to internal and external contributors to guarantee that content aligns to strategy and quality standards.   Collaborate with teams across the company to make recommendations and improve content and content consumption based on analytics, testing, and performance.   Created digital and physical content to improve the aesthetics of the final product.  Performed field research to exhibition locations in order to get feedback on future  improvements to the companys end product and optimize displays to given location and culture.  DESIGN ASSISTANT  JACK MORTON WORLDWIDE (UK)  07/2009 - 10/2009  Researched, created and optimized page layouts with InDesign for proof-of-concept materials to  distribute to clients.   Brought a user-focused perspective to content creation.  Conceptualized bid for the Commonwealth Games Handover Ceremony 2010.    PUBLISHING ASSISTANT  DAVID WEST CHILDRENS BOOKS (UK)  06/2007 - 10/2007  Assisted in publishing ~10 illustrated children's books, sold worldwide.  Constructed and organized optimal page layouts for best legibility, storytelling and visual appeal.  Graded register of vocabulary to target audience, for ideal clarity. Typesetting and proofreading.   Drew preliminary sketches for illustrated books with digital equipment. Edited, colored and refined existing illustrations.   Conducted market research and background research on the historical subjects of the books to ensure both accuracy and consumer appeal.  EDUCATION  Bachelor of Arts: University of East Anglia  Certificate of English Language Teaching to Adults: University of Cambridge  SCREENWRITING AWARDS  Barry Josephson Fellowship 2019 Winner  American Film Institute (DWW) 2019 Winner  Austin Film Festival 2018 Nominee  A-List 2017 Winner   ", "tokens": [{"text": "FACEBOOK", "start": 2375, "end": 2383, "token_start": 445, "token_end": 445, "entityLabel": "company"}, {"text": "2/2020 - Present", "start": 2385, "end": 2401, "token_start": 447, "token_end": 449, "entityLabel": "period"}, {"text": "JAVAHERI & YAHOUDAI", "start": 6187, "end": 6206, "token_start": 1076, "token_end": 1078, "entityLabel": "company"}, {"text": "4/2017 - 10/2018", "start": 6208, "end": 6224, "token_start": 1080, "token_end": 1082, "entityLabel": "period"}, {"text": "JACK MORTON WORLDWIDE", "start": 7953, "end": 7974, "token_start": 1376, "token_end": 1378, "entityLabel": "company"}, {"text": "07/2009 - 10/2009", "start": 7981, "end": 7998, "token_start": 1383, "token_end": 1385, "entityLabel": "period"}], "relations": [{"child": 447, "head": 445, "relationLabel": "duration"}, {"child": 1080, "head": 1076, "relationLabel": "duration"}, {"child": 1383, "head": 1376, "relationLabel": "duration"}]}, {"document": "                                                   HEENA UGALE https://github.com/heena-jvs  200 2nd Ave W, Seattle WA 98119  6072165298  hru3@uw.edu  EDUCATION  Cornell University August 2018 - May 2019 Master in Chemical Engineering Major CGPA - 3.9/4, Overall CGPA - 3.7/4 Focus on Computational Informatics  Indian Institute of Technology, Bombay July 2012 - June 2017 Integrated Bachelor & Master in Chemistry Major CGPA - 3.6/4, Overall CGPA - 3.4/4  WORK EXPERIENCE  Data Scientist/Research Engineer 2 - University of Washington(UW) Nov 2020 - current  Technical Lead, on a healthcare cross functional, client-facing project by Novo Nordisk responsi- ble for building robust machine learning model to predict insulin related outcomes in diabetics  Derived trends, conclusions, and actionable recommendations for clients using regression and neural networks to interpret results from real world and clinical trial big data, behind clinically- relevant responses  Developed, validated ,and executed algorithms and predictive time series models to investigate prob- lems, detect patterns, and recommend solutions  Research Scientist - Population Informatics Lab, Texas A&M University Aug - Oct 2020  Cleaning real world population big data, to measure impact my models had on the decisions patients in choosing their healthcare provider  Research Scientist - Systems Eng. Dept. Cornell University Sept 2019 - July 2020  Regression analysis of survey real world big data to understand smart energy choices in EVs  Measuring impact of my models on EV choice decisions made by people dimension reduction, to investigate underlying correlations between key demographic factors and energy choices  Data Scientist - LMU, Munich, Germany Jan 2018 - July 2018  Analyzed biochemical to explore, examine, and interpret large volumes of data relating to cancer  Research Associate - LMU-BASF, Munich, Germany July 2017 - Jan 2018  Performed analysis of structured and unstructured data to solve moderately complex business prob- lems utilizing advanced statistical techniques and mathematical analyses for large scale production  Used data visualization to effectively communicate analytical results and communicate business de- cision that increased 15% profits amounting to 100eu/lit  Data Analyst Internship - Texas A & M University May 2015 - July 2015  Designed most efficient mechanistic data model for axially chiral compounds, crucial for biochemical reactions inside human beings by analyzing clinical dataset  Data Analyst Internship - Indian Institute of Technology, Bombay May 2013 - May 2014  Designed catalyst using correlations between geometrical parameters and yield for anti-coagulant drug warfarin synthesis by analysing reaction data  SOFTWARE/PROGRAMMING EXPERIENCE  Machine Learning packages: Keras, Tensorflow, Pytorch, Numpy, Scipy, Matplotlib, scikit-learn Pandas Machine Learning Models: Regression, Random Forest, Neural Network, K means clustering, Bayesian Languages : Excel, Powerpoint, Word, R (dplyr, ggplot2), SQL, python, SAS, tableau, latex, VBA Applications : AWS(EC2, S3, CodeCommit), MS office, Google Cloud Platform, Jupyter Notebook Version control : CodeCommit (AWS), Github   ", "tokens": [{"text": "University of Washington", "start": 511, "end": 535, "token_start": 82, "token_end": 85, "entityLabel": "company"}, {"text": "Nov 2020 - current", "start": 540, "end": 558, "token_start": 86, "token_end": 89, "entityLabel": "period"}, {"text": "Texas A&M University", "start": 1167, "end": 1187, "token_start": 186, "token_end": 188, "entityLabel": "company"}, {"text": "Aug - Oct 2020", "start": 1188, "end": 1202, "token_start": 189, "token_end": 192, "entityLabel": "period"}, {"text": "Cornell University", "start": 1382, "end": 1400, "token_start": 225, "token_end": 226, "entityLabel": "company"}, {"text": "Sept 2019 - July 2020", "start": 1401, "end": 1422, "token_start": 227, "token_end": 231, "entityLabel": "period"}, {"text": "LMU", "start": 1714, "end": 1717, "token_start": 279, "token_end": 279, "entityLabel": "company"}, {"text": "Jan 2018 - July 2018", "start": 1735, "end": 1755, "token_start": 284, "token_end": 288, "entityLabel": "period"}, {"text": "LMU-BASF", "start": 1876, "end": 1884, "token_start": 310, "token_end": 312, "entityLabel": "company"}, {"text": "July 2017 - Jan 2018", "start": 1902, "end": 1922, "token_start": 317, "token_end": 321, "entityLabel": "period"}, {"text": "Texas A & M University", "start": 2306, "end": 2328, "token_start": 377, "token_end": 381, "entityLabel": "company"}, {"text": "May 2015 - July 2015", "start": 2329, "end": 2349, "token_start": 382, "token_end": 386, "entityLabel": "period"}, {"text": "Indian Institute of Technology, Bombay", "start": 2539, "end": 2577, "token_start": 415, "token_end": 420, "entityLabel": "company"}, {"text": "May 2013 - May 2014", "start": 2578, "end": 2597, "token_start": 421, "token_end": 425, "entityLabel": "period"}], "relations": [{"child": 86, "head": 82, "relationLabel": "duration"}, {"child": 189, "head": 186, "relationLabel": "duration"}, {"child": 227, "head": 225, "relationLabel": "duration"}, {"child": 284, "head": 279, "relationLabel": "duration"}, {"child": 317, "head": 310, "relationLabel": "duration"}, {"child": 382, "head": 377, "relationLabel": "duration"}, {"child": 421, "head": 415, "relationLabel": "duration"}]}, {"document": "SHAMSUZZAMA HODA      shams.aakifah@gmail.com            +91-8087118260              18 years of experience in the Analysis, Design, Development, Implementation and Production Support of applications.  Experience in Data Analysis, Data Cleansing, Data Migration, Report Generation and Application DBA.  9 years of experience in Oracle ,SQL,PLSQL, 3+  years in Pro*C Oracle  4+ years of experience in Investment Banking , 7 years in Insurance and 6 years in travel domain.  Hands on experience in handling client onshore as well as offshore.  Good Exposure for working in GCC region.  3+ years of experience in Application DBA.  Proficient in all phases of the SDLC and well exposed to the Quality Processes.  Ability to work in highly visible and high-pressure environment.  Project execution in compliance with CMM Level 5 standards.  Building knowledge base by conducting sessions for sharing best practices/ lessons learned.  Experience in Transition knowledge with 5 resources from other organization.  Professional mindset having worked with world's one of largest bank and Insurance company.  Strong analytical skill, Good inter personal skill.        WORK HISTORY  Duration  Organization  Designation  Mar 2015 to till date  IGT Solutions, Gurgaon  Tech Lead  Dec 2005  Feb 2015  Tata Consultancy Services. PUNE  Assistant Consultant  May 2002  Nov 2005  Polaris Software Lab, Chennai  Systems Engineer      ACADEMICS  Degree/Certificate  College/University  Year of Passing  Master of Computer Science and Application(M.C.A)  AMU Aligarh  1999  Master of Science (M.Sc.  STATISTICS)  AMU Aligarh  1996  Bachelor Of Science(B.Sc. STATISTICS)  AMU Aligarh  1994       PRIMARY TECHNICAL SUMMARY  SKILLS  TOOLS  Language  SQL,PL/SQL, Pro*C  Databases  Oracle 11g, 12c, SQL Server  Scripting  Shell Scripting  Operating Systems  Unix/Windows  Tools  SQL Loader, External Table, TOAD, SQLDeveloper, JIRA, Data pump, RMAN       SECONDRY SKILLS:  Data Science  R, Python, Numpy, MatplotLib        Big Data  HDFS, Hive, Impala, Sqoop        Database  AWS Redshifts, SQL Server, MongoDB    CERTIFICATIONS:  OCP: Oracle certified professional.  INS-21 Property and Liability Insurance Principles.        WORK EXPERIENCE:   Nov 2019 to Dec  2020: SITA Oman Project, Muscat Oman (Onsite, Project for Royal Oman Police)  Data warehouse: Working on data ware house to develop SQL queries to research, analyze, and troubleshoot data to create business reports related to immigration and traffic.  Roles and Responsibilities:  Client Interaction, requirement gathering.  Data Analysis.  Data cleansing.  Report generation for Immigration and Traffic.    Jan 2016 to Nov 2019:  SITA Oman Project, Muscat OMAN (Onsite, Project for Royal Oman Police).  eVISA:  eVisa is an online application which facilitates PROs and travelers to apply for visa online, which   Increases efficiency and allows the ease of processing of the applicants. It has two portals, public and DGPR portal.  Working on SQL Developer tool extensively to create plsql procedure, function, triggers, packages and SQLs for implementing new features one visa applications.  VIS: VIS system is central repository of   Tracking visa issues by eVisa.  Tracking Entry and exit movement performed by immigration.  Tracking in country data.  Perform risk assessment.  Technology: Oracle11g, SQL, PLSQL, UNIX  Roles and Responsibilities:  Oracle objects creation, providing grants and privileges.  User creation and grants given to user.  Export/Import of schema/Tables from one database to another database.  Comparison of schemas across DBs.  Managing memory, clearing memory space.  Database patches preparation and implementation across the database.  Poorly written queries were tuned by implementing indexes and queries are rewritten with sql logic to reduce the resource time.  Shell scripting to find archive log cleanup every day.    March 15 to Dec 15, SITA Oman Project, Muscat OMAN (Onsite, For Royal Oman Police).  DMS (Data Migration and Synchronization).  Application for visitors information system and visa application was on main frame. New system VIS and evisa are developed using backend as Oracle. Main frame data are migrated to oracle.  Roles and Responsibilities:  Application DBA.  Client coordination in Oman and requirement gathering.  Data cleansing, column mapping, data migration from legacy system to new system by writing plsql procedures, functions and packages.  Report generation from new system.  Technology: Oracle11g, PLSQL, VNC, SVN, Sql loader, toad, Sql developer.  June 2014 Jan 15  HSBC ( HONGKONG and Shanghai Banking Corporation)  SPARC: SPARC is designed for sub ledger accounting using Microgen Aptitude Hub (MAH) for deals flowing through multiple source systems like Murex, Sophis and Summit. It is a complicated architecture dealing with daily processing of deals, notionals and cash flows, valuation processing. Business as usual activities are governed as per the Agile and Scrum methodologies.  Extracts are generated with processed trades and sent it to downstream system for reconciliation and general ledger preparation.     Technology Used: Oracle, PLSQL, UNIX, shell scripting.  Roles and Responsibilities:   1. Team Lead. 2. Single point of contact with client. 3, working at client site HSBC GLTi Pune India. 4. Understanding the requirement and sharing with team members. 5. Rota preparation for support work for the team. 6. Looking into the quality process.7. Monitoring production tickets.  Sept 2013  May 2014   Western Union  Brand Central: It is a report generating tool, which generates market Intel Report and map point Report for agents. It takes input from three different applications, which consists of agent information, settlement details and process it to generate Reports.  EAS (Entity Analytic System): The main purpose of EAS is for customers data matching and is used in CDB. It get data from CDB as input, applies some matching rules and generate gallectic id and return it to CDB application. gallectic Ids are used in AML ( Anti Money laundering).  CDB (Consumer database): All processed data is stored in CDB, which is used for reporting purpose and helps the business users to take further decisions. BO is used as a reporting tool.     Technology Used: Oracle, PLSQL, UNIX, shell scripting, SQL Server, VB.      Roles and Responsibilities:  Leading a team of four people, Single point contact to client, Interacting with other application if required, Report matrix generation, Involved in month end activities, Transition Process and quality checks.    Apr 2013 to Aug 2013: Toysrus:   Registry  2B: The primary intent of the project was to improve the BRU Baby Registry guest experience for both registrants and purchasers. As part of this project we had redesigned the checklist of in-store systems (missing and forgotten list) to behave similarly to on-line checklist. New email capabilities has been introduced as below     EDQ being the single source of item data in ToysRUs the registry back end system has been migrated to take the item foundation data from EDQ instead from old legacy system.  Email to merchants/marketing when showcase items become discontinued, stop sale, registry ineligible, or does not have a registry categorization.  Email to all registrants (store and online) when any hard line item they are registered for becomes discontinued, or flagged for stop sale.  Welcome email to registrants (real time) on creation of new registry/wishlist online / in-store.    Environment:   Oracle PLSQL.  Sept 2012  Feb 2013 BANCS  BANCS- AXA Belgium: BANCS is a TCS product for banking and insurance segment catering to the entire BFSI domain. BANCS is implemented in many financial sectors in India and abroad. AXA is an Insurance company which is using BANCS for implementing it in various countries customized as per country specific requirement.  Feb 2010  July 2012: The Hartford Insurance.  The CatRM (Catastrophe Risk manager) application provides the ability to analyze the risk associated with the location with natural perils and man-made perils. Perils include Terrorism, earthquake, flood and hurricane. It interacts with various other application to get regeocoded data and flood and earthquake history for a location.  Environment:    Oracle, PLSQL, SQL, UNIX, shell Scripting.  Role:   Sr Developer.  Responsibilities: Requirement gathering, analysis and high level solution. Coordinate and communicate on site team. Designing solution Design specification, Effort estimation, Develop the code, Unit testing, system testing, quality process, creating test data, onsite offshore coordination and regular communication with clients on various aspects, Production support and Release management.    May 2006  Jan 2010: The Hartford Insurance.  TABS (Total Accounting and Billing System):  The Hartford Insurance group (HIG) underwrites both life and non-life policies. The non-life organization is broadly divided as Personal Insurance and Commercial Insurance. TABS is the direct billing system of the non-life organization, which caters to both personal and commercial insurance.     Environment: PLSQL, SQL, UNIX, Oracle Forms.  Role:   Sr Developer.   Responsibilities:  Requirement gathering, analysis and high level solution. Coordinate and communicate on-site team. Solution Design specification, Effort estimation, Develop the code, Unit testing, system testing, quality process, creating test data, on-site offshore coordination and regular communication with clients on various aspects, Production support.    Dec 2005 Apr 2006: AIG (American International Group)  VARS (Vendor assignment and reporting system): AIG hires vendors for evaluation of claims, made by the customer. It has two main applications, standalone and external. Standalone application is used by the employee of AIG, mainly Super administrator, administrator and a third party member lines of business. For hiring a vendor, by this process AIG sends a set of questionnaires for answer and to evaluate the same.  Environment: Oracle, PLSQL  Role:   Developer  Responsibilities:     Requirement Analysis, Preparation of system requirement specification, Creating tables, sequences, indexes and views, Developing procedures and packages, Unit testing and system testing, Quality process (Internal and External quality) , Coordinating with client and on site counterparts.   Sept 2003  Dec 2005: Citi Bank. DMF (Domestic Mutual fund System)  Domestic Mutual Fund System is a part of investment product family offered under the ORBIT suite of banking products by Citibank, mainly to its retail domestic customers. It is used for offering local third party mutual funds and government bonds to resident customers.  SIP (Systematic Investment Plan) User can give instruction for subscription on every month.  SWP(Systematic withdraw Plan) User can give instruction for redemption on every month.  Alert(Whenever a customer asset allocation deviates from the recommended asset allocation,  Automatic Email alert is send to relationship manager.   Online profile (If customer profiling is expired, then online profiling is available)  Order letter generation (After contract order letter for transaction is generated)    Environment:   Pro*C, Oracle, UNIX, PLSQL  Role:   Team Member  Responsibilities:     Requirement analysis, Design document preparation, Writing unit test cases and system test cases, Testing, Unit test results and system test results, Production Support, Coordinating with client.    Oct 2002 to July 2003. Citi financial. CASS (Chesapeake Appraisal and settlement system):  Legacy system was in fox-pro database. Data are migrated from fox-pro to oracle database which is developed for new application which is used for Appraisal of Property, Title Search & Title Policy, Closing & Settlement Services, Flood Report and Automated Valuation Model for property appraisal.  Environment:  Pro*C, Oracle, PLSQL, SQL Loader.  Role:  Team Member (Data Migration)    PERSONAL DATA  Date of Birth/Nationality  20th May 1972/Indian  Marital Status /Gender  Married/Male  Passport Number  Valid   M6157929 expiry date  29/01/2025      Page 1 of 4", "tokens": [{"text": "IGT Solutions", "start": 1232, "end": 1245, "token_start": 226, "token_end": 227, "entityLabel": "company"}, {"text": "Mar 2015 to till date", "start": 1209, "end": 1230, "token_start": 220, "token_end": 224, "entityLabel": "period"}, {"text": "ata Consultancy Services", "start": 1288, "end": 1312, "token_start": 240, "token_end": 242, "entityLabel": "company"}, {"text": "Dec 2005  Feb 2015", "start": 1267, "end": 1285, "token_start": 234, "token_end": 238, "entityLabel": "period"}, {"text": "Polaris Software Lab", "start": 1362, "end": 1382, "token_start": 255, "token_end": 257, "entityLabel": "company"}, {"text": "May 2002  Nov 2005", "start": 1342, "end": 1360, "token_start": 249, "token_end": 253, "entityLabel": "period"}], "relations": [{"child": 220, "head": 226, "relationLabel": "duration"}, {"child": 234, "head": 240, "relationLabel": "duration"}, {"child": 249, "head": 255, "relationLabel": "duration"}]}, {"document": "                                              Srinidhi G                                            Rajeev B Pandey, India Program Director - IndEA  ( Digital Transformation Program) , MeitY                                                                                                          (Driving IndEA  Digital Transformation Program across Pan India)                      Digital Transformation Programs  Large Deal Architect - e2e Solutioning- Program Management-Solution Selling                                                                                                               e mail   rajeevpandey0987@gmail.com, mobile  - +91   9810889712 IST                                                                                                              Positioning  New Delhi , India      1                                                                             Past Organization  IBM (BU: Global Business Services) , Tech Mahindra , Uniport , entity , Transasia  .  Domain  e-governance, Indian Defense, BFSI (Banking and Insurance) , smart Cities , Energy and Utilities , Healthcare ,   Countries Worked  USA, Europe (Belgium, France, and Norway), Japan,  HK and India.  Certifications -  Sr. Certified Enterprise Architect Expert Level , Sr. Certified Project Manager , API Management , BlockChain  Consulting Level , Cloud enabled Services , IOT-Watson , TOGAF 9.2 ,Microsoft SE-Professional ,Rational Unified Process ,  Digital 201  , SOMA , IBM Signature Selling Specialist ,  AI- Consulting  + 9 more industry certifications.    Credentials - nominated for Honoris Causa (Honorary Doctoral Awards) by CIAC and Top 10 Enterprise Architect Practitioner  Awardees by ICMG , India , No. 1 Performer in Transasia Group , IBM Thought Leader - Cisco , Service Now- CoE award   Academics  Post Graduation in Computers from National Computing Center , Bachelor of Engineering  Electronics,Nagpur      Executive summary-     Sr. Certified Industry Leader  , having more than two decade , of proven global experience ,in  e2e Solutioning of Large  /Complex   engagements , , Devising Enterprise / Solution Architecture , Program Management including P&L for large  engagements , Crafting  Digital Service Platforms & eco systems, Business enablement through  technology enablement   ,Setting up CoE , Capacity Building ,  Infusion of Software Engineering  Practice/s  in Delivery Centers and running mentorship   Programs .    Current Role  India Program DirectorIndEA  Digital Transformation Program .  Designation  Sr. Principal Consultant , L4 Director Level.   Period : 02nd Sep , 2019 until date.  Organization  Ministry of Electronics and Information Technology, GOI.    In my Current role with MeitY , I am entrusted to lead IndEA- Digital Transformation  Program across Pan India . Its an unique  outcome based KPI driven program, aiming to improve UN-SDG rating , by improvement in Service Delivery efficiency,  through Technology enablement , in an Federated Enterprise Architecture Pattern.     my role encompass of  creating awareness , custom solutioning , devising Value Proposition for adoption/funding  and   delivery  through Partners/OEM's , by bringing participating Players, with in eco system on common Platform.    The integrated Platform also provisions candidates for Predictive Analysis using AI , smart contracts using  Block chain ,  Integration with weather forecasts  , IOT sensing devices  , social analytics , payment gateways, data exchange  platforms ,  analytics providing enhanced value  to suppliers and consumers of this Platform.    As of now , I am involved in some of the IndEA adoption  engagements with Meghalaya , Railways , Mizoram , WCD , Power ,  Education , Health  and MoHUA Ministries ( Smart Cities) .    Coming from consulting org. like IBM - Global Business Services , Tech M , entity , uniport  had been leading Large Scale  Integration Programs and devising value proposition for complex solutioning engagements.     had also been involved ,in enabling Sales , in solutioning of Large bids ( 50m$+) , Partner / OEM evaluation / Selection and  Proposal Defense . had managed  Product Development and Program management (P&L) with Geo distributed teams .                                                 Rajeev B Pandey, India Program Director - IndEA  ( Digital Transformation Program) , MeitY                                                                                                          (Driving IndEA  Digital Transformation Program across Pan India)                      Digital Transformation Programs  Large Deal Architect - e2e Solutioning- Program Management-Solution Selling                                                                                                               e mail   rajeevpandey0987@gmail.com, mobile  - +91   9810889712 IST                                                                                                              Positioning  New Delhi , India      2   As global worker, for more than two decade , lived and worked mostly in USA  ,with few stints in  Japan , HK and few cities of  Europe along with India   .    had spearheaded,  some of the significant global engagements, including co-location of on-primes data centers for an Global   Bank , Creating an Integrated Business Platform  for an US Insurance giant in USA , Designing an Digital Service Platform for an  Prominent Telecom Player in Japan , IT modernization of an Investment Bank in Germany , Setting up Analytics Platform for  UAE bank , Smart Cities Proposals Offering , Delivering an IT Modernization Project for an Indian Defense to name a few.    had also been involved ,in enabling Sales in solutioning of Large bids ( 50m$+) , Partner / OEM evaluation / Selection and  Proposal Defense . had managed  Product Development and Program management (P&L) with Geo distributed teams .     The largest Team handled is 550+ FTE for an Insurance Client in US /India.    ..    Past Role: Solution Leader ( Large and Strategic Bids) , General Manager  SGS Enterprise  , Tech M , India ( Nov , 2016  until Aug, 2019)  :      As an Sales Enabler  , I was leading Pre-Sales , which  includes assessment and Solutioning of Large and Strategic Bids, RFP  /RFI Response , selection/ evaluation  of Partners /OEM, Devising Value Propositions  , own the AMS and cross-integrated  Solution for strategic pursuits providing  holistic view of client architecture, and acting  as an integrator ,across internal  service lines and stakeholders, to create a joint up solution and present the same to Client as Tech M Offering.    Detail Responsibilities -        Provide proactive technical counsel to Sales and key management resources on technical strategy, direction and  Capability Skill Roadmap.    Improve and broaden client access to global technical and innovation expertise.    Work with client Line of Business executives and their teams to identify clients business and IT needs and design  high level solutions to fit business problems.    Create solution design assets for internal use to accelerate sales cycles and positively impact solutions quality.    Responsible for Client architectural activities from requirements analysis through systems, application and/or  process design specification.    Sharing of understanding the technology trends and how they can help assigned clients and leverage research and  development groups to bring innovation to assigned clients.    Leads negotiations with engagement partners and team members, subcontractors, customers, to define project  goals and strategies for attaining them.    Assesses business and technical impact of solutions. Leads the implementation of solution or provides expert  guidance to the solution implementation team.    Participates in exploratory activities into new market sectors, architectural solutions, technologies and the use of  business partners; leads selection of methodologies, tools and components of total architectural solutions; exhibits  strategic vision of functional or unit mission and applies vision in engagements; influences people & organizations,  including client and executive management, on issues related to the development of architectural solutions     Key Solutions Proposed       Smart City Proposals .    IT Modernization engagement for an Large Insurance Client in India.    Application Modernization Engagement for an Para-Military force  in India.    Co-location Shift of Data Centers of an Global Nationalized Bank.    Large Scale Application Modernization India Defense engagements ( Multiple Bids).                                              Rajeev B Pandey, India Program Director - IndEA  ( Digital Transformation Program) , MeitY                                                                                                          (Driving IndEA  Digital Transformation Program across Pan India)                      Digital Transformation Programs  Large Deal Architect - e2e Solutioning- Program Management-Solution Selling                                                                                                               e mail   rajeevpandey0987@gmail.com, mobile  - +91   9810889712 IST                                                                                                              Positioning  New Delhi , India      3    Digital Aggregator Platform for Insurance Client in India.    IBM , Global Business Services , Delivery Project Executive /Enterprise Architect / Client Solution Executive  ,   India /USA/Japan /Europe Geo/Hong Kong - ( Jun, 2007  Nov, 2016)  :    I had worked in IBM, Global Business Services,  for a more than decade into Technology, Program Management &  Pre Sales  Solutioning for Global Clients working in USA , Japan , Europe, HK  and India.      As Client Solution Executive was engaged with clients across multiple geographies to map customer Pain /Motivations, built  Solutions to address them, devised Value Propositions, explored new opportunities and craft response to complex and large  opportunities.    As Delivery Project Executive,  had also successfully delivered some large scale Integration Projects like EBPS. StateFarm in  USA  , American Express. USA , Japan Post .Tokyo , EUEAI . Brussels, edf.France , MDM. Abu Dhabi Bank , Cathay Pacific. HK  to  name a few .    As IBM Enterprise  Architect ,  I had also been an IBM Enterprise Architect for few of Global Clients in USA, Europe and UAE  for implementation of Large Scale Business Transformation program, integrating BPM  through Consumer Channels ,  Middleware , Business Applications ( Product Stack ) ,  Third Party Integration  and external entities .    As an Technologist, I had also contributed in IBM Global Solution Architect Repository, in setting standards for best Software  Engineering Practices and devised Sequence Design Pattern for SE Practice.    Overall Key Role & Responsibilities in IBM :     1. Manage Delivery Management (Onsite / Offshore).  2. Pre-Sales Solutioning of Large /Complex Deals.  3. Manage Client Relationship & facilitate Business Development.  4. Manage Solution Delivery Processes.  5. Provide Technical Leadership.  6. Contract Management.  7. Capacity Building and Mentorship.      Key Technology Initiatives:      Spearheaded Application Portfolio Rationalization engagement of Global Bank in USA    Devised and Implemented Enterprise Integration framework for a Global Bank in Japan.    Enterprise Application Integration for Retail Banking Client in Europe.    Solutioning for an Integrated Digital Platform for a Global Banking Client @USA.    Devised Architecture of MDM-ETL  engagement of Large Retail Bank in UAE, which resulted in streamlining data  storage, data accuracy and Distribution over real time to Business Applications, increasing customer satisfaction  Index through B2C response.    Devised and Implemented Middleware Integration Solution for European Union in Brussels , using SOA Architecture    Devised EA Assessment and Enterprise Integration Roadmap (SOA Road Map) for Large Insurance Client @USA.    Devised and Implemented Migration Road Map for Application  Transformation for Large Scale Investment Bank in  India                                                          Rajeev B Pandey, India Program Director - IndEA  ( Digital Transformation Program) , MeitY                                                                                                          (Driving IndEA  Digital Transformation Program across Pan India)                      Digital Transformation Programs  Large Deal Architect - e2e Solutioning- Program Management-Solution Selling                                                                                                               e mail   rajeevpandey0987@gmail.com, mobile  - +91   9810889712 IST                                                                                                              Positioning  New Delhi , India      4     entity -  Functional Head & Software Architect : Energy and Utilities  , India   ( Apr , 2005  June , 2007) .      Prior Joining IBM , I was engaged  with Energy and Utilities Company , in managing end to end Product Development Life  Cycle  , of Energy and Utilities Product Portfolio , for Energy Management Systems .    I was involved in devising Technical Solutions on Solution / Application Architecture, managing Distributed Teams of Project  Managers, Team Leads, Developers, Testers at Delivery Center in Gurgaon.    I was also instrumental in setting up Architect Competency, Best practices of Software Engineering into Delivery Cycles and  Capacity Building.  As an Change Agent, I had revamped Delivery centers, by setting up Best practices in SDLC cycle , while  implementing IBM Rational Unified Process (RUP) and related IBM Tools like Requisite Pro, IBM Rational Software Architect,  to Streamline Solution Delivery for Product Development Life Cycle.    The Product life cycle development,  which I spearheaded, were Smart Metering, RMS, M Cube, AMR / AMI Solutions.      Aditya Birla Group , Asst. General Manager - Software Services , India ( Oct , 2003  Mar, 2005 ) :     I  was engaged, for Custom Development of Enterprise Wide, Multi-Currency Business Suite - vsf connect and management of  Infrastructure and Data Center set up , for Enterprise Users across Grasim Industries.    I was  responsible for Program Management including P&L ,  for end to end Delivery Cycle Management and Implementation  of the Enterprise Suite , by Managing PMO Office , Project Managers , Stakeholder Management , Contract Management ,  Vendor Management  with  Team Size of 300+ Resources  , on  T&M engagement  .    Some of my major Tasks includes       Program Management for Design and Development  of Vsf connect,  enterprise wide application suite.     Setting up of Development and Testing Environments with IBM Processes and IBM Tools.     Management of the IT Infrastructure for the Enterprise application Deployment.    Managing the Maintenance and enhancement of the enterprise applications.    Planning and execution of new software development programs.    Design , Documentation and user training for the enterprise software applications    Data Center set up, Management and support for enterprise Application.    ..    TransAsia ,  Project Manager  Software Development , Healthcare , Mumbai ,  India / France ( Nov , 1998  Oct , 2003 ) :     I  was responsible for Project Management and Technical Leadership to Projects involving Fully Automated Cynical Analyzers  and Laboratory Management Information Systems, managing Multiple Teams for Different Product Variants for Healthcare  Large Pathology Labs in Europe and Japan.  I also presented company , in Global trade fairs , in Paris and Tokyo.    I  was also instrumental in implementing IBM Rational Unified Process with IBM Rational Tools like Requisite Pro , Rational  Rose , Rational Purify , Quantify , Clear Quest , Robo  for Streamlining Solution Delivery .    I was awarded No 1. Performer   in the Entire Trans Asia Group for two consecutive years for Quality Product Design and  Delivery  .  The Software Products claim International Accelerations in European and Japanese Labs.                                                Rajeev B Pandey, India Program Director - IndEA  ( Digital Transformation Program) , MeitY                                                                                                          (Driving IndEA  Digital Transformation Program across Pan India)                      Digital Transformation Programs  Large Deal Architect - e2e Solutioning- Program Management-Solution Selling                                                                                                               e mail   rajeevpandey0987@gmail.com, mobile  - +91   9810889712 IST                                                                                                              Positioning  New Delhi , India      5     Uniport  Systems , India /USA  ( Nov , 1991  Nov , 1998 ) :           I   started  my career with Software Services Organization, as Cob 85 Programmer and worked on Business Systems using  Client Server Solutions using D2K .VB5 /Oracle, Promoted to System Analyst, where I was into  Client Facing Role, Mapping  User Requirements into Designer 2000 and Database Design using Erwin, Solution Design and Inputs to Project Planning.     During my tenure , I lived and worked in  USA on H-1B , for few years on Walmart and NYSE Projects .    Some of the Major  Key Projects , in which , I Contributed , were      1. ERP Customization and Implementation of Retail Distribution Chain in Chicago.  2. NYSE Value Chain enterprise suite  , NYC , USA.  3. Rail Tendering System for Central Railways   4. Corporate MIS Application for State Bank of India   5. Time Control Monitoring System for IndoRAMA  6. Integrated ERP Solution System for DCL Polyesters.  7. Financial Package for SME Industry .        Thanks for Reading  End of Document    ", "tokens": [{"text": "Ministry of Electronics and Information Technology", "start": 2643, "end": 2693, "token_start": 404, "token_end": 409, "entityLabel": "company"}, {"text": "02nd Sep , 2019 until date", "start": 2600, "end": 2626, "token_start": 394, "token_end": 399, "entityLabel": "period"}, {"text": "Tech M", "start": 6075, "end": 6081, "token_start": 978, "token_end": 979, "entityLabel": "company"}, {"text": "Nov , 2016  until Aug, 2019", "start": 6092, "end": 6119, "token_start": 983, "token_end": 990, "entityLabel": "period"}, {"text": "IBM", "start": 9439, "end": 9442, "token_start": 1484, "token_end": 1484, "entityLabel": "company"}, {"text": "Jun, 2007  Nov, 2016", "start": 9597, "end": 9617, "token_start": 1513, "token_end": 1519, "entityLabel": "period"}, {"text": "Aditya Birla Group", "start": 14105, "end": 14123, "token_start": 2265, "token_end": 2267, "entityLabel": "company"}, {"text": "Oct , 2003  Mar, 2005", "start": 14178, "end": 14199, "token_start": 2279, "token_end": 2285, "entityLabel": "period"}, {"text": "TransAsia", "start": 15356, "end": 15365, "token_start": 2489, "token_end": 2489, "entityLabel": "company"}, {"text": "Nov , 1998  Oct , 2003", "start": 15449, "end": 15471, "token_start": 2507, "token_end": 2513, "entityLabel": "period"}], "relations": [{"child": 2507, "head": 2489, "relationLabel": "duration"}, {"child": 2279, "head": 2265, "relationLabel": "duration"}, {"child": 1513, "head": 1484, "relationLabel": "duration"}, {"child": 983, "head": 978, "relationLabel": "duration"}, {"child": 404, "head": 394, "relationLabel": "duration"}]}, {"document": "                                       Jagan_K_1   .  Having over-all Software\\Product Development experience of almost 10 plus years with leading  companies in domains like BPO management system, HRMS, Supply-chain management, Advisory  and Health Care. Have gained confidence and adequate knowledge in developing, designing and  architecting solutions in ETL, Data-Warehouse, Business Analytics, Business Intelligence, Big Data  Analytics, Cloud Infrastructure and Security.   AWS Big Data      Very Good   SQL, Python, PySpark      Very Good   ETL      Very Good   Shell, YAML, UNIX, Docker      Good   Airflow, Superset, Presto, Ranger, Kafka     Good   Jagan Kumar  Senior Data Engineer    Chennai, TN, 600100    9884674334     email2jagank@gmail.com    Skills  Work History  2016-02 -  Current  Senior Consultant   EMIS Health India Pvt Ltd, Chennai, Tamilnadu   Design, implement and maintain all AWS infrastructure and services   within a managed service environment Design, Deploy and maintain  enterprise class security, network and systems management  applications within an AWS environment    Implement process and quality improvements through task  automation. Institute infrastructure as code, security automation and  automation or routine maintenance tasks    Perform data migration from on premises environments into AWS   Support the business development lifecycle (Business Development,   Capture, Solution Architect, Pricing and Proposal Development)   Strong AWS knowledge on S3, EMR, ECS, EC2, Glue, Lambda, IAM,   Kafka, RDS, Route 53, VPC, Code build, Code pipeline, Cloud watch,  Cloud formation.    Strong in Hadoop Ecosystem and its major components and Pyspark,  Presto, Airflow, Hive, Ranger, Livy ,Zeppelin  .    .  2014-04 -  2016-01  Senior Software Engineer   Hexaware Technologies Ltd, Chennai, Tamilnadu   Designed and created ETL code installations, aiding in transitions from   one data warehouse to another.   Contributed ideas and suggestions in team meetings and delivered   updates on deadlines, designs and enhancements.   Interpreted data models for conversion into ETL diagrams and code.  2012-11 -  2014-04  Lead Engineer   HCL Technologies Ltd, Chennai, Tamilnadi   Selected methods and criteria for warehouse data evaluation   procedures.   Developed and modified programs to meet customer requirements.   Completed quality reviews for designs, codes, test plans and   documentation methods.  2011-11 -  2012-11  Software Engineer   Melstar Information Technologies Ltd, Chennai, Tamilnadu   Implemented system enhancements to propose, design and develop   solutions to fulfill requests and address problem reports.   Designed and implemented TSQL queries for reporting and complex   solution development.   Maintained complex T-SQL queries, views and stored procedures in   multi-database environment with little supervision.   Supported multiple concurrent projects enabling delivery efficiencies   and technical resolution.  2009-09 -  2011-11  Junior Programmer   Allsec Technologies Ltd, Chennai, Tamilnadu   Created optimal technical solutions to user needs through research and   in-depth system analysis.   Developed, implemented and optimized stored procedures and   functions using T-SQL.   Maintained complex T-SQL queries, views and stored procedures in   multi-database environment with little supervision.  .    .  Father's Name - K. Kumar  Mother's Name - K.Vasanthi  Permanent Address - No.9/950, 3rd Street, Sivagami Nagar, Medavakkam, Chennai -  600100  Languages - English and Tamil  Date of Birth - 1st July 1988  Marital Status - Married  Passport Number - R9732346  Date of Expiry - 26th March 2028  Education  2012-01 -  2014-02  Master of Science: Information Technology   Madras University - Chennai  2006-01 -  2009-04  Bachelor of Science: Computer Application   Guru Nanak College - Chennai  Personal Information  .   ", "tokens": [{"text": "Hexaware Technologies Ltd", "start": 1793, "end": 1818, "token_start": 340, "token_end": 342, "entityLabel": "company"}, {"text": "2014-04 -  2016-01", "start": 1746, "end": 1764, "token_start": 327, "token_end": 334, "entityLabel": "period"}, {"text": "HCL Technologies Ltd", "start": 2169, "end": 2189, "token_start": 410, "token_end": 412, "entityLabel": "company"}, {"text": "2012-11 -  2014-04", "start": 2133, "end": 2151, "token_start": 398, "token_end": 405, "entityLabel": "period"}, {"text": "Melstar Information Technologies Ltd", "start": 2480, "end": 2516, "token_start": 468, "token_end": 471, "entityLabel": "company"}, {"text": "2011-11 -  2012-11", "start": 2440, "end": 2458, "token_start": 456, "token_end": 463, "entityLabel": "period"}, {"text": "Allsec Technologies Ltd", "start": 3015, "end": 3038, "token_start": 558, "token_end": 560, "entityLabel": "company"}, {"text": "2009-09 -  2011-11", "start": 2975, "end": 2993, "token_start": 546, "token_end": 553, "entityLabel": "period"}], "relations": [{"child": 546, "head": 558, "relationLabel": "duration"}, {"child": 456, "head": 468, "relationLabel": "duration"}, {"child": 398, "head": 410, "relationLabel": "duration"}, {"child": 327, "head": 340, "relationLabel": "duration"}]}, {"document": "                                                   Jeevan R Polamarasetty  Mob: +91 973 983 0104   Email: prjeevan@gmail.com     Summary   Seasoned and versatile Management Professional with incredible experience acquired over the years in diverse areas   encompassing Process Management & Optimization, Continuous Improvement, Strategic Management, etc.     Strong People Management Skills, capable of managing team members of various capacities and currently leading the team  of 40 who manages Marketing Data Onboarding, Data Stewardship, Data Governance, Data Analytics, Business Analytics,  Digital & Content operations for Marketing         Key Skills & Strengths   People Management, Career Development & GPTW Initiatives    Data Stewardship & Customer Hierarchy  Data Mining, Data Profiling & Data Trending   Data Governance & Data Quality  SQL & SSIS   GAP Analysis, Root Cause Analysis & White Space Analysis         Professional Overview   Pega Systems India Pvt. Ltd  April 2017 to Till Date    Data Management & Operations    Marketing Contacts enablement & Attribute completeness    Whitespace Analysis & Statistical Sampling    Customer Hierarchy & Foundational Analytics    Stakeholder Connect & Requirements Gathering    Data Quality Initiatives & process improvements      People Management    Recruit the team based on the skills & culture fitment   Budget forecast & cost centers management   Work with L&D teams to plan the required trainings for the team   R&R Framework design & Implementation to motivate the team members   Performance Management & Career Development    Captains program/Focus Group to pave the way for the next line of leadership    GPTW initiatives: Focused way of approaching the people issues   EMC Data Storage Systems - February 2012 to March 2017   Data Analytics & GTM Analytics:   Segmentation & Vertical Analysis    Market Research, Segmentation & White Space Analysis   Increased Global & Site Dunsing accuracy from 58% to 95%   Transition & Migration:    Involved in multiple process transitions & travelled to US 3 times for process migration &  tool implementation    Engaged with BT team to design & develop CRM tool (Informatica  Siperian tool) for  MDM    Prepared functional specification documents and use cases   Responsible to prepare test scenarios (UAT) & Defect Tracking       DATA   Stewardshi  p   mailto:prjeevan@gmail.com   Digital Transformation & Visualization:    ETL processes, Process Derivation (SMART) & Automation   SSP (Self Service Portal) designed on PHP for submitting fuzzy requests & GU information   Forbes & Fortune companies analysis on Tableau   Wipro Corporate Knowledge Services (for EMC) - February 2008 to January 2012           Customer Data Intelligence:    Hierarchy Management & Attribute Enrichment, maintenance with the help of tracking  tool raised from different users of customer master database    Design the processes to eliminate all the manual intervention and run different fuzzy logic  look ups by comparing DNB data to reach at single point of reference    Data Preparation & Verticals   Stewardship:    Manage the stewardship team on householding Net New records, current quarter  and  other opportunity files to keep the data completeness rate always more than 90%    Match & Merge the Golden Site Key (GSK) based on the Site relationship   Top accounts validation and maintenance on a weekly basis along with Mergers &   Acquisition (M&A) analysis   Generate the scorecard based on the overall company revenue for business insight and for   betterment of stewardship activities   Monitor the SQL tables of the data feed from Oracle 11i, SFDC, MAP (Siperian Tool) & GDW   to find the data issues and provide solutions regularly     GE Money Bank - August 2004 to January 2008    Billing Auditions and gap analysis for more than 20 different credit cards   Promotional coding, finance charge calculations, debiting stores for finance charges   assessed on sales purchased on promotion   Correct the credit profile errors by directly working with Equifax, Experian & TransUnion to   meet FCRA guidelines   Develop Macros for adding the promotional code and to allocate the payment based on   promotion    Researching missing payments and applying credits when the negotiable instruments are  cashed by our banks, encoding error corrections, payment corrections, and funds transfer  between different credit cards    Part of the workstation designing team and tested all the functions from business  standpoint   Margadarsi Chits Pvt. Ltd. - January 2002 to July 2004     Testing the website www.margadarsi.com and report on bugs to the development team   Report logs and measure the traffic based on the number of users visited the website   Update the finance information like late fee charges, Bid amount & dividend amount on   the website   Providing the shells (Linux Shell Scripting) to the users across 85 branches   Collect the data from across all the branches by closing hour and update the outstation   payments in the centralized database   Data backup restoration and validation      Academic Record   MBA (IT) from Amity University, Noida   B.Sc. (Computer Science) from Andhra University, Visakhapatnam   MECs (Computer Science) from National Open School, Visakhapatnam   EMC Proven Data Science Certification         http://www.margadarsi.com/      Skill Mapping & Tools     Customer Data Insight People Management Operations Management Programs & Initiatives   Data Trending   Data Visualization   Hierarchy Mgmt.   Tableau Basics   SQL Server    Individual Development  Planning    Group Dynamics sessions  to capture the team pulse    CU Monitoring &  reporting    Big Picture, small  actions    Creative, Discipline and  Focus    Deriving People  Engagement Events    Execution of theme  based All Hands  across the BU      Social Connect https://twitter.com/prjeevan          in.linkedin.com/pub/jeevan-polamarasetty/16/943/532/   https://twitter.com/prjeevan http://in.linkedin.com/pub/jeevan-polamarasetty/16/943/532/  ", "tokens": [{"text": "EMC Data Storage Systems", "start": 1740, "end": 1764, "token_start": 277, "token_end": 280, "entityLabel": "company"}, {"text": "February 2012 to March 2017", "start": 1767, "end": 1794, "token_start": 282, "token_end": 286, "entityLabel": "period"}, {"text": "Wipro", "start": 2633, "end": 2638, "token_start": 436, "token_end": 436, "entityLabel": "company"}, {"text": "February 2008 to January 2012", "start": 2680, "end": 2709, "token_start": 445, "token_end": 449, "entityLabel": "period"}, {"text": "GE Money Bank", "start": 3741, "end": 3754, "token_start": 634, "token_end": 636, "entityLabel": "company"}, {"text": "August 2004 to January 2008", "start": 3757, "end": 3784, "token_start": 638, "token_end": 642, "entityLabel": "period"}, {"text": "Margadarsi Chits Pvt. Ltd", "start": 4519, "end": 4544, "token_start": 763, "token_end": 768, "entityLabel": "company"}, {"text": "January 2002 to July 2004", "start": 4548, "end": 4573, "token_start": 769, "token_end": 773, "entityLabel": "period"}], "relations": [{"child": 769, "head": 763, "relationLabel": "duration"}, {"child": 638, "head": 634, "relationLabel": "duration"}, {"child": 445, "head": 436, "relationLabel": "duration"}, {"child": 282, "head": 277, "relationLabel": "duration"}]}, {"document": "                                                   Jeevan R Polamarasetty  Mob: +91 973 983 0104   Email: prjeevan@gmail.com     Summary   Seasoned and versatile Management Professional with incredible experience acquired over the years in diverse areas   encompassing Process Management & Optimization, Continuous Improvement, Strategic Management, etc.     Strong People Management Skills, capable of managing team members of various capacities and currently leading the team  of 40 who manages Marketing Data Onboarding, Data Stewardship, Data Governance, Data Analytics, Business Analytics,  Digital & Content operations for Marketing         Key Skills & Strengths   People Management, Career Development & GPTW Initiatives    Data Stewardship & Customer Hierarchy  Data Mining, Data Profiling & Data Trending   Data Governance & Data Quality  SQL & SSIS   GAP Analysis, Root Cause Analysis & White Space Analysis         Professional Overview   Pega Systems India Pvt. Ltd  April 2017 to Till Date    Data Management & Operations    Marketing Contacts enablement & Attribute completeness    Whitespace Analysis & Statistical Sampling    Customer Hierarchy & Foundational Analytics    Stakeholder Connect & Requirements Gathering    Data Quality Initiatives & process improvements      People Management    Recruit the team based on the skills & culture fitment   Budget forecast & cost centers management   Work with L&D teams to plan the required trainings for the team   R&R Framework design & Implementation to motivate the team members   Performance Management & Career Development    Captains program/Focus Group to pave the way for the next line of leadership    GPTW initiatives: Focused way of approaching the people issues   EMC Data Storage Systems - February 2012 to March 2017   Data Analytics & GTM Analytics:   Segmentation & Vertical Analysis    Market Research, Segmentation & White Space Analysis   Increased Global & Site Dunsing accuracy from 58% to 95%   Transition & Migration:    Involved in multiple process transitions & travelled to US 3 times for process migration &  tool implementation    Engaged with BT team to design & develop CRM tool (Informatica  Siperian tool) for  MDM    Prepared functional specification documents and use cases   Responsible to prepare test scenarios (UAT) & Defect Tracking       DATA   Stewardshi  p   mailto:prjeevan@gmail.com   Digital Transformation & Visualization:    ETL processes, Process Derivation (SMART) & Automation   SSP (Self Service Portal) designed on PHP for submitting fuzzy requests & GU information   Forbes & Fortune companies analysis on Tableau   Wipro Corporate Knowledge Services (for EMC) - February 2008 to January 2012           Customer Data Intelligence:    Hierarchy Management & Attribute Enrichment, maintenance with the help of tracking  tool raised from different users of customer master database    Design the processes to eliminate all the manual intervention and run different fuzzy logic  look ups by comparing DNB data to reach at single point of reference    Data Preparation & Verticals   Stewardship:    Manage the stewardship team on householding Net New records, current quarter  and  other opportunity files to keep the data completeness rate always more than 90%    Match & Merge the Golden Site Key (GSK) based on the Site relationship   Top accounts validation and maintenance on a weekly basis along with Mergers &   Acquisition (M&A) analysis   Generate the scorecard based on the overall company revenue for business insight and for   betterment of stewardship activities   Monitor the SQL tables of the data feed from Oracle 11i, SFDC, MAP (Siperian Tool) & GDW   to find the data issues and provide solutions regularly     GE Money Bank - August 2004 to January 2008    Billing Auditions and gap analysis for more than 20 different credit cards   Promotional coding, finance charge calculations, debiting stores for finance charges   assessed on sales purchased on promotion   Correct the credit profile errors by directly working with Equifax, Experian & TransUnion to   meet FCRA guidelines   Develop Macros for adding the promotional code and to allocate the payment based on   promotion    Researching missing payments and applying credits when the negotiable instruments are  cashed by our banks, encoding error corrections, payment corrections, and funds transfer  between different credit cards    Part of the workstation designing team and tested all the functions from business  standpoint   Margadarsi Chits Pvt. Ltd. - January 2002 to July 2004     Testing the website www.margadarsi.com and report on bugs to the development team   Report logs and measure the traffic based on the number of users visited the website   Update the finance information like late fee charges, Bid amount & dividend amount on   the website   Providing the shells (Linux Shell Scripting) to the users across 85 branches   Collect the data from across all the branches by closing hour and update the outstation   payments in the centralized database   Data backup restoration and validation      Academic Record   MBA (IT) from Amity University, Noida   B.Sc. (Computer Science) from Andhra University, Visakhapatnam   MECs (Computer Science) from National Open School, Visakhapatnam   EMC Proven Data Science Certification         http://www.margadarsi.com/      Skill Mapping & Tools     Customer Data Insight People Management Operations Management Programs & Initiatives   Data Trending   Data Visualization   Hierarchy Mgmt.   Tableau Basics   SQL Server    Individual Development  Planning    Group Dynamics sessions  to capture the team pulse    CU Monitoring &  reporting    Big Picture, small  actions    Creative, Discipline and  Focus    Deriving People  Engagement Events    Execution of theme  based All Hands  across the BU      Social Connect https://twitter.com/prjeevan          in.linkedin.com/pub/jeevan-polamarasetty/16/943/532/   https://twitter.com/prjeevan http://in.linkedin.com/pub/jeevan-polamarasetty/16/943/532/  ", "tokens": [{"text": "Pega Systems India Pvt. Ltd", "start": 951, "end": 978, "token_start": 149, "token_end": 154, "entityLabel": "company"}, {"text": "April 2017 to Till Date", "start": 980, "end": 1003, "token_start": 156, "token_end": 160, "entityLabel": "period"}, {"text": "EMC Data Storage Systems ", "start": 1740, "end": 1765, "token_start": 277, "token_end": 282, "entityLabel": "company"}, {"text": "February 2012 to March 2017", "start": 1767, "end": 1794, "token_start": 282, "token_end": 286, "entityLabel": "period"}, {"text": "Wipro", "start": 2633, "end": 2638, "token_start": 436, "token_end": 436, "entityLabel": "company"}, {"text": "February 2008 to January 2012 ", "start": 2680, "end": 2710, "token_start": 445, "token_end": 451, "entityLabel": "period"}, {"text": "GE Money Bank", "start": 3741, "end": 3754, "token_start": 634, "token_end": 636, "entityLabel": "company"}, {"text": " August 2004 to January 2008", "start": 3756, "end": 3784, "token_start": 637, "token_end": 642, "entityLabel": "period"}, {"text": "Margadarsi Chits Pvt. Ltd", "start": 4519, "end": 4544, "token_start": 763, "token_end": 768, "entityLabel": "company"}, {"text": "January 2002 to July 2004", "start": 4548, "end": 4573, "token_start": 769, "token_end": 773, "entityLabel": "period"}], "relations": [{"child": 156, "head": 149, "relationLabel": "duration"}, {"child": 282, "head": 277, "relationLabel": "duration"}, {"child": 445, "head": 436, "relationLabel": "duration"}, {"child": 637, "head": 634, "relationLabel": "duration"}, {"child": 769, "head": 763, "relationLabel": "duration"}]}, {"document": "                                          Microsoft Word - Debabrata_Guha_Resume.docx                        D a t a  S o l u t i o n  A r c h i t e c t    Debabrata Guha  debabrataguha20@gmail.com   ddebb.guha@gmail.com     Senior Business Intelligence  Developer with 9 year   experience in the areas of  Microsoft Business   Intelligence, Data Analytics  and Data Warehousing.   ABOUT ME   SKILLS   Azure (Data engineering)   DAX   SSRS   SSAS (Data cubes)   Excel Vba Macro   Warehousing Design         WORK   From Jun 2020 To    Jun 2021t  IBM India Private Limited  Client: Philip Morris International  Project: Flex L0 and L1 reporting  Location: Kolkata, India and Neuchatel, Switzerland   Flex ,a 3rd party system outside of Phillip Morris  International, generally take cares the electronic device  holder for the e-cigarette. Flex is responsible to take  care all the complaints raised by the customers for the  holder device and to perform seamless inspection, Flex  reuired Product Information along with L0 inspection  results provided by call center agents during 1st level of  support. Flex runs on an environment managed by  Azure cloud. They requires a daily incremental  automated interface with detailed data of L0  inspections. It helps them deriving into the quick and  effective solution for each case.  Flex also share the actions performed by their level 1   technical team. These data are planned for being used  in analyzing and understanding the nature of  complaints in prior and to take the necessary corrective  and preventive actions for achieving better customer   Summary  Designed and developed a new ETL interface with the  help of Azure Data Factory and Azure SQL to convert  and prepare the level 0 and level 1 data as required by  the Flex team.    With the help of Azure function, set up a repository of  L0 files in Azure storage account with a economical  archiving strategy.    Implemented a serverless solution to store the  processed data in datalake and synapse.    Created power BI report for the final reporting of   Roles As  Data Solutions  Architect  (+91) 9804 436 193   AWS (Data Flow)      Core Java   SSIS (ETL)   Power BI   PL SQL/ TSQL   SQL Server   Currently Serving Notice Period   (Last Date is 9th June 2021)                          Power BI & MSBI Developer,  Expert in Power BI,   SQL,SSIS,SSRS,SSAS Data  warehouse modelling and   Reporting Solutions.     Had exposure in working in  AWS cloud system and in   microsoft azure     Worked in both OLTP and  OLAP systems   Job Profile   Certifications   Microsoft Querying Data with  Transact SQL (70-761)   DevOps from DevOps foundation     Microsoft Azure Fundamentals (AZ900)   IBM India Private Limited  Client: Philip Morris International  Project: iQAR improvements 2019  Location: Kolkata, India and Neuchatel, Switzerland   Integrated existing underling service-level related data  from Service now database in order to generate  business insights. All the data transforms are done  using the existing features of Power BI.   Transformed enterprise data into rich visuals for the  easy understanding of underlying data pattern.   Used DAX scripting language for the some logical   transformations of data.   Setup of automatic data refresh and publish reports  allow all its intended audience to avail the latest  information.   Roles  As  Data Solutions  Architect   (Power BI)  Designed and developed a new cube where users can  see all the data related to product specification coming  from PLM (Product Layer Management) and can  prepare reports using that cube.    Optimized ETL job for performance improvement of  several interfaces. Proposed and implemented an  architectural changes for faster execution. It resulted  with 90% faster performance.   Summary  From Feb 2019 To Dec 2019  Administering Relational Databases  on Microsoft Azure (DP-300)   IBM India Private Limited  Client: Philip Morris International  Project: Project KPI management  Location: Kolkata, India   IBM Leadership team felt the need to leverage an analytics  platform to perform real-time or predictive analytics to extract  actionable insights from their massive daily-business activity  data. It was decided to share some of the metrics with the  business PMI on the basis of day to day buisness process  related data. An workspace using Power BI would be  exposed with a list of KPIs using some standard visual  representation in a monthly dashboard. Metrics like Incident  Reopening Percentage for several assignment groups , SLA  breach count for speific validated systems etc. It would create  a transparent, easy mode of representation of those highly  important business KPIs.       PMI Warehouse acquires data from the various data sources of  the company and insert or update data into Data warehouse.  This data warehouse allows the Management for effective  decision-making and to transform product centric approach to  customer centric approach to gain competitive advantage by  combining data from different sources.    In this project the data is being loading from a different source  system like SAP. Then transformed into the data warehouse  and is used to generate cubes. This process will be done by  using SQL Server Analytical Services.      From Jan 2020 To   May 2020  Summary  Roles  As  Data Solutions  Architect   (Microsoft Azure)  Microsoft Azure: Designing an  Azure Data Solution (AZ-204)   Microsoft Analyzing and Visualizing  Data with Power BI (70-778)   AWS Certified Developers Associate     Microsoft Azure: Implementing an  Azure Data Solution (DP-200)   Microsoft Azure: Designing an  Azure Data Solution (DP-201)                    Then transformed into the data warehouse and is used to generate cubes. This process  will be done by using SQL Server Analytical Services.         Languages Known   SECONDARY SKILLS   MDX Query   C   C++   IBM India Private Limited  Client: Philip Morris International  Project: SAPBW Cube Implementation  Location: Kolkata, India     Used C# scripting to read files and process in  a batch,  instead of using old XSD files to read the incoming XML  files one by one. Replaced cursor based mechanism with  batch processing.    Implemented Dev-ops for DTSx packages deployment  and script execution firstly with the package development  , then by visual studio.    Implemented recommendations made by Microsoft for the  improved performance of Microsoft products in iQAR  system.   From Mar 2017 To Jan 2019  Analyzed the requirement and developed a cube which  would contain Sample-Test-Result related data of a  sample cigarette in the local plants.    Developed custom SSRS report to analyze the  performance and local plants, global plants and  production centers of various cigarettes to take a better  business decision.    Developed some custom report for a specific group of  users using excel macro and XL Cube to check the  Brand hierrachy of a cigarette and packaging items.    Designed and implemented multilingual data to some  existing conventional cubes. It would help users to get  the result and information in various languages for  different european countries.   SQL Server Profiler   IBM India Private Limited  Client: Philip Morris International  Project: iQAR KLO Project 2018 & 2017  Location: Kolkata, India and Neuchatel, Switzerland   Summary  Roles  As   Senior Data  Analyst  Data is being loaded into iQAR reporting server from various  source.Then transformed into the data warehouse and is used  to generate cubes. This process will be done by using SQL  Server Analytical Services. Business users also uses SSRS  report and XL cube report.   From Oct 2016 To Feb 2017  Python   6/1/25 Beleghata Main Road,  Kolkata, West Bengal   India-700085   ADDRESS   PMI collects data, whether it's sales figures, market  research, logistics, or transportation costs. Most of these  data are being captured in SAP BW database. Out job was  to take that data into iQAR reporting server and merge it with  the other dimensions of data to help companies make better  business decisions by figuring out how to price new  materials for the market, how to reduce transportation costs,  solve issues that cost the company money.     Summary  English   Bengali   Hindi   German                Then transformed into the data warehouse and is used to generate cubes. This process  will be done by using SQL Server Analytical Services.         I have also a significant  experience in production  support (24x7 operational  team) and maintenance   project. I am looking for a  challenging position to utilize   my knowledge of  implementing and designing  solutions in enterprise data  integration which will help   business to take necessary  steps.   Interesting Facts   Experience on Tools   Raymark POS   Team Foundation Server   VSTS   Cognizant Technology Solutions  Client: Williams Sonoma   Project: Workforce Management  Location: Kolkata, India     From Jan 2016 To Sept 2016  Our responsibility was to provide some information/feed  from our ERP system to Kronos. Those were - Sales at  point of sale (POS) for different stores, Incentive  information (Budget), Traffic footfall at different stores. To  make it happen we needed to develop six interfaces which  in turn generate some outbound files and send to Kronos  environment.    Worked on SSIS for extracting source data, transforming  and then loading into target database. And also extracting  data from legacy system and generate outbound files.    Developed store procedures, functions and different type of  queries in SQL SERVER 2008R2.     Roles  As  Senior Data  Analyst  JIRA   Roles   As   Senior ETL  Developer         Cognizant Technology Solutions  Client: Levis Strauss & Co.   Project: Reflexis Task Management  Location: Kolkata, India   Summary   From July 2015  To     Dec 2015   Designed suitable database model (Staging, ODS,   Datawarehouse layers ) for the new data coming from SAP.  Also prepared all the technical design documents (BRD,  SDD & PDD)     Development of packages according to the ETL  specifications for the staging area & Warehouse data  loading using SQL Server Integration Services & SQL  Server 2012.    Used Excel Macro funcationalities to automate all the unit  test cases and the manual processes to validate the data   William Sonoma.retail store associates in US were scheduled  manually. That was time consuming, prone to over/under  staffing and results in unplanned overtime as well as low staff  productivity. William Sonoma needed a scalable and  automated solution for scheduling the right people at the right  time in the right locations to optimize store labor and drive  customer conversion. Kronos was selected as the solution  provider and the solution would be implemented in US stores   Summary   Store audits across all regions were done manually previously.  LS&Co was planning to standardize the audit process. Every  region had their own audit process which was not streamlined.  Levis has identified a global audit & task management solution  that can streamline the process across regions.   Own Youtube Channel   Own Blog channel   https://www.youtube.com/cha nnel/UCMezjQxJUhjSmC-  tT7p2KYA    https://binaryworld- debabrataguha.blogspot.com/   Own Facebook Group  https://www.facebook.com/gro  ups/331231161093613/                  From Jan 2015  To    Jun 2015     B .Tech in Information  Technology from RCC   Institute of Technology in  2011, Kolkata with a GPA of   7.9.     Higher secondary(class Xii)  from Hindu School, Kolkata   in 2006  With an aggregate of 75.8%.     Secondary exam (Class X)  from Dr. Shyama Prasad   Mukherjee Institution, Kolkata  in 2004 with an aggregate of   88.5%.   Education   Internal Certifications   Cognizant Retail L1   IBM Big data Solutions   IBM Agile Certification   Certifications on Linux  operations   Summary   Roles  As  Senior ETL  Developer  Cognizant Technology Solutions  Client: Levis Strauss & Co.   Project: MPOS (Mobile Point Of Sales)  Location: Kolkata, India     Our responsibility was to develop Reflexis Store audit &  task management interface by which Levis can achieve a  global audit & task management solution. In order for this  application to be accessible for Store & Corporate users,  it required employee information feed from the source  and interface will provide the final output solution.    Involved in analyzing the business requirements and  various business rules for different regions and developed  SSIS package for implementing all the transformation  logics.    Developed optimized store procedures, functions and  different type of queries in SQL SERVER 2008R2 in a  way that it could give the high performance level.   Our responsibility was to develop multiple interfaces to  synchronize mPOS database (starmount posgres) and Levis  raymark database. Most of them were outbound job for which  mPOS database could get the updated data from raymark  like latest promotion details, employee details, product  details etc. 3rd party named starmount was responsible for  mPOS data and the medium was xml files. So our main  concern was to transform the data in form of XML.    Developed SSIS package for implementing all the  transformation logics and generating the XML outbound files  as MPOS system was able to take only XML inputs    Wrote XQueries to extract, transform and load the XML data.    Worked on Posgres SQL as it was the database of MPOS  system     Roles  As  ETL Developer  Springboard Phase I was already implemented in Europe.  Phase II was focused on the US market.Earlier of that project,  the US eCommerce solution was run by GSI who had ownership  of web store, order management, fulfillment, customer services,    From Feb 2014 To Dec 2014  Cognizant Technology Solutions  Client: Levis Strauss & Co.   Project: Springboard Phase 2  Location: Kolkata, India     Cognizant Retail L0   IBM certification on industry  knowledge of Consumer   Goods   LS&Co introduced a store based tablet solution, that armed the  store associate with the ability to save the sale, by fulfilling a sale  through online inventory, as well as to enable an endless  ecommerce aisle for the customers, Pay in Store & Ship to  Home capability in US Levis Store. Store Associates were  provided iPAD minis supporting this capability.     Summary             Date of Birth  : Dec 20, 1988  Sex           : Male  Marital Status : Married  Nationality : Indian  Language Known: English,  Bengali, Hindi            Hobbies : Writing poems,  painting, playing football.   Personal Information   Achievements   Got Best Non-SAP team  award from PMI business on   the month of Jan 2018   Got Master-Stroke award  from Levis business on the   month of Jan 2016   Won more than 50 awards on  painting competitions   Was convenor of RCCIIT  Student Cultural committee   payment collection and taxes.  After completion of the  project, only fulfillment remained GSIs responsibility; that  would be managed by GSIs DLx Warehouse Management  System (WMS) and integrated with LS&Co solutions through  GSIs DirectConnect Fulfillment Solution (DCFS).  The new Webstore and Order Management System (OMS),  as well as a Customer Services would be the key  components of the new eCommerce solution relevant to this  document.  These modules were built by Arvato on a Hybris  platform.Our responsibility was to build around 20 ERP  interfaces (Utilities) which can support the newly developed  eCommerece Solution - Hybris.    Worked on SSIS & SSRS for extracting source data,  transforming and then loading into target database. And also  extracting data from legacy system and generate outbound  files.    Gathered functional knowledge on ERP system of LS&Co.     Performed researches and innovation to improve the quality  of deliverables. Used excel VBA macro to make the process  fast.   Roles  As  BI Developer   Cognizant Technology Solutions  Client: Levis Strauss & Co.   Project: Retail Production Support  Location: Kolkata, India    Cognizant provides technical resources to work on the  issues on the LS&CO. They are compromised of the  following elements like Level 3 Support for Asia Region  related issue, Level 2 support for Raymark Application  regarding US and Europe region. Cognizant also handles  the issue related to the third parties like AT&T, Granaite,  Bank of America, SAP, SAP FICO, Just Enough, Pivot Link  and LDS which includes bug fixing/enhancement in  Raymark Products/ Infrastructure (Networks / Hardware   Support) / Data Processing outside the Raymark  etc.Cognizant builds necessary applications needed for  the  retail flow in Levis Strauss & CO and helps to upgrade them  as well.    Provided time saving resolutions by creating SQL queries  for file re-creation, error checking and root cause analysis of  the production issues and provided complete solution to the  client through Raymark applications which include complete  Store Operations, Customer-Centric Retailing, Planning and  Inventory Management & Enterprise Reporting and  Business Intelligence.    Provided support to Remarks E-Commerce part which is a  highly-profitable sales and interaction channel and  performed as shift leads in US support area.    Provided resolution for saving time by creation of several  production reports by automation using Excel macro and  Documented and shared the Tool respective Support  knowledge with the team.   From Sep 2011 To Jan 2014  Roles  As   Support Team  Member  Summary   Got Spark Award (USD 180)  for excellent performance in  IBM on the month of Sept   2018   Regular writer of college wall  magazine    ", "tokens": [{"text": "IBM India Private Limited", "start": 545, "end": 570, "token_start": 104, "token_end": 107, "entityLabel": "company"}, {"text": "Jun 2020 To    Jun 2021", "start": 519, "end": 542, "token_start": 96, "token_end": 101, "entityLabel": "period"}, {"text": "Microsoft Azure", "start": 5365, "end": 5380, "token_start": 1029, "token_end": 1030, "entityLabel": "company"}, {"text": "Jan 2020 To   May 2020", "start": 5274, "end": 5296, "token_start": 1006, "token_end": 1011, "entityLabel": "period"}, {"text": "IBM", "start": 3880, "end": 3883, "token_start": 750, "token_end": 750, "entityLabel": "company"}, {"text": "Feb 2019 To Dec 2019", "start": 3792, "end": 3812, "token_start": 733, "token_end": 737, "entityLabel": "period"}, {"text": "From Mar 2017 To Jan 2019", "start": 6459, "end": 6484, "token_start": 1237, "token_end": 1242, "entityLabel": "period"}, {"text": "IBM India Private Limited", "start": 7182, "end": 7207, "token_start": 1372, "token_end": 1375, "entityLabel": "company"}, {"text": "Cognizant Technology Solutions", "start": 8828, "end": 8858, "token_start": 1694, "token_end": 1696, "entityLabel": "company"}, {"text": "From Jan 2016 To Sept 2016", "start": 8946, "end": 8972, "token_start": 1714, "token_end": 1719, "entityLabel": "period"}, {"text": "Cognizant Technology Solutions", "start": 9684, "end": 9714, "token_start": 1862, "token_end": 1864, "entityLabel": "company"}, {"text": "From July 2015  To     Dec 2015", "start": 9818, "end": 9849, "token_start": 1887, "token_end": 1894, "entityLabel": "period"}, {"text": "Cognizant Technology Solutions", "start": 13772, "end": 13802, "token_start": 2626, "token_end": 2628, "entityLabel": "company"}, {"text": "From Feb 2014 To Dec 2014", "start": 13745, "end": 13770, "token_start": 2619, "token_end": 2624, "entityLabel": "period"}, {"text": "IBM", "start": 17568, "end": 17571, "token_start": 3356, "token_end": 3356, "entityLabel": "company"}, {"text": "From Sep 2011 To Jan 2014", "start": 17440, "end": 17465, "token_start": 3325, "token_end": 3330, "entityLabel": "period"}], "relations": [{"child": 3325, "head": 3356, "relationLabel": "duration"}, {"child": 2619, "head": 2626, "relationLabel": "duration"}, {"child": 1887, "head": 1862, "relationLabel": "duration"}, {"child": 1714, "head": 1694, "relationLabel": "duration"}, {"child": 1237, "head": 1372, "relationLabel": "duration"}, {"child": 1006, "head": 1029, "relationLabel": "duration"}, {"child": 733, "head": 750, "relationLabel": "duration"}, {"child": 96, "head": 104, "relationLabel": "duration"}]}, {"document": "                                                        Kamal Sharma    E- Mail: kamalsharma10f@gmail.com  Phone: +91-9004040187      Cloud Solutions Architect / Consultant     ~ Google Cloud Platform ~ Microsoft Azure ~Amazon web Services (AWS) ~   I am a Consultant with a strong knowledge in Cloud Solutions, DevOps and have gained experience in leading Clouds Google  cloud platform, Azure, AWS including rdbms, big data projects design, implementing & Processing based solutions.  I am actively seeking to leverage my Cloud expertise in the areas of Healthcare as a Solutions Architect aligning with my  current skills.        ~ Cloud Design & Implementation                     ~ Business Driving Technology             ~ Project Management & Delivery   ~ Cross-functional Coordination                        ~ Technical Documentation                    ~ Systems Integration   ~ Manage Customer/Vendor Relationships   ~ Hands on Unix / Windows Admin   ~ Technology Architecture & Integration      IT Professional having over 11 years of working experience in Software Systems, architecture planning, design, migration,   implementation, and Infrastructure IT Operation projects.    Experience as a Cloud solution architect with a focus on planning, architecting, deploying, integration, and operating cloud-  native applications, on public and private clouds. Identification of appropriate techniques and methods using various   services like IAM, EC2, RDS, VPC, S3, EBS, ELB, Auto-scaling to develop infrastructure for highly scalable cloud applications.    Expert in the design and delivery of cost - effective, high-performance information technology infrastructures and   applications to address complex business problems by assessing current capabilities and designing future technology   architectures to achieve strategic technology plans that align with the businesss strategic initiatives.    Experience in selecting and setting up IAM on Google Cloud Platform.    Extensive qualifications in all facets of project life cycle, from the initial feasibility analysis and conceptual design through   documentation, implementation, user training and enhancement. Strong server, storage and networking background with a   proven ability to build and manage creative, highly energized, focused teams.    Leading, providing mentorship to the teams of various sizes. Dealing with cross BU/LOB teams for various project initiatives.    Architecting enterprise applications using BDAT (Business, Data, Application and Technology).    Experience in Requirement Analysis, test execution, Change Management, Defect and Incident Management.    Architecting and developing solutions on Google Cloud Platform, AWS and Azure.    Experience in working with project managers to coordinate product data deliveries with cross-functional groups, ensuring   high quality of data, and meeting schedules & budgets    An excellent communicator with impeccable coordination skills; proficiency in team management with a flexible attitude.       Oracle GoldenGate 12c Certified Implementation Specialist.   Oracle Certified Associate 11g Administrator (OCA).    Cloud Platforms: GCP, Azure and AWS.   Cloud Agnostic:  Kubernetes, Docker, Terraform.   Scripting: Unix Shell Scripting, Python   ETL: MS Excel, Data Flow, Cloud Functions   Hadoop Ecosystem: HDFS, Sqoop   Databases: Oracle, Toad Tool   Solution Design/Modeling: MS Visio, Oracle SQL Developer.   Operating Systems: UNIX/Linux (Red hat, CentOS, Ubuntu, Debian), Windows 10/7.     2009                                   Bachelors of Technology in Electronics and Communication Engineering from Kurukshetra University    CAREER OBJECTIVE    PROFILE SUMMARY    CERTIFICATIONS    TECHNICAL SKILLS    ACADEMIC DETAILS   mailto:kamalsharma10f@gmail.com ksharm49 Stamp  ksharm49 Stamp       Optum Global Solutions (UnitedHealth Group) as Data Engineering Consultant                                                      Jan17- Present  Client: OHBS Facets, GPS, Micro strategy, Compas, Locator DB, Rxbuilder, PBS, RXCMS (DC- Migration) - USA    IBM India Private Limited, Bangalore as Technical Services Specialist                                                                          Nov16-Jan17  Client: Telstra-Australia    Wipro Technologies, Mumbai as Senior Administrator                                                                                                          Jul14-Oct16  Client: CITI Bank, USA    Hughes Communications India Ltd, Gurgaon as Consultant                                                                                                 Jul09 - Jul14  Client: IMN Toshniwal, Enercon India Ltd, Banking projects     PROJECTSDETAILS    Cloud Solutions Architect/Consultant  Responsibilities:    Create IT Infrastructure HLD/LLD design documents, Build documents, BOMs.   Performed Proof of Concept (PoC) on Teradata to BigQuery Migration   Have exposure on Apache Beam to handle Batch and Streaming Data Processing and deploy the Apache Beam Pipeline in   GCP's Dataflow   Performed Proof of Concept (PoC) on Cloud Data Fusion (Data Integration tool from GCP)   Working knowledge on Data Studio, BI tool from GCP   Plan, Design, Implement & lead various data center consolidation & migration efforts.    Designing & Driving Transformation of scalable, highly available solutions with DR capability for global customers   around on premise or colo data center converged and hyper converged infra, Hybrid & public cloud platforms.   Working as Infrastructure Architect for various transformations build and migration projects.   Conduct client interactions / Presentations / Workshops with Business Owners / Stakeholders / Process Owners to get   functional and architectural requirements.   Environment Assessment, Capacity Sizing and creating specifications to prepare and implement cost effective and   precise solutions for customers   Defining capacity needs and implementation/migration plans for target IT infrastructure.   Design and implementation of virtualization platforms based on VMware, vendor products such as VRealize Automation   environments, integrating with other IT infrastructure products etc.   Work closely with Sales / Business Dev. Team for new RFPs and making customer proposals. Support re-Sales team in   RFPs, Due Diligence for transformation & migration projects.   Project Stages: Go-Live, Project Stability, Identifying Gaps, CIs, Design improvement, future expansion, Operations   Handover, Transition from Onshore to Offshore.   Environment: Google Cloud Platform     Cloud Solutions Architect  Responsibilities:    Defining our Cloud IAM policies for different GCP components   Defining and provisioning Cloud IAM policies and roles for the service accounts by different GCP components   Knowledge of compute, network, storage, design and architecture.   IaaS, SaaS, PaaS platforms for example: GCP, AWS, Azure.   Architected the Data integration (acquisition, ingestion and publication/consumption) patterns, security, naming   standards for the solution in co-operation with the ETL and BI architects.   Cost optimization on Google Cloud Platform.   Design and deployment of enterprise wide complex cloud environments on a global scale.   Ability to translate traditional IT infrastructures to global cloud deployments.   Review, analyze and evaluate the business requirements to determine and implement the most appropriate AWS   architecture solution for migration of existing web applications and deployment of DevOps applications.   Utilize AWS resources such as IAM, VPC, EC2, ELB, Route53, S3, RDS, SNS, NACL, Security groups, auto scaling etc. to   design cost effective, fault tolerant and highly available systems across multiple accounts and regions.   Design Cloud Formation templates for different environments (Dev, Stage, Prod) to automate infrastructure and create   Custom sized VPC, Subnets, and NAT to ensure successful deployment of applications.   ORGANISATIONAL EXPERIENCE       Initiating alarms in Cloud Watch service for monitoring the server's performance, CPU Utilization, disk usage etc. to take  recommended actions for better performance.    Creating tagging standards for proper identification and ownership of EC2 instances and other AWS resources.   Partner with AWS cloud solution architects, DevOps and application architect teams to implement solutions and ensure   effective integration with downstream systems.   Work with Application teams, Infrastructure team, network groups and vendors to resolve complex system issues.   Part of active Agile/Scrum environment for project executions.   Environment: Google Cloud Platform, AWS   Consultant  Responsibilities: -   Administered activities like:   Experience in Cloud Consulting -AWS    o Upgrades / Migration / patching in cluster environments / standalone ASM databases / databases from 10/11 to 12c.  o Oracle software installation, creation and maintenance of script, cloning, patching and up-gradation of database and so on   o Export and import through data pump   o Production issues DR SYNC, locking- blocking, recovery, OS network issues.  o Creating & managing the database objects and schemas, rebuilding index & patching RDBMS using opatch utility  o Installation, configuration and upgrading of Oracle RAC, data guard, GoldenGate   o Cloning & Refreshing databases from production to test, PreProd & Dev environment.  o Creating new databases from scratch as per clients demand   o Space management so as to provide the fault tolerant functionality  o Coordinating with Prod, Dev, QA and project teams in implementation of changes   o Configuration of Oracle GoldenGate and monitoring lag also troubleshooting with abended process for extract, pump & replicat      Key Result Areas:   Configuring of Oracle GoldenGate and monitoring lag also troubleshooting with abended GoldenGate process.   Monitoring and optimizing the database performances through Database Tuning, System Tuning and Application Tuning   using Stats pack, AWR, ADDM, Explain Plan, and TKPROF.    Patching, cloning, Enterprise Manager Cloud / Grid Control 12c, security, RMAN backup and recovery procedures and Very   large and highly transactional Oracle databases.    Engaging in activities like:  o Installation, configuration and upgrading of Oracle RAC, Data guard, GoldenGate  o Cloning & Refreshing databases from production to Test, PreProd & Dev environment  o Creating new databases from scratch as per clients demand  o Space management to provide the fault tolerant 24*7*365 functionality   o Experience in migration & upgrade between Oracle Databases and applying patches.    ", "tokens": [{"text": "Optum Global Solutions", "start": 3835, "end": 3857, "token_start": 633, "token_end": 635, "entityLabel": "company"}, {"text": "Jan17- Present", "start": 3963, "end": 3977, "token_start": 645, "token_end": 646, "entityLabel": "period"}, {"text": "IBM India Private Limited", "start": 4088, "end": 4113, "token_start": 675, "token_end": 678, "entityLabel": "company"}, {"text": "Nov16-Jan17", "start": 4231, "end": 4242, "token_start": 686, "token_end": 686, "entityLabel": "period"}, {"text": "Wipro", "start": 4273, "end": 4278, "token_start": 694, "token_end": 694, "entityLabel": "company"}, {"text": "Jul14-Oct16", "start": 4429, "end": 4440, "token_start": 702, "token_end": 702, "entityLabel": "period"}, {"text": "Hughes Communications India Ltd", "start": 4468, "end": 4499, "token_start": 711, "token_end": 714, "entityLabel": "company"}, {"text": "Jul09 - Jul14", "start": 4619, "end": 4632, "token_start": 720, "token_end": 722, "entityLabel": "period"}], "relations": [{"child": 645, "head": 633, "relationLabel": "duration"}, {"child": 686, "head": 675, "relationLabel": "duration"}, {"child": 702, "head": 694, "relationLabel": "duration"}, {"child": 720, "head": 711, "relationLabel": "duration"}]}, {"document": "                         Karthick_cv (1)   Karthick  Email: rkarthick2006@gmail.com  Ph: 88705 30131, 99629 49400  Linkedin: https://www.linkedin.com/in/rkarthick2006/  Experience Summary  I have about 9.6 years of experience in Data Analytics using Talend ETL and Java development and handling Big Data. I have worked in diverse domains such as Banking, Healthcare and logistics in consulting and Data warehouse implementations engagements.  I am currently working with CITI India, Chennai as a Senior Consultant specializing in Talend Data Integration (ETL).  I have good knowledge in Talend Data Integration, Shell Scripting, Java, Scala, AWS SageMaker, AWS Analytics (S3, Athena, Glue, Redshift, SQS, EMR) and Big Data ecosystem (Apache Spark Batch processing, SparkSQL, Hive, Sqoop, Oozie). Also I have good exposure to real time message streaming using Kafka, NoSQL(MongoDB) and deployment/monitoring tools (Talend Administration, Autosys, IBM Urban deploy, Talend AMC, Grafana) and Teamcity for build management and Idera ER studio for Data Modelling.  I have engaged in Data Analysis, Data Profiling, Data Modeling, Design, Configuration, and Data Mapping for ETL Transformation using Talend and Loading in an extremely complex environment processing large volumes of data.  Highlights of my experience are the following:  Proficient in object-oriented design, data structures, problem solving, complexity analysis and  debugging.  Strong ETL Experience in Design, Configuration, Data Mapping, Extraction, Transformation and  Loading in an extremely complex environment processing large volumes of data.  Very good working knowledge of Java/J2EE related technologies.  Working knowledge of Agile methodology, XML-RPC, JSON, SOAP and Restful Web Services  Skills in maximizing the high-performance capabilities and parallel execution of processes  provided by Talend functionality.  Excellent written and verbal communication skill  Skills in maximizing the high-performance capabilities and parallel execution of processes  provided by Talend functionality  Handling Big Data (HDFS, Hive, Sqoop, Spark)  Short Introduction  I graduated from Kovai Kalaimagal College, Coimbatore with M.Sc. (Software Systems).  I have worked in the latest versions of Talend integration and Administration activities. I have much experience in creating logical and innovative solutions to complex problems.  mailto:rkarthick2006@gmail.com https://www.linkedin.com/in/rkarthick2006/   Education  Title of the Degree with Branch  College/University Year of Passing  Master of Science in Software Systems  HSC(+2)  SSLC(10)  Kovai Kalaimagal Institute / Bharathiar University  North Coimbatore Corp School, Coimbatore  Y.W.C.A Matric Higher Sec school, Coimbatore  2010  2005  2003  Technical Skills  Language & Tools Core Java, Scala, Talend Enterprise Data Integration, Talend Administration, Amazon AWS, Hadoop Ecosystem (HDFS, Hive, Spark RDD / SparkSQL, SparkStreaming), Sqoop, Kafka, Oozie, Eclipse, Maven, Visual SVN Server, GitHub, BitBucket, TeamCity, IBM Udeploy, Autosys, Idera ER Studio  Databases MySql, Amazon Redshift, Oracle 11g  Scripting Languages JavaScript, HTML, XSLT  Servers Apache Tomcat  Others XML, JSON, EDI, HL7  Relevant Project Experience  Organization:   CITI Corp India (2019 Sep - till date)  Project #1  Title Build Factory Operating System Unix Tools Talend Data Integration 6.5, Teamcity, Udeploy, Autosys Language Core java Team Size 6 Role Senior Developer    Project Objective To understand the custom ETL requirements from different teams for the proprietary reconciliation tool and build an etl solution for it.  Role and Responsibilities  Working as a primary Data integrator/ETL Developer under Capital Market Operation Technology  Team.  Responsible for end-to-end of feed on boarding, all feed level changes for various products (FX,  Future, Fixed Income, EQMO (Equity Market),ETD(Future & Options) GFS (Global Fund Services)   Worked in reviewing Data contract for project and providing estimations  Worked in Teamcity, Udeploy deployment tools and Autosys scheduling  Handling unstructured, semi-structured data formats  Organization:   AstraZeneca India Pvt Ltd (2016 Sep - 2019 Sep)  Project #1  Title Global Medical Affairs Application Operating System Windows & Unix Tools Talend Data Integration 6.1, AWS EMR Language Core java Team Size 4 Role Developer Period Apr 2017  Sep 2019  Project Objective Global Medical Analytics Program looks to measure performance and address areas of risk by developing global KPIs and KRIs. The Global Medical Analytics Program will build a data mart platform to store data from a wide range of transactional systems utilized by Medical Affairs teams at a global and market level. Data mart on this platform will be used to establish a global benchmark for the performance and risk of Medical Affairs activities.  The GMA Analytics consists of the following components  GMA Data Lake where data is stored as flat files from the source systems. The data is then fed  into the GMA ODS  GMA ODS holds the transformed data from the GMA data lake  A data integration platform that transforms and loads data from the GMA data lake into the GMA  ODS. The data from here is then fed into the Data Mart.  GMA Data mart is a star schema based data mart for the use by the analytics dashboard  An analytics dashboard that is accessible to users to view different graphical and data reports.  The source for the dashboard is the Data Mart    Project Description Worked as a SME and Talend developer  Role and Responsibilities  Developed in Talend Data Integration 6.1 in Amazon cloud  Worked on Agile methodology to meet timelines  Worked in reviewing Data contract for project and providing estimations  Worked in Amazon Redshift, Amazon S3, Amazon SQS integrations  Consuming data from the Salesforce CRM and Amazon RDS MySQL  Mapping and transformation using Talend built-in components and User components  Publishing the data to Amazon Redshift  Deploying and Scheduling the jobs in TAC server  Unit testing, Job issue tracking and performance tuning.  Worked in support/resolving SNOW tickets raised by business  Project #2  Title Compliance Analytics Application Operating System Windows & Unix Tools Talend Data Integration 6.1 Language Core java Team Size 6 Role Developer Period Sep 2016  Mar 2017  Project Objective Compliance Analytics project to provide an analytics platform for Global Compliance and Internal Audit Services team. These reports highlight any key risk areas for compliance and internal audit with a view to highlight any anti-bribery and anti-corruption type activities.  The Compliance Analytics project will build a data and analytics platform to store data from a wide range of transactional systems utilized by various functions at a global and market level. Data mart on this platform will be used to establish a global benchmark for the compliance risk signals.  The CA consists of the following components  Data lake where data is stored as flat files from the compliance specific source systems. The data  is then fed into the relevant functional ODSs.  Compliance ODS holds the transformed data from the data lake.  A data integration platform that transforms and loads data from the data lake into the ODS. The  data from here is then fed into the data mart.  Compliance data mart is a star schema based data mart for the use by the analytics dashboard  An analytics dashboard that is accessible to users to view different graphical and data reports.  The source for the dashboard is the data mart. The dimensions in the data mart are selected in line with the key risk signals identified by the business    Project Description Worked as a SME and Talend developer  Role and Responsibilities  Developed in Talend Data Integration 6.1 in Amazon cloud  Worked on Agile methodology to meet timelines  Engaging with Business Analyst and Data Modeler to understand Business requirement and  review the Logical & Physical model for the project.  Worked in Restful API call from Salesforce reports and tuning the API performance in Talend  jobs.  Worked in job scheduling, Execution plan of Talend jobs in TAC and Monitoring the DB and job  server using Grafana tool.  Performance tuning and creating re-usable job pattern for loading high volume jobs and low  volume jobs  Worked in Authentication with OAuth 2.0 and JWT using Talend.  Organization:   Cognizant Technology Solutions (2015 Feb - 2016 Sep)  Project #1  Title The Enable Program (TEP) & One CRM Client Pearson Education Operating System Windows & Unix Tools Talend Open Studio for Data Integration & Informatica Data Quality Language Core java Team Size 10 Role Developer Period Jun 2015  Aug 2016  Project Objective  The Enable Program consists of Pearson Customers, transforming Pearsons Customers (Account), Customer Sites and Customer contact data from the legacy systems Vista, SAP, Oracle and TQS to the Pearsons Data Hub. The Data Hub is the central repository for all Pearsons Customer and Item data originating from the legacy systems. Once the data reaches the Pearson Hub there will be a separate set of logic which transfers and enriches the Customer data to be in the correct format for loading to the Oracle E-Business (R12) system.    OneCRM consists of UK Higher Education data from the legacy systems UKHE, Sigma, UK ELT Spreadsheet, ANZ. Once the consumption is done from the Legacy to DH then need to publish the data to the target systems like One CRM and R12 as per the requirements.  Project Description Worked as a Talend developer and Informatica Data Quality support  Role and Responsibilities  Developed in Talend open studio  Worked on Agile methodology to meet timelines  Consuming data from the legacy sources which is in flat file, Oracle, Salesforce  Mapping and transformation using Talend built-in components and User components  Publishing the data from Oracle Data Hub to Salesforce  Deploying and Scheduling the jobs in Unix server  Unit testing, Job issue tracking and performance tuning.  Organization:   CG-VAK Software & Exports Ltd (2011 Sep - 2015 Feb)  Project #1  Title IRMS 360 Integration Client UPP Technology Inc Operating System Windows Tools Talend Enterprise Studio for Data Integration Language Core java Team Size 2 Role Developer Period Mar 2012  Feb 2015  Project Objective  The irms|360 Integration Layer consists of two components that require constant communication between each other and other entities like the back end database. The first layer is the Talend layer which handle file manipulation, formatting, reading and validations. Additionally, Talend will handle communication of data to and from the integration service. The integration service run as a Windows service but acts as a web service taking advantage of HTTP protocols. Each component runs inside of its own environment but both can exist on the same machine or separate machines.  The irms|360 Integration products consist of Warehouse Management, Inbound, Outbound, Order Management, Inventory Management and Healthcare. Purpose of this platform is to provide Core Functions, understanding, incorporating, managing, monitoring and improving the process from Inbound to Outbound and everything in between as well as commanding information with Labor Management and reporting. The system support for EDI, HL7, Postional & Delimited files.    Role and Responsibilities   Developed in Talend Studio IDE using Core Java with web services.  Reading, parsing, formatting, manipulating and generating XML. JSON, EDI, HL7, Excel,  Postional and Delimited files  Handling different EDI Types (856, 940, 850, 846, 888, 940 and 945) and HL7 format (2.2, 2.3,  2.5).  Worked on Agile methodology to meet timelines.  Creating custom components for Talend which perform a specific task and re-using those  components for all other jobs.  Strong experience with client interaction, understanding business applications, business data  flow and data relations   Involved on designing Business models.  Involved in installation, deployment and maintenance of application on client environment.  Documenting features, technical specifications & infrastructure requirements.  Talend job issue tracking and performance tuning.  Organization:   CG-VAK Software & Exports Ltd  Project #3  Title Payment and Shipment Integration Client Site Alive Tools Eclipse Juno Framework Spring MVC Team Size 1 Role Developer Period Sep 2011 to Mar 2012  Project Objective Integrating Authorize.net, Paypal and UPS Shipping into the existing Web application.  Project Description Worked as a Java developer.  Role and Responsibilities   Developed in Eclipse IDE using Spring MVC with Ajax.  Integrated UPS Shipping, Rating, Time-in-Transit and Tracking using SOAP API.  Integrated Authorize.net payment gateway using SIM method (Server Integration Method).  Integrated Paypal payment gateway using REST API.  Handling exceptions and providing user friendly error message.   ", "tokens": [{"text": "CITI Corp", "start": 3273, "end": 3282, "token_start": 594, "token_end": 595, "entityLabel": "company"}, {"text": "2019 Sep - till date", "start": 3290, "end": 3310, "token_start": 598, "token_end": 602, "entityLabel": "period"}, {"text": "AstraZeneca India Pvt Ltd", "start": 4172, "end": 4197, "token_start": 759, "token_end": 762, "entityLabel": "company"}, {"text": "2016 Sep - 2019 Sep", "start": 4199, "end": 4218, "token_start": 764, "token_end": 768, "entityLabel": "period"}, {"text": "Cognizant Technology Solutions", "start": 8446, "end": 8476, "token_start": 1542, "token_end": 1544, "entityLabel": "company"}, {"text": "2015 Feb - 2016 Sep", "start": 8478, "end": 8497, "token_start": 1546, "token_end": 1550, "entityLabel": "period"}, {"text": "CG-VAK Software & Exports Ltd", "start": 10101, "end": 10130, "token_start": 1849, "token_end": 1855, "entityLabel": "company"}, {"text": "2011 Sep - 2015 Feb", "start": 10132, "end": 10151, "token_start": 1857, "token_end": 1861, "entityLabel": "period"}], "relations": [{"child": 1857, "head": 1849, "relationLabel": "duration"}, {"child": 1546, "head": 1542, "relationLabel": "duration"}, {"child": 764, "head": 759, "relationLabel": "duration"}, {"child": 598, "head": 594, "relationLabel": "duration"}]}, {"document": "+91 7259215566    KAVITHA S                                 kavitha.ram2510@gmail.com                       OBJECTIVE   To pursue a challenging and exciting career in the field of Business Intelligence, thereby acquiring knowledge and work towards organizational and personal growth   SOFTWARE EXPOSURE    Power BI Experience  A Microsoft Certified Technology Specialist with 10+ Years of experience involving various aspects of Power BI. Well versed with complete BI Development Life Cycle process including requirement analysis, documentation, development, testing, implementation and maintenance.  Deep knowledge and ability to use and explain all aspects of, relational database design, multidimensional database design, OLTP, OLAP, KPIs, Scorecards, and Dashboards.  Hands-on experience with scripting, data source integration and advanced visual development in Power BI, good skill in writing Complex DAX power Queries, data security and encryption techniques.  Experience working with Redshift cluster and data modeling aspects and good understanding of Azure Cloud Architecture (AWS).    Business Intelligence Knowledge   9+ years of strong experience in Data Warehousing and Data Mining along with Reporting solutions. Designing and Developing Integration in SSIS, Reporting in SSRS and Cube Development and Maintenance in SSAS.  Involved in Creating complicated MDX Queries. Strong experience in building OLAP Cubes by using SQL SSAS and SSIS, worked on Performance tuning of SSAS Cubes using Analysis Management Object (AMO).  Strong understanding of designing dimensional and relational models and different DW design paradigms, also Knowledge in designing and implementing relational database model as per business needs.  Experience in creating, populating and maintaining data marts. Thorough knowledge of Features, Structure, Attributes, Hierarchies, Star and Snow Flake Schemas of Data Marts.  Strong Experience in Optimizing Technique of SQL Objects, SPROCs, UDFs and SQL Queries.  Good Programming knowledge on Python and C# to implement BI logics and to run automation testing.      Project Management & Team Building  Can understand existing frameworks, analyze further for investigating possibilities for replacing to new framework / architecture, redesign, work on POC and final migration, with testing and feature suggestions.  Highly motivated, quick learner and Exceptional ability to quickly master new concepts and technologies, also can mentor / train existing team members on aspects of architecture & design.  Having experiences in Agile/scrum project development methodologies.  Have extensive & effective experience of collaborating with clients/customers from diverse geographies.      CERTIFICATION  Microsoft Certified Technology Specialist in SQL Server database development and Maintenance   TOTAL EXPERIENCE  10+ Years   TECHNICAL SKILLS   OS    Windows 10/7/2008, Linux Ubuntu/RedHat   Database    SQL Server 2018/2016, MySQL, Oracle    BI Services    POWER BI, SQL Server Integration Services, SQL Server    Analysis Services, SQL Server Reporting Services, MDX,    RedShift, DAX, Azure DW, Amazon AWS   Languages & Tools    C++, Python, SQL Profiler, Visual Studio Debugger,      Excel, Word, PowerPoint   Scripting     HTML, XML, UML   .NET Technologies    C#, ASP.NET, ADO.NET    EXPERIENCE SUMMARY      SL NO  ORGANIZATION  DURATION  DESIGNATION   1     Marlabs Software  2017 Feb  Till date  Technical Lead  2  IBM India Private Ltd      2015 Jan  2017 Feb  Application Developer  3  NTT DATA, Bangalore   2012 Apr 2015 Jan  System Analyst  4  Aprameyah Technologies Private Limited Bangalore   2009 Feb2012 Mar  Software Engineer  EDUCATION  Master of Computer Applications (MCA) from MK University, Madurai (First Class with Distinction  81%)                  B.Sc. (Physics) from MK University, Madurai (First Class - 87%)   H.S.C   from State Board (First Class- 81%)   S.S.L.C from State Board (85% of marks)  PROJECTS  Project Name : IFS- Business Intelligence   Employer         : MARLABS   Duration  : Feb 2017 to till date   Role               : Technical Lead    Gate Gourmet is a leading global provider of a full range of catering services for todays airline industry.Gate Gourmet also provides food and beverages for airport lounges that are managed directly by customers or through a partnership with our colleagues. Gate Gourmetsupport menu development and delivery of a wide array of hot and cold buffet meals including breakfasts, lunches, dinners and afternoon tea throughout the day. They focus in  Logistics   Business Intelligence  IT Sourcing   Customer Services   Responsibilities:  Leading the team members to deliver the modules in specified time.  Involved in design & code review functions for the development Modules.  Regularly interact with the product owner to ensure clarity of the problem and elicit business requirements.  Analyzing, developing multidimensional and tabular model cube using SSAS.  Developed Dynamic cube processing through Analysis Management Objects to improve the performance.  Implemented POWER BI KPIs, Scorecards, and Dashboards  Also used Redshift with Azure cloud Architecture Data loading and extraction through data warehouse services.    Project Name : HM  Logistics Online    Employer          : IBM    Duration  : Jan 2015 to Feb 2017   Role                    : Application Developer            H & M is a Retail client. Report Management have different business areas for H&M. Each business area has their own reporting system. Different business areas within RM are:  Logistics   IT Governance  Production Office  Global Security  IT Sourcing   Customer Services   This reporting project was mainly to enable the business professionals to access the information which describes the enterprise, to analyze it to gain insight into its workings. We have two BI solutions called RM (report management) and LDW.  Both of these applications performs data-warehousing operations, analyzes the data and displays it in the form of SSRS and reports. This project is a staff Augmentation project where in one has to work closely with the client and Business users. There are different areas (Functional & Technical) where applications are built on using varied technologies based on the business requirement. I am responsible for developing and maintaining new applications on SQL Server 2012, SSIS, SSRS, SSAS, MDX, MDS and Excel 2010 based reports..  Responsibilities:  Understanding user requirements and translating those requirements into well designed solutions.   Initiated daily scrum calls.  Involved in Sprint Backlog meetings.  Perform design & code review functions for the development Modules.  Regularly interact with business partners of to ensure clarity of the problem and elicit business requirements.  Estimate the size of the effort required to complete complex enterprise Modules.  Involved in analyzing, researching, processing and administering data.  Analyzing, developing and deploying reports.  Reviewed requirements for multiple modules.  Ensured that documentation is consistent and falls within the standards of the solution delivery methodology.    Project Name : HLL - SPS & HLL OLAP    Employer            : NTTDATA  Duration  : May 2012 to Jan 2015 Role   : System Analyst  Responsibilities:  Importing Source/Target tables from the respective databases by using Execute Package Task using Control Tasks in SQL Server 2005 Integration services.  Used Control Flow Tasks like For Loop Container, For Each Loop Container, Sequential Container, Execute SQL Task, Email Task, and Data Flow Task.  Scheduled the ETL Package (Monthly) Using SQL Server 2005 Management Studio.  Adding parameters and filters while building advanced reports.  Deployed and generated reports on to the server using the Reporting Services Report Manager.  As per the requirements, rendering the reports in the form of PDF, XML and CSV formats.  Developed OLAP Cubes by using SQL Server Analysis Services (SSAS), worked on Performance tuning of SSAS Cubes.  Releasing the Reports and Packages Weekly on basis as per the Client Requirements  Hindustan Unilever Limited (HUL) is India's largest Fast Moving Consumer Goods Company. Leadership in Home & Personal Care Products and Foods & Beverages. HLL is built to analyze the Customer profitability. All the Sales data is extracted from different source system and consolidate under HLL Data warehouse. On top of warehouse, SSAS Databases build so that users can slice and dice on the cubes.     PERSONAL DETAILS    Sex : Female    Nationality : Indian     Marital Status   : Married      S. Kavitha", "tokens": [{"text": "Marlabs Software", "start": 3399, "end": 3415, "token_start": 575, "token_end": 576, "entityLabel": "company"}, {"text": "2017 Feb  Till date", "start": 3417, "end": 3436, "token_start": 578, "token_end": 582, "entityLabel": "period"}, {"text": "  IBM India Private Ltd", "start": 3455, "end": 3478, "token_start": 587, "token_end": 592, "entityLabel": "company"}, {"text": "2015 Jan  2017 Feb", "start": 3484, "end": 3502, "token_start": 594, "token_end": 598, "entityLabel": "period"}, {"text": "NTT DATA", "start": 3530, "end": 3538, "token_start": 605, "token_end": 606, "entityLabel": "company"}, {"text": "2012 Apr 2015 Jan", "start": 3552, "end": 3569, "token_start": 610, "token_end": 613, "entityLabel": "period"}, {"text": "Aprameyah Technologies Private Limited", "start": 3590, "end": 3628, "token_start": 620, "token_end": 623, "entityLabel": "company"}, {"text": "2009 Feb2012 Mar", "start": 3641, "end": 3657, "token_start": 626, "token_end": 628, "entityLabel": "period"}], "relations": [{"child": 578, "head": 575, "relationLabel": "duration"}, {"child": 594, "head": 587, "relationLabel": "duration"}, {"child": 610, "head": 605, "relationLabel": "duration"}, {"child": 626, "head": 620, "relationLabel": "duration"}]}, {"document": "                                          Microsoft Word - CV_Kesavan_2020_Nov.docx                         Profile Summary   Achievement-driven professional with around 19 years of total IT experience    16+ years experience in Data Analytics (DW/BI)   Cross-functional experience in Software Programming, Project / Program Management, Delivery Management, Client   Relationship Management, Crisis Management, Portfolio Management, People Management, Pre-Sales & Business  Development    Experience in consulting for multiple domains / sectors including Hospitality, Life Insurance, Utility, Banking, FMCG,  Automotive, Automotive Ancillary, Telecommunication, Logistics, IT Services & Payments    Experience in managing teams of more than 100 people across locations   Experience in managing application development as well as support projects   Certified in MSP and Lean Six Sigma Green Belt   Alumnus of iNautix Technologies, PricewaterhouseCoopers, HCL, Cognizant, Western Union            Technical Competence Summary    Data Warehousing (ETL) -  Strategy, Dimensional Modeling & end-to-end implementation   Data Visualization  BI Reports & Dashboards   Experience in delivering/managing projects using tools such as Oracle Warehouse Builder, Business Objects, Pentaho,   Tableau, SAS Demand Forecasting   Knowledge and hands-on experience in Machine Learning & Artificial Intelligence using Python    Experience in delivering academic projects using EDA, Linear Regression, Logistic Regression, Classification,  Clustering, Decision Trees, Ensemble techniques, Principal Component Analysis, Support Vector Machines, Artificial  Neural Network, Convolutional Neural Network, Deep Learning, Statistical NLP & Sequential NLP    Certification in Basics of Big Data & Hadoop          Career Timeline (Recent 3)  2014 to 2016   Associate Director  @PricewaterhouseCoopers India (P) Ltd      2016 to 2019  Portfolio Manager  Solution   Engineering   @Western Union Technology Engineering   Services (P) Ltd   2020 till date  Delivery Head    @Kritilabs Technologies (P) Ltd      Nature of Responsibility:  Delivery Management and Business   Development for South India   Nature of Responsibility:  Delivery Management for Compliance Portfolio   Nature of Responsibility:  Program Management for pan-India   implementation of IoT solution for a prestigious  client in India        Academic Credentials    2019-20: Post Graduate Program in Artificial Intelligence & Machine Learning  offered jointly by Great Lakes, Chennai and University of Texas   @ Austin   2008: Post-graduate Diploma in Customer Relationship Management from ICFAI University   2004:  Master of Business Management from  Vinod Gupta School of Management, IIT Kharagpur   o Exchange student at Peter Kiewit Institute of Technology @University of Nebraska, Omaha (2003)    1998:  B.Tech. (Computer Science and Engineering) from College of Engineering, Trivandrum, Kerala University      KESAVAN HARIHARASUBRAMANIAN  B.Tech, MBM, PGDCRM, PGP-AIML                                        kesavan_h@yahoo.com | kesavan.hari@gmail.com            +91 9884025720      ", "tokens": [{"text": "PricewaterhouseCoopers India (P) Ltd", "start": 1850, "end": 1886, "token_start": 301, "token_end": 306, "entityLabel": "company"}, {"text": "2014 to 2016", "start": 1814, "end": 1826, "token_start": 294, "token_end": 296, "entityLabel": "period"}, {"text": "Western Union Technology Engineering   Services (P) Ltd", "start": 1951, "end": 2006, "token_start": 319, "token_end": 328, "entityLabel": "company"}, {"text": "2016 to 2019", "start": 1892, "end": 1904, "token_start": 308, "token_end": 310, "entityLabel": "period"}, {"text": "Kritilabs Technologies (P) Ltd", "start": 2043, "end": 2073, "token_start": 337, "token_end": 342, "entityLabel": "company"}, {"text": "2020 till date", "start": 2009, "end": 2023, "token_start": 330, "token_end": 332, "entityLabel": "period"}], "relations": [{"child": 294, "head": 301, "relationLabel": "duration"}, {"child": 308, "head": 319, "relationLabel": "duration"}, {"child": 330, "head": 337, "relationLabel": "duration"}]}, {"document": "Krishnaveni Reddy  Mob: 9500064765  Email: venisri02@gmail.com      PROFILE    5.6 years of overall experience with strong emphasis on design, development, Implementation, Testing and Deployment of Software applications in Big data, Oracle, UNIX, Informatica, RDBMS, Unix SHELL Scripting extensive development experience     PROFESSIONAL SUMMARY  Having the experience on the development and maintenance of the Hadoop eco systems  Exploring Hadoop Applicationsand recommending the right solutions and technologies for the applications.  Having good expertise on Hadoop tools like Hive, Sqoop, Scala, Spark, Autosys, Cron Tab.   Knowledge on Spark framework for batch and real-time data processing.  Good Knowledge on Scala Programming Language.  Having strong knowledge on Agile and Water fall model and involving sprint planning.  Capable of processing large sets of structured, semi-structured and unstructured data and supporting systems application architecture  Handled the TEXT, JSON, XML using Hive(SERDE) & Spark-Scala  Familiar with data architecture including data ingestion pipeline design, Hadoop information architecture, data modeling and data mining and advanced data processing.  Good Communication and Interpersonal skills. Technically sound, Result-Oriented with strong Problem-solving skills. Innovative & efficient. Capable of working as a Team Member or Individually with minimum supervision.  Flexible and versatile to adapt to any new environment with a strong desire to keep pace with latest technologies.  Closely worked with UI team for preparing Dash board reports & segments TECHNICAL.  Working Experience in Informatica, SQL, Unix, Shell scripting, Oracle, Tera data, Solar. SKILLS:    Qualifications:  Degree     Institute    Major and Specialization  B. Tech    JNTU University    Information Technology (IT)    Professional Experience   Working   as a Bigdata Developer at Cognizant OCT 2015 to Tilldate.        Projects & Assignments:  Project 2:    Name  AbbVie     Client  AbbVie    Period  Jan 2017 to till date    Description    Comparison of  MDM golden records to the NDB,MOCA records to be as dashboard for showcase of the mismatch  no.of records, type of data elements, etc., The database is intended to used by business teams  HUMIRA, IT teams and UAT teams. The dashboard will ease the identification and review of the gaps in the matching and information would be used by the connected team to rectify the errors/mismatched records.      Role  Bigdata Developer.    Environment  Scala, Spark, HDFS, Hive, Unix Shell Script, AWS  EMR,S3,Step Functions, Terraform Script, Jenkins, Git Hub    Name  Cognizant     Responsibilities  Analyze and identify the resources (NDB, EPIM, NICE) to extract information for selected data elements.  Reading the json file generated by the EPIM for the input data elements.  Converting the JSON file data from EPIM into data frames, so the records can be easily compared at data element level  Comparing the EPIM and NDB records and loading the result data to ES.  Creation of the Step functions for Spinning up the cluster (EMR), Execution of Multiple Spark Jobs and Terminating the Cluster.  Creation of the Terraform scripts by calling the Step Functions with it.  Updating of repository for building the Jenkins Pipeline.  Provided design recommendations and thought leadership that improved User recommendation engine and resolved technical problems.  Used different Kinds of Wide-Narrow transformations and Actions on Data Frames using Spark Scala.  Written Shell Scripts for Doing the Secure Copy of Json Data Logs from Application Server to Big Data Servers.  Used Spark andSpark-SQLto read the Json data and create the tables in hive using the Scala API.               Project 1:    Name  KBC Bank     Client  KBC    Period  Oct 2015 to 2016 December    Description    KBC Bank is the largest foreign exchange dealer in the world with a market share of 21 percent. KBC Bank is a leading provider of financial services to agencies, corporations, governments    Role  ETL Developer,     Name  Cognizant     Responsibilities   Mainly involved in ETL developing Creating the mappings and transformations using Informatica.   The data from the staging area was transformed and loaded to the target On Oracle using Informatica Tool.   Creating validation and loading the rejected records into the relevant tables and send to the users for re-submitting.   Involved in Enhancement and discretionary work.  Analyzing and understanding the business requirements and providing the solutions.   Create unit test and integration test scripts and test results documents.   Perform data validation tasks after data load and identify the data mismatches.   Review the ETL design, code and test scripts and provide comments.   Preparing unit test cases and documentation.", "tokens": [{"text": "Cognizant", "start": 1905, "end": 1914, "token_start": 330, "token_end": 330, "entityLabel": "company"}, {"text": "OCT 2015 to Tilldate", "start": 1915, "end": 1935, "token_start": 331, "token_end": 334, "entityLabel": "period"}], "relations": [{"child": 331, "head": 330, "relationLabel": "duration"}]}, {"document": "Lakshmanan Raghunathan    Personal Information   Contact Information    https://www.linkedin.com/in/lakshmanan-raghunathan  Date of birth: 20/08/1991   Mobile: +91 9790349945      Country of birth: India   laksh1991raghu@gmail.com      Nationality: Indian            Education      VIT University  Vellore, India  Btech in Electrical Engineering   CGPA  7.3/10        WorkExperience      JP Morgan and Chase  Chennai, India  Industry: Banking  Jul  2013  Aug  2016  Job position: Developer  Development  Understanding business requirements, according to that we will edit the Unix shell script  Unit Testing  testing the edited Unix shell script with multiple test case scenarios  Deployment  Deploy the Unix script to the PROD env with proper testing  User Acceptance Testing - Understanding and analyzing UAT defects and providing solution.      Standard Chartered  Chennai, India  Industry: Banking  Aug 2016  Present  Job position: Technical Lead  Experience in analyzing the VB batch scripts to understand the flow of the project  Developed the Unix scripts for the password vaulting (high privileged ID)  Experience in creating the partition like transparent, Replicate etc.  Has experience in writing the Maxl scripts to automate the repetitive process like level 0 data export and import database etc...  Developed new Business rules and rule sets for the business requirement to pre-populate the data for FC1.  Experience in analyzing the Jython scripts to understand the flow of the FDMEE  Experience in creating the job in the explore for running the unix shell scripts which in turns trigger the FDMEE process  Created the FDMEE location and DLR for the new business requirements  Has experience in changing the hierarchy members (Addition of new members and deletion) in planning and essbase.  Has experience in taking the LCM exports and importing the LCM back to the applications  Developed creating the infrastructure for installing the ODI Studio.  Experience in converting the informatica mappings into ODI packages and procedures with the oracle as a DB  Removed the Obsolete component (Control Center Mappings which was in windows XP) with the FDMEE component for mapping changes.  Has minimal Experience in converting on prem application into AWS Cloud platform like Redshift  Machine Learning - Linear Regression, Logistic Regression/Classification  Basic Neural Networks  Convolutional Neural Networks (CNN)  Completed POC for separating the outliers from the T&E and SCM data by using the statistical analysis (Robust scaler and Z- score method) Supervised learning       Technical Skills      Data warehousing ETL tools (Informatica,Abinitio,ODI)  Databases (MYSQl,Oracle,Teradata)  Cloud Deployments (Airflow,Jenkins,AWS EC2 instance,Redshift,DynamoDB,Amazon S3)  OLAP Databases (Hyperion Essbase,Planning,HPCM,FDMEE)  Scheduling tools (Control M  and TWS)  Programming languages (Unix Shell scripting, Python)        Language Skills      English(Fluent), Tamil (Native), Hindi(Intermediate)      Additional Information      Active Stock Market Investor  Active Member of an NGO called Connect for a Cause, aiding women and children of the rural parts of India.  Represented college and high school in Table Tennis and Volleyball respectively.", "tokens": [{"text": "JP Morgan and Chase", "start": 388, "end": 407, "token_start": 54, "token_end": 57, "entityLabel": "company"}, {"text": "Jul  2013  Aug  2016", "start": 444, "end": 464, "token_start": 67, "token_end": 73, "entityLabel": "period"}, {"text": "Standard Chartered", "start": 848, "end": 866, "token_start": 140, "token_end": 141, "entityLabel": "company"}, {"text": "Aug 2016  Present", "start": 903, "end": 920, "token_start": 151, "token_end": 154, "entityLabel": "period"}], "relations": [{"child": 67, "head": 54, "relationLabel": "duration"}, {"child": 151, "head": 140, "relationLabel": "duration"}]}, {"document": "Madan Kumar                                      Big Data Lead having 7.5 Years of Work Experience     pammalkmadan@gmail.com|+91 9790821782  __________________________________________________________________________________________         KAFKA| GCP| BIG QUERY|AZURE|SPARK|AWS|HIVE|PIG|SQOOP|SCALA|LINUX|RDBMS| NOSQL    Profile Summary:  Having 7.5 years of experience in IT and As a Hadoop Developer with 5+ years of IT experience in developing, delivering of software using wide variety of technologies in all phases of the development life cycle. Experience in Big data technologies as an engineer, proven ability in project-based teamwork and good communication skill    Career Highlights:    Currently Working as Software Analyst III in Walmart and previously worked as Big Data Tech Lead in Dossier Analytics and Senior Software Consultant in Team Lease for TCS and worked in Wipro Technologies as Associate Consultant     Key Responsibility Areas:     Experienced on working with Big Data, Hadoop File System (HDFS), Azure and AWS and GCP.  Hands on Experience in working with ecosystems like Kafka, Hive, Big Query, Spark, Impala, Pig and Sqoop    Good Experience in Kafka Connect and Kafka Streams  Experience in architecture of data streaming platforms like Apache Flink and Spark Streaming  Good Experience in NoSQL Databases like MongoDB, Cassandra, HBase and Cosmo DB  Strong Knowledge of Hadoop and Hive's analytical functions.  Hands on Experience on Linux system  Deep Knowledge in UC4 Scheduling and Hands on experience in UC4 development and Migration  Management of Hadoop cluster, with all included services, including resolving any ongoing issues with operating the cluster  Capturing data from existing databases that provide SQL interfaces using Sqoop.  The ability to collect and analyze information, problem-solve, make decisions and having strong analytical skill.  Experience in loading logs from multiple sources directly into HDFS using Flume.  Developed Batch Processing jobs using Spark and Hive  Good Knowledge on OLTP and OLAP Databases  PROJECTS HANDLED:   WALMART GLOBAL TECH May20- Till Date  (Big Query,Hive,Azure,GCP)  Big Data Engineer with Walmart Global Tech India working for a retail business to design and develop Applications using Big Data Technologies for the merchandise applications   Responsibilities:  Explored Dataset with SQL in Google Big Query  Explore and Create Reports with Data Studio  Creating Permanent Tables and Access-Controlled Views in Big Query  Creating a Data Warehouse through Joins and Unions  Creating Date-Partitioned Tables in Big Query  Working with JSON, Arrays, and Structs in Big Query   Troubleshooting Common SQL Errors with Big Query  DOSSIER ANALYTICS DEC18-Apr20  (KAFKA, SPARK, AZURE, AWS)   Dossier Analytics is an Indian based company for solving the real-world problems in Big Data Engineering, ETL, Cloud Data Migrations, Data Analytics and AI based enterprise challenges. It provides quality and architect-ed Big Data Solutions to the enterprises on improving their business    Responsibilities:  We work on an array of technologies includingKafka, Spark, Hive, Sqoop, Pig, Rest-API services, Mongo DB, Cassandra, HBase, AWS and Azure for the client Avik Cloud.  Installation, configuration and administration of a single-node and multi-node cluster with technologies like HDFS, Apache Spark and Apache Kafka.  Research, evaluate and utilize new technologies/tools/frameworks around Hadoop ecosystem such as Apache Spark, Apache Flink and Apache Kafka.      CIGNA MAY18  DEC18   (SPARK, HIVE)   Project description:                      Cignais an American worldwide health services organization based in suburbanBloomfield, ConnecticutandPhiladelphia, Pennsylvania.Its insurance subsidiaries are major providers of medical, dental, disability, life and accident insurance and related products and services, the majority of which are offered through employers and other groups (e.g. governmental and non-governmental organizations, unions and associations).    Responsibilities:  Involved in creating User-Defined functions, Data Transformation with custom scripts, SerDes and Indexing Data.  Developed data pipeline using SQOOP, Pig and Spark to ingest customer data, invoice, and work order data into HDFS for analysis.    Created the hive tables with the similar structure of Oracle tables and Connected to the Oracle via JDBC through Sqoop and import the tables to hive.  Performing data migration from Legacy Databases RDBMS to HDFS using SQOOP.   Worked With Hive Complex Data Types and Relational Data Analysis with Impala and Hive  Implemented Hive tables and HQL Queries for the reports. Written and used complex data type in Hive.    Storing and retrieving data using HQL in Hive. Developed Hive queries to analyzed reducer output data.     Paypal Dec17- Apr18  (Hive, Spark SQL, UC4)  Project Description:            The goal of segmentation cube is to have all possible adjacency data ( Paypal, Xoom, Venmo) so that we can a get clear picture on the customer as a PayPal entity. Segmentation lenses are categorized as Engagement, Attribute and Demographic  Engagement  Defined based on customer engagement (transaction activity) & their life cycle.  Attribute  Identifies usage preferences & lifestyle trends allows insight into use cases and marketing actions  Demographic  Defined based on customer age & income buckets.   Responsibilities  Involved in creating Hive tables, and loading and analyzing data using hive queries  Hands on experience in implementing Partitions, Bucketing on Hive tables and designed both Managed and External tables in Hive to optimize performance.  Handled Different File Formats for storing data in HDFS.  Transferred data from Hadoop to Teradata using Bridge  Worked on UC4 development, migration and scheduling for Risk, Regulatory and Consumer and GOPS domains  Experience in creating Containers, Job Plans, Jobs and Events in UC4 and successfully deployed to prod.    JPMC  Jan15- Aug17  (HDFS, Sqoop, Hive, Spark SQL)  JPMC is engaged in investment banking, financial services for consumers and small businesses commercial banking, financial transaction processing and asset management. The goal of the project is understanding the customer activities and their daily and monthly transaction activities     Responsibilities:     Developed PIG Latin scripts to extract data from source system.  Worked on a live1000 nodes Hadoop clusterrunningCDH4.4 and CDH5.7  Worked with highly structured and semi structured data.   Use the Spark shell for interactive data analysis  Worked on different file formats like text, sequence, avro and parquet file formats.  Extracted the data from Teradata into HDFS usingSqoop and Python Script.  Extensive experience in Designing, Capacity planning and cluster setup for Hadoop  Very good understanding ofPartitions, Bucketingconcepts in Hive and designed bothManaged and Externaltables in Hive to optimize performance  Develop, implement and maintain High Availability (HA) strategy.  Product knowledge on Hadoop distributions such as Cloudera and Hortonwork  Worked with Hive, Spark SQL and Sqoop.  Installation and setup of Hadoop.  Very good experience in monitoring and managing the Hadoop cluster usingCloudera Manager.  Hands on Experience on Linux system  Documentation of the day to day tasks.    JPMorgan Chase Jan14  Dec14  SQL Server/SSIS/SSAS   Project Description - JPMC banking group business people requires reports at every weekend for monitoring banking progress and growth in certain sector. For the application migrated to new version in front end.    Responsibilities:   Involved in designing, developing and deploying reports in MS SQL Server environment using SSRS-2012andSSIS in Business Intelligence Development Studio (BIDS).  Involved in creating multiple parameterized stored procedures which were used by the reports to get the data.  Used ETL (SSIS) to develop jobs for extracting, cleaning, transforming and loading data into data warehouse.  Using SSAS created OLAPcubes for data mining and created reports from OLAP cubes using SSRS.  Designed Star Schema and Snow flaked schemas in the process of data modeling  Prepared the completedata mappingfor all the migrated jobs using SSIS.  Created databases and schema objects including tables, indexes and applied constraints, connected various applications to the database and written functions, stored procedures and triggers.  Extensively used SSIS transformations such asLookup, Derived column, Data conversion, Aggregate, Conditional split, SQL task  Used Execution Plan,SQL Profiler and Database Engine Tuning Advisor to optimize queries and enhance the performance of databases.  Expert in creating Star schema cubes usingSSAS and  Extensively worked on OLAP cubes using SSAS    Scholastic Credentials:   B.Tech at Easwari Engineering College (2012) with an Aggregate of 70.5%  12th at Sri Sankara Vidyalaya Matric School (2008) with an Aggregate of 88.58%    Personal Information:   Fathers Name  : Mr. C. Kamaraj  Mothers Name : Mrs. K. Munia Puspham  Date of Birth  : 05th October 1990  Sex   : Male  Nationality  : Indian  Marital Status  : Unmarried    Declaration:    I hereby declare that the above mentioned particulars are true to the best of my knowledge.               Yours Sincerely         Madan Kumar", "tokens": [{"text": "WALMART GLOBAL TECH", "start": 2093, "end": 2112, "token_start": 427, "token_end": 429, "entityLabel": "company"}, {"text": "May20- Till Date", "start": 2113, "end": 2129, "token_start": 430, "token_end": 432, "entityLabel": "period"}, {"text": "DOSSIER ANALYTICS", "start": 2718, "end": 2735, "token_start": 542, "token_end": 543, "entityLabel": "company"}, {"text": "DEC18-Apr20", "start": 2736, "end": 2747, "token_start": 544, "token_end": 544, "entityLabel": "period"}, {"text": "CIGNA", "start": 3549, "end": 3554, "token_start": 702, "token_end": 702, "entityLabel": "company"}, {"text": "MAY18  DEC18", "start": 3555, "end": 3567, "token_start": 703, "token_end": 705, "entityLabel": "period"}, {"text": "Paypal", "start": 4842, "end": 4848, "token_start": 925, "token_end": 925, "entityLabel": "company"}, {"text": "Dec17- Apr18", "start": 4849, "end": 4861, "token_start": 926, "token_end": 927, "entityLabel": "period"}, {"text": "JPMC", "start": 6008, "end": 6012, "token_start": 1134, "token_end": 1134, "entityLabel": "company"}, {"text": "Jan15- Aug17", "start": 6014, "end": 6026, "token_start": 1136, "token_end": 1137, "entityLabel": "period"}, {"text": "JPMorgan Chase", "start": 7381, "end": 7395, "token_start": 1369, "token_end": 1370, "entityLabel": "company"}, {"text": "Jan14  Dec14", "start": 7396, "end": 7408, "token_start": 1371, "token_end": 1373, "entityLabel": "period"}], "relations": [{"child": 1371, "head": 1369, "relationLabel": "duration"}, {"child": 1136, "head": 1134, "relationLabel": "duration"}, {"child": 926, "head": 925, "relationLabel": "duration"}, {"child": 703, "head": 702, "relationLabel": "duration"}, {"child": 544, "head": 542, "relationLabel": "duration"}, {"child": 430, "head": 427, "relationLabel": "duration"}]}, {"document": "Mahathi M  mahathi06@gmail.com  Contact: +91 9849632891    JOB OBJECTIVE:  Intend to build a career with leading corporate of hi-tech environment with committed and dedicated people which will help me to explore myself fully and realize my potential. Willing to work as a key player in challenging and creative environment.  PROFESSIONAL SUMMARY:      Total 5.7 years of experience in Analysis, Design, Development, Implementation, Testing and Support of Data Warehousing using IBM Infosphere Information Server Data Stage 9.1 and 11.7 and in between worked on cloud migration project as well.    Have experience in Teradata,DB2 and Bigquery.    Have extensively worked in developing ETL program for supporting Data Extraction, transformations and loading using Data stage.    Have experience with CI/CD tools like GitHub,G3 Web,Nexus,SonarQube and Jenkins.    Have experience in data ingestion to Cloud through Juniper.    Independently perform complex troubleshooting, root-cause analysis and solution development.    Used theIBM WebSphere Data Stage designerto develop jobs for extracting, cleansing, transforming, integrating, and loading data into data warehouse.    Retrieved the source data from Teradata,Db2,BQ tables and Sequential files.    Used theData Stage Directorand its run-time engine for testing and debugging its components and monitoring the resulting executable versions.    Provided production support and performed enhancement on existing multiple projects.    Ability to meet deadlines and handle multiple tasks, decisive with strong leadership qualities, flexible in work schedules and possess good communication skills.    Team player, Motivated, able to grasp things quickly with analytical and problem solving skills.    Comprehensive technical, oral, written and communicational skills    TECHNICAL SKILL SET:  ETL: Data Stage 11.7  Database:  DB2, SQL,Teradata and Big Query  Tools: Teradata Sql Assistant,Quality Gate  Platforms:  Windows, UNIX,Python  Others: MS Word, MS Excel, Outlook, PowerPoint,Spyder  Scheduling Tool: Control-M,Juniper Scheduler    Academic Profile:   BTech from RGUKT,IIIT in year 2015 with percentage of 87.6 in Computer Science   PUC from RGUKT,IIIT in year 2011 with percentage of 95.4   SSC from Bhashyam Public School in year 2009 with percentage of 96.16        Working Experience:   Working at HSBC from Aug 2018-Till Date as Senior Software Engineer in HYDERABAD.   Worked at CAPGEMINI from Sep 2015 -Till July 2018 as Associate Consultant in HYDERABAD.    PROFESSIONAL EXPERIENCE:  The details of the various projects that I have handled are listed here.  Client    HSBC  Project 1                           CADW(CMB Analytics Data Warehouse)  Methodology                   Agile  Team Size                         6  Duration                           Jan 2021  Till now    About Client                     HSBC Bank USA, National Association, an American subsidiary of UK-based HSBC Holdings plc, is a bank with its operational head office in New York City and its nominal head office in McLean, Virginia (as designated on its charter). HSBC Bank USA, N.A. is a national bank chartered under the National Bank Act, and thus is regulated by the Office of the               Comptroller of the Currency (OCC), a part of the U.S. Department of the Treasury.    Project Description        The objective of CADW is to create a CMB Analytics Data ware house which has all the   Commercial Banking Customers.     We take all the customers from MDM which is the golden source for US and then   from that CMB customers are filtered and sent to UK server. In the UK server the data is loaded into the work        tables and then into warehouse which is used for analytical purpose. Similarly we are working on building              datawarehouses in Canada,Mexico and Argentina regions as well.    Roles & Responsibilities:      Team Member/Developer/Support    Monitoring all the jobs and providing production support to ADI UK Applications.  Using Quality Gate tool to upload the jobs into Control-M so that jobs can run automatically.  Written UNIX shell script to load the data into respective tables.  Involved in Performance Tuning of Jobs, identifying and resolving performance Issues.  Automated the process of jobs by setting up the scheduling in Control-M.    Environment: Data Stage, UNIX, SQL,Teradata, Quality Gate and Control-M.      Client   HSBC   Project 2                           TD-GCP Migration  Methodology                   Waterfall  Client      HSBC North American Holdings  Location  HSBC, Offshore  Team Size  25  Duration  Apr 2019  Dec 2020    Project Description       Common Staging is a staging area with Teradata database. Common Staging holds data                                                        retrieved from HUB as400, Legacy and Global source systems. Common Staging consists of around 500 tables    which occupies nearly 800 GB. Common staging acts as data repository for many downstream applications like RMP, CMB, HDW, CDW, CODS, Tradenet and CADW and It will also assist in evolving RBWM & CMB data and intelligence capabilities to efficiently enable business and customer experiences at scale, in business relevant timescales.  The purpose of this migration is to save Teradata lease renewal cost and future state of architecture is to run and keep data in BQ using cloud services.     Roles & Responsibilities:      Team Member/Developer/Support    Migrated few sources from 6-series to 2-series and then to Db2 and few others from 6-series to GCP.  Hands on expertise with Juniper tool to get the data from On-prem to GCS Bucket.  Worked on Python Scripts to get the data from GCS buckets to Google Biq Query Tables.  Used Control-M for scheduling the jobs on Cloud platform.  Migration of On-Prem DataStage jobs to Cloud Supported Environment using Spyder.  Hands on expertise with CI/CD tools like GitHub, Jenkins and G3 Web.  Working with JIRA to track project defects and tasks.  Using GIT source control systems to manage code.    Environment: GCP, Juniper Scheduler, Cloud Storage, Biq-Query, Spyder, Web-ui, Control-M, SDK, DataStage,                            CI/CD    Client   HSBC   Project 3                           CMS(Complaints Management System)  Methodology                   Waterfall  Team Size                          2  Duration                            Mar 2019    Project Description This project is mostly related to Customer Complaints and experience and how they get   tracked in HSBC Systems.    Data is sent from CMS Source system in the form of files and this data is loaded into the respective stage and         base tables by doing some transformations.  Written UNIX shell script to handle automation of batch.   Used CONTROL-M  to create the jobs so that jobs can run automatically in Datastage once the scheduling is         done.    Environment: Data Stage, UNIX, SQL, Control-M.    Client   HSBC   Project 4                           GLCM(Global Liquidity Cash Management)  Methodology                   Waterfall  Team Size                          2  Duration                            Jan 2019  Feb 2019    Project Description GLCM is actually product line which offers different type of products to HSBC Customers.GLCM has various departments under it like Business Management, Client Management,Client Services,Product,   Sales & RM. Here the data is received from Payments Data Lake Team and then Auto Convert, Smart Route and   Scorecard reports are generated from that data. Auto Convert and Smart Route reports give revenue and rebate   information for affiliate and third party clients. Scorecard is used for doing yearly client reviews and it gives how   a client is doing, how much volume is coming out of that client,rebates earned by that client and product used.      Roles & Responsibilities:      Team Member/Developer/Support    Developed datastage jobs to extract the data from source and to load into the target tables by doing specific         transformations.  Written UNIX shell script to handle automation of batch.   Used CONTROL-M Scheduler to schedule the DataStage jobs to run it on monthly basis as the reports need to        be generated on monthly basis.  Automated the process of data stage jobs.    Environment: Data Stage, UNIX, SQL, Control-M.      Client   HSBC   Project 5                           CMB  Methodology                   Agile  Team Size  10  Location                            Cap Gemini, Offshore  Duration  Jan 2018  Dec 2018      Project Description       CMB stands for Commercial Banking and as a part of CMB US, all CMB customers will be     loaded into a separate CMB warehouse. Data from HUB,WDA,GBS,PEP,MCC,GPS and HDW will be loaded into        Common Staging Area and then EDW tables will be loaded which are used for reporting purpose.      Roles & Responsibilities:      Team Member/Developer/Support Enhancements     Resolving abends and updating the resolution in the repository.  Meeting the SLA standards.  Executed adhoc tasks if assigned.  Worked with the Business process analyst to thoroughly understand the different business processes and       requirements.  Used Data Stage Director for running, view, monitoring and scheduling the jobs.  Extensively used DataStage Designer to design and develop jobs for extracting, cleansing, transforming,        integrating, and loading data using various stages Remove Duplicate, Surrogate Key, Aggregator, Funnel, Join      , Merge, Lookup, Change Capture, Change Apply and Copy.  Extensively worked on migrating DataStage jobs from development to test and to production environments.       Performed Unit Testing, Regression Testing, and User Acceptance Testing (UAT) for every code change and        enhancement.  Worked on different weekly and monthly reports.  Provided operational support to all production practices on holidays and weekends.    Environment:  UNIX, Teradata,DB2, Control-M,Quality gate      Client                                 HSBC   Project                               HDW  Methodology                    Waterfall  Location                             Cap Gemini, Offshore  Team Size                          18  Duration                             Feb 2016 -  Dec 2017    Project Description         The objective of HDW is basically to have an in-house management of customer data to   support the Banks Revenue and Deposit Growth strategy. Create a Central accessible       data store in HSBC for deposit account and transaction data, with supporting customer      level data, to improve our informational capabilities in terms of more frequent, more       detailed information to ultimately drive deposit growth. This is envisioned as Central         Enterprise Data warehouse for HBUS PFS Data.    Roles & Responsibilities:      Team Member/Developer/Support    Monitored all data stage jobs and provided production support to 15 applications.  Used CONTROL-M Scheduler to schedule the DataStage jobs to run the daily jobs and weekly jobs  Written UNIX shell script to handle automation of daily batch   Worked on DataStage Designer  for Extracting from Source Transactional Database and then  loading into                Target Database after Transformations.  Used Sequential file, Join, Lookup, transformer, file set, datasets, Aggregator,Merge,        Funnel,Change capture, filter and remove duplicate stage for designing the jobs in the DataStage Designer.  Used  Data Stage Director for running, monitoring and scheduling the jobs.  Involved in Performance Tuning of Jobs, identifying and resolving performance Issues.  Worked on DataStage Designer for Exporting & Importing Jobs.  Automated the process of data stage jobs.  Implementation of all jobs in production for all newly changed jobs to improve performance.    Environment: Data Stage, UNIX, SQL, Control-M.      Award and Accolades Received    Got STAR OF THE MONTH award in BU Quarterly Award - Q3 (2016) and again in Q2(2017).  Received appreciations from the Client Manager after the successful Implementation of Projects.  Received Star performer award in the year 2019 in HSBC  Top rated in the annual appraisal for the year 2019-2020.      Certifications    Completed Google Cloud  Associate Cloud Engineer Certification in Oct 2020.      |INTERNAL|  |INTERNAL|", "tokens": [{"text": "HSBC", "start": 2357, "end": 2361, "token_start": 425, "token_end": 425, "entityLabel": "company"}, {"text": "Aug 2018-Till Date", "start": 2367, "end": 2385, "token_start": 427, "token_end": 429, "entityLabel": "period"}, {"text": "CAPGEMINI", "start": 2440, "end": 2449, "token_start": 440, "token_end": 440, "entityLabel": "company"}, {"text": "Sep 2015 -Till July 2018", "start": 2455, "end": 2479, "token_start": 442, "token_end": 446, "entityLabel": "period"}], "relations": [{"child": 427, "head": 425, "relationLabel": "duration"}, {"child": 442, "head": 440, "relationLabel": "duration"}]}, {"document": "                                              MAYUR SANJAY DHIKLE   (415)623-4122  mayursanjay14@gmail.com  LinkedIn Profile  Los Angeles, CA   PROJECT MANAGER / BUSINESS SYSTEMS ANALYST  Conscientious and resourceful Project Manager/ Business Analyst with experience conceptualizing and using data to tell a story  while demonstrating proficiency in requirements gathering, documentation, performing analysis, and providing effective  solutions. In-depth knowledge of Software Development Life Cycle (SDLC). Strong experience with Waterfall, Rational Unified  Process, and Agile-Scrum methodologies. Excellent business writing skills, including Business Requirements Documents (BRD),  Use Case Specifications, Functional Specifications, Systems Requirements Specifications (SRS) and Workflows.     TECHNICAL SKILLS  MySQL | SQL | Spiral | RUP | Agile  SCRUM | Agile  Kanban | SharePoint | HP ALM | Atlassian-JIRA | Trello |   Azure DevOps | MS Project | MS Visio | MS Dynamics 365 CRM | Lucid Chart| Balsamiq| InVision | PostgreSQL   MS Access | Excel | Tableau | Google Analytics | Power BI |C | Python for Data Science | JavaScript | HTML | CSS     PROFESSIONAL EXPERIENCE  FLEXON TECHNOLOGIES (SIGUE CORPORATION) | SYLMAR, CA 03/2018  PRESENT  Project Manager/ Business Systems Analyst    Successfully implemented 11 projects in a timely, cost effective manner keeping the scope of the project intact.    Overtook Dynamics CRM related projects (3) which had a stringent deadline, got them in a good shape to go on and  successfully implement them within the given time frame.    Versatile PM/ BA as have worked on various technologies/ projects within the company from developing a money  remittance app, Dynamics 365 related projects, Contact Center Upgrade as well Regulatory projects.      Technical Analysis, Development and Documentation:    Analyze, interpret, and elicit requirements related to various Software Development Kit (SDK) integrations in the  application bundle like CyberSource, Threat Metrix, Facebook Pixel, and Sift Science, Plaid SDK for fraud prevention.    Create wireframes, mockups and prototypes using InVision, & Balsamiq which helped the business stakeholders get  clear picture of the process, or app being developed, thus reducing the redevelopment work. (SiguePay, Dynamics CRM)    Provide a clear vision to development team by translating complex business and technical requirements in the form of  Process Flow Diagrams, Use Case Diagrams, Sequence Diagrams and Activity Diagrams using MS Visio.    Encouraged the team to make use of SharePoint to maintain important project documents and processes like Change  Management and Approval Management, which in turn proved beneficial for auditing purposes as well.      Project Management, Data Analysis, and Reporting:    Script SQL queries to pull data from MS SQL Server and create ad-hoc reports as per senior managements needs.     Perform data analysis on transactional data using Tableau and create reports related to the Key Performance Indicators  (KPIs) like Chargeback, Cancellations, MTD & YTD transfers.     Conduct data analysis to support customer requests including reconciliation of submissions.    Analyze and interpret data to identify trends, patterns and opportunities for the business and clients.    Involved in verification of data that is quantified and qualified after ETL process is like the original source Data.    Extensively took part in Data mapping and filtering, consolidation, cleansing, Integration, ETL.    Implemented validation of source data systems along with data mappings.    Updated all reports for users in SharePoint and Tableau server for easy access.    Prepared dashboards using calculations, parameters in Tableau.    Used Tableau to analyze data from Excel files and created graphical representations of our top competitors, our top  selling products and any other ad-hoc requests based on geo-locations.    Recommend solutions based on technical issues identified from Backlog Grooming, Backlog prioritization, and Scope  Planning to support prioritization and decision-making process of the Product Owner.    Establish RACI matrix for better accountability and tracking of tasks in the project.     Work closely with CTO and IT Infrastructure head to implement triage escalation process using ServiceNow.    Introduced Agile culture to a team used to work in a traditional methodology and coached them to utilize various Scrum  ceremonies and estimation techniques, which helped the team gauge their capacity.     Utilize Azure DevOps for recording the estimations and utilized burn down chart to track the progress of the sprint.   mailto:mayursanjay14@gmail.com http://www.linkedin.com/in/dhiklemayur    User Acceptance and Production Beta Testing; Supporting Quality Assurance Team:    Write test scripts/cases for testing of the app in Production Beta Environment, performing Functionality testing as well  as Smoke/Sanity testing. Document requirements and test cases using Requirement Traceability Matrix (RTM).       Change Management, Approval Management:   Manage Project Change Requests with as little disturbance as possible to an existing system. First, by determining the   scope of the change, determining the incorporation of the change, gaining approval or rejection of the change, and  finally communicating and implementing an approved change request.     Facilitate change control board meeting, driving the meeting with updated and changed requirements.   Automate the process of capturing and maintaining approvals (for regulatory purposes) using MS FLOWS by developing   a process flow utilizing the integration between SharePoint, Outlook, Teams, Azure, etc. which allows us to capture  approvals instead of manually maintaining them via Email.     SAN FRANCISCO STATE UNIVERSITY | SAN FRANCISCO, CA 09/2015  12/2017  Graduate Teaching Assistant      ACME IT SOLUTIONS | NASIK, INDIA 01/2014  01/2015  Jr. Business Analyst    As a quick learner, grasped on the various technical solutions that the company provided to its clients, which helped  me analyze clients needs and convert them into high-level business requirements effectively.    Being a junior employee, never hesitated to raise questions or concerns during our JAD sessions.    Produced Activity Diagrams and Use Case Diagrams to better support the business and functional requirements.    Created Mockup Screens using Balsamiq for clients and clarified QA teams issues; reviewed test plans and test scripts  developed by QA team to ensure requirements would be covered in scripts and effectively tested    Used HP ALM for defect management as well.       CERTIFICATIONS & TRAINING  Scrum Master Accredited Certification | Jira Administrator | Python for Data Science and Machine Learning   Google Analytics |The Complete SQL Bootcamp | Tableau | Business Analysis Nano Degree     EDUCATION & PUBLICATIONS  San Francisco State University | Master of Science in Embedded Electrical and Computer Systems  3.7 GPA  University of Mumbai | Bachelor of Engineering in Electronics 4.0 GPA     \"Android Application Controlled Surveillance Vehicle,\" Mayur Dhikle and Hamid Shahnasser, 3rd International Conference for  Convergence in Technology (I2CT), Pune, 2018   https://www.linkedin.com/company/4832913/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BALnsTaioR3StVely6wsTZw%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_profile_view_base-background_details_certification https://www.udemy.com/certificate/UC-HVXWQS6M/ https://www.udemy.com/certificate/UC-XOQ6KXUH/ https://www.udemy.com/certificate/UC-S758VYRY/ https://www.udemy.com/certificate/UC-J6R0DC7N/  ", "tokens": [{"text": "FLEXON TECHNOLOGIES", "start": 1177, "end": 1196, "token_start": 203, "token_end": 204, "entityLabel": "company"}, {"text": "03/2018  PRESENT", "start": 1230, "end": 1246, "token_start": 213, "token_end": 215, "entityLabel": "period"}, {"text": "ACME IT SOLUTIONS", "start": 5913, "end": 5930, "token_start": 1007, "token_end": 1009, "entityLabel": "company"}, {"text": "01/2014  01/2015", "start": 5946, "end": 5962, "token_start": 1014, "token_end": 1016, "entityLabel": "period"}], "relations": [{"child": 213, "head": 203, "relationLabel": "duration"}, {"child": 1014, "head": 1007, "relationLabel": "duration"}]}, {"document": "Mohit Kumar    Ph. No. +91- 9663321008                                         Email: mohitbuheja88@gmail.com                                                                                                                                                                        Introduction Synopsis   To excel in my profession by increasing the sphere of my knowledge, improving the quality of work through experience, constantly redefining my skills and building a mutually beneficial association with the organization to pursue my career aspirations.    Professional Experience  Presently working with Tata Consultancy Services as Assistant Consultant form 28th Oct 2015 to till date located in Bangalore.   Worked with IBM India Pvt. Ltd as Senior Operations Professional form 27th April 2012 to 21st Oct 2015.   Worked with iGATE Infrastructure Management Services Limited as Lotus Domino Administrator From 15th September 2011 to 14th April 2012.  Worked with CMC (A TATA Enterprise) as a Lotus Domino Administrator From 14th June 2010 to till 14th September 2011.  Worked with HCL Info-systems Ltd.  as a IT Executive engineer 26th May 2009 to 13th June 2010 .  Worked with NH Info-media Ltd. As a Technical support engineer Nov 2008 to April 2009.  Worked with Micro Clinic India Pvt. Ltd. As a System Support Engineer April 2007 to July 2008.    Technical Qualification:  AWS Certified Solutions Architect  Associate  Clouds Devops tools - Ansible, Chef, Jenkins, Git & GItHub, Docker, Kubernetes  Certified by IBM for Lotus Domino Administrator 8.5   Certified ITIL V3 Foundation in IT Service Management  Mobileiron V8.5 Support Specialist Fundamentals  Expert in Change & Incident Management  Trained by IBM Experts.  Effective communication with customers (RCA & regular updates on problems) -Trained by IBM Experts.  Certified Proofpoint Accredited Administrator : Email Protection  Have Basic Knowledge on Microsoft Azure Cloud Services   Basic Knowledge on PowerShell    Currently working as Devops India Lead, handling 7 members team, below are the basic roles and responsibilities which are completely taken care by me and my team.    Note:- Please note that both the projects are handleded simultanesously   Project Details:- GE healthcare and FCA (Fiat Chrysler Automobiles).      DevOps in GE Healthcare Project 1 ( From Dec 2019):    Supporting Jenkins installing, configuration, and maintaining/updating pipelines.  Handlind daily isseues on Jenkins pipeline or build issues.  Utilized configuration management tool Ansible & created Ansible Playbook to automate system operations.  Supporting Docker, Gitlab, and J-frog (Artifactory + X-ray).  Providing support for multiple tools that are part of CI/CD pipelines like, Coverity, SonarQube and Jacoco.  Providing support for ElasticSearch server and Grafana as dashboard.  L1 support for VMware on bare-metal, installing operating systems and various tools.  Maintaining process documents in central knowledge base.  Provide training to team members and keep them up to date.  Currently learning shellscripting and groovy scripting to improve skills on current environment.    AWS Project Responsibilities FCA (From  January 2017):    Designed, configured and deployed Amazon Web Services (AWS) for a multitude of applications utilizing the AWS stack (Including EC2, Route53, S3, RDS, Cloud Formation, Cloud Watch, SQS, IAM), focusing on High-availability, Fault tolerance and Auto-scaling.  Good knowledge on Creating and Deploying Elastic Beanstalk Environment.  Successfully delivered complete AWS Infra scripts using Cloud Formation.  Monitoring App & Service Instance usage, threshold, logs.  Configuring CloudWatch Alarm for capacity & performance.  Architect and deploy security-conscious infrastructure within an AWS VPC environment. Understand what services are and are not available in a Private Cloud infrastructure via AWS..  Hands on experience setting up Transit Gateways, End to End routes.  Setting up AWS ElasticSearch Services and configure with CloudWatch Logs and others services.  Beginner level information on Alteryx, worked on basic POC.    Past Experience (2007-2017):    Handling Global infrastructure for Lotus Domino Servers.  Part of the implementation team for the messaging infrastructure consolidation and upgrades.   Responsible for Monitoring Lotus Server (Performance, Space).  Responsible for Managing escalation of Mail Routing, Archiving and Replication problems of the users at client site.  Monitoring server statistics for pending and dead mail & take action accordingly.  Responsible for managing the mail Quota size of a user's mail file.  Identify & troubleshoot replication & mail-routing problems at server end.  Troubleshooting the server and handling crash cases, SSL certificates.  Managing maintenance tasks on all servers.  Maintaining Directory catalog and troubleshoot.  Providing RCA on high severity tickets.  Handling Traveller issues at devices and server level  Supporting Google calendar, working as Google Admin at Level 2 support. (Google Cloud Email and Collaborations)  Supporting Mobileiron technology and handling level 2 support issue         Academic Qualification:  Diploma in Computer Science from State Board of Technical Education Haryana in 2007(3 Years).  10th passed from the Haryana Board in 2004.    Personal Data:  DOB       19th OCT 1988  Gender       Male  Nationality       Indian  Marital Status      Single   Language Knows                 Hindi & English   Hobbies                    Helping needed one.    Declaration:   I hereby declare that all information given above is true and I hold the responsibility of its authenticity.      Date:  Place:                          (Mohit Kumar)", "tokens": [{"text": "Tata Consultancy Services", "start": 604, "end": 629, "token_start": 64, "token_end": 66, "entityLabel": "company"}, {"text": "28th Oct 2015 to till date", "start": 659, "end": 685, "token_start": 71, "token_end": 76, "entityLabel": "period"}, {"text": "IBM India Pvt. Ltd", "start": 722, "end": 740, "token_start": 84, "token_end": 88, "entityLabel": "company"}, {"text": "27th April 2012 to 21st Oct 2015", "start": 780, "end": 812, "token_start": 94, "token_end": 100, "entityLabel": "period"}, {"text": "iGATE Infrastructure Management Services Limited", "start": 828, "end": 876, "token_start": 105, "token_end": 109, "entityLabel": "company"}, {"text": "From 15th September 2011 to 14th April 2012", "start": 907, "end": 950, "token_start": 114, "token_end": 121, "entityLabel": "period"}, {"text": "CMC", "start": 965, "end": 968, "token_start": 126, "token_end": 126, "entityLabel": "company"}, {"text": "From 14th June 2010 to till 14th September 2011", "start": 1021, "end": 1068, "token_start": 137, "token_end": 145, "entityLabel": "period"}, {"text": "HCL Info-systems Ltd", "start": 1083, "end": 1103, "token_start": 150, "token_end": 155, "entityLabel": "company"}, {"text": "26th May 2009 to 13th June 2010", "start": 1133, "end": 1164, "token_start": 161, "token_end": 167, "entityLabel": "period"}, {"text": "NH Info-media Ltd", "start": 1180, "end": 1197, "token_start": 172, "token_end": 177, "entityLabel": "company"}, {"text": "Nov 2008 to April 2009", "start": 1231, "end": 1253, "token_start": 182, "token_end": 186, "entityLabel": "period"}, {"text": "Micro Clinic India Pvt. Ltd", "start": 1268, "end": 1295, "token_start": 191, "token_end": 197, "entityLabel": "company"}, {"text": "April 2007 to July 2008", "start": 1326, "end": 1349, "token_start": 202, "token_end": 206, "entityLabel": "period"}], "relations": [{"child": 202, "head": 191, "relationLabel": "duration"}, {"child": 182, "head": 172, "relationLabel": "duration"}, {"child": 161, "head": 150, "relationLabel": "duration"}, {"child": 137, "head": 126, "relationLabel": "duration"}, {"child": 114, "head": 105, "relationLabel": "duration"}, {"child": 94, "head": 84, "relationLabel": "duration"}, {"child": 71, "head": 64, "relationLabel": "duration"}]}, {"document": "                                                 Microsoft Word - Narendra.Kaushik   Narendra Kaushik  Senior Project Manager  HCL Technologies       n.kaushik@hotmail.com    Having around 14+ years of IT experience in software industry with  Development and Project Management experience in various technologies  & domains.     Specialized in Program & Project Management along with technical skills like  Java/J2EE, Amazon Web Services (AWS), Infor XA, IBM AS400/iSeries,  Python, Machine & Deep Learning.   +91-9910100842   K-906, AGV-II, Sector 78, Noida, India    linkedin.com/in/narendra-kaushik    WORK EXPERIENCE    Senior Project Manager  HCL Technologies Ltd. (2/2009  Present)   Strengths / Achievements    Able to lead all phases of technologically diversified projects from   initiation to closure in Waterfall and Agile methodologies.   Strong credentials in establishing and leading global teams   (onshore/offshore model) through energetic leadership and empowered  work culture.    Currently handling multiple projects & teams working in development  (T&M / Fixed bid) and L3 support.    Able to effectively perform end to end project management activities  including but not confined to Schedule management, Task allocation,  Communication management, Deliveries, Conflict handling, Managing  SLAs etc.    Capable of motivating team to its full potential and proven ability to  communicate with stakeholders to provide accurate reporting and  information.    Get involved in technical paradigm of project in short curve of time to  support team technically as well as functionally.     Have good understanding of designing and deploying scalable, highly  available & fault-tolerant AWS cloud solutions.    Tech Savvy and always willing to enhance skill set by engaging in various  certification programs. Completed HCLs D3-Concumerization course on  Artificial Intelligence, Machine Learning. & Python.     Having hands on experience in Java/J2EE, Infor XA development, Amazon  AWS, IBM AS400 / iSeries, Python & machine learning.     Senior Software Engineer   Euronet Worldwide Ltd. (4/2008  2/2009)  Strengths / Achievements   Technical Design writing, Coding (Java, C++, RPGLE, HTML, Java Scripting),   Testing & System Monitoring.   File setups & end-to-end testing (i.e., transaction generated from ATM   going to Issuer and vice versa).   Learnt about various ATM machines & Electronic Funds Transfer (EFT)   basics.    Software Engineer   Keane India Inc. (3/2007  4/2008)  Strengths / Achievements   Analysis and reverse engineering of existing application   Creating small technical designs, coding (Java/J2EE), Unit/Regression   Testing and client interactions.     TECHNICAL SKILLS  Cloud Based Technologies   AWS EC2, Lambda, S3, IAM, HA (Auto scaling & Load  balancing), VPCs, RDS, Cloud watch, ECR &  CodePipeline, API Gateway etc.   Languages  Java/J2EE, HTML, CSS, Web Services, Microservices,  REST APIs, IBM iSeries, RPGLE, CL/400, Java Script &  Python (along with Keras, Tensorflow, SciKit etc.).  Tools  PM Smart, MPP, JIRA, IBM HATS, Docker, GIT  Repository, Eclipse, SVN, Infor-XA, BLADE etc.  Database Management  MySql, RDS, DB2, MS Access    CERTIFICATIONS    AWS Certified Solutions Architect -  Associate      Managerial Excellence Program  Tenured Manager: Advance Level          D3-Consumerization  Artificial  Intelligence, Machine Learning &  Python   EDUCATIONAL QUALIFICATION     M.S. (Software Systems)  6/2014 (7.9 CGPA)    Master of Computer Application (MCA)  12/2004 (65 %)    Bachelor of Information Technology (BIT)  6/2002 (66 %)     AWARDS   O5 Infinity-League Award Hall of Fame for best   performance FY 2016  2020   INTERESTS  Artificial  Intelligence   Robotics   Video Games  Trekking        Educational Profile  Master of Computer Application (MCA)                                Achievements:     Completed MCA (2002-2004) with 66% marks.   Studied various foundational computer science subjects like Operating System, Computer Networks, Databases etc.    Project completed: Embedded City Guide  In 2004, when GPS in mobile phone were not quite common, city guide was developed to enable its user with details of visitable  amenities in the city of his/her tour. The whole software was developed in J2ME (Java 2.0 for Mobile Editions).      Bachelor of Information Technology (BIT)                            Achievements:     Completed BIT (1999-2002) with 65% marks.   Studied various foundational computer science subjects like RDBMS, Software Engineering,    Computer System Architecture, Operating System, Computer Networks etc.    HND in Computing and Multimedia was also awarded from EdExcel University, London    Project completed: Interactive website.  A lucrative website was created using tools like Adobe Photoshop and Dreamweaver and languages like ASP, HTML & java script. Few  Flash effects were also embedded.      Senior Secondary (12th)                                                                 Completed class 12th with 66% marks in 1998 with Physics, Chemistry & Mathematics as primary subjects along with English  and Hindi, from DAV College, Ambala.      Secondary (10th)                                                                             Completed class 10th with 76% marks in 1996 with standard subject (Mathematics, Science, Social Science, English and Sanskrit)  from SA Jain Senior Model school, Ambala.       Additional Qualification  MS (Software System)                                                                  Achievements:    1. Completed MS (Software Systems) from BITS Pilani (2012-2014) with 7.9 CGPA.  2. Studied various niche subjects like pervasive computing, data mining, data warehouse etc.    Project completed: Intelligent e-tutoring system.  The research was done on various learning modalities and proposal for a new intelligent e-tutoring system was given, capable to  understand the patterns of its leaner over the course of time and accordingly present the course content to the learner; per her/his  learning modality.      Certifications  1. AWS Certified Solutions Architect - Associate  2. Managerial Excellence Program Tenured Manager: Advance Level  3. HCL Certified (D3-Consumerization) in Artificial Intelligence / Machine Learning / Python        Additional Learnings  D3 Consumerization (AI/ML/Python)   Learned & implemented various Machine learning and Deep learning concepts. Covered many courses on AI/ML few of which are:    Python for Data Science and Machine Learning Bootcamp   Machine Learning A-Z: Hands-On Python in Data Science   Python A-Z: Python for Data Science with Real Exercises!   Deep Learning A-Z: Hands-On Artificial Neural Networks   Complete linear algebra: theory and implementation     Implemented couple of regression / classification & clustering models, created own small CNN models, trained and tested them on  image sets. Also tried hands-on with few pre-trained models viz. VGG16, VGG19, MobileNet etc. Done some hands on Natural Language  Processing (NLP)  Bag of words model and few Face Recognition model (Haar /HOG cascade classifier) etc. Now working on object  recognition and machine learning of video dataset.     Project Profile  Infor XA                                                                                         Infor is the third largest provider of enterprise applications and services, helping 70,000 customers in 194 countries improve operations,  drive growth and quickly adapt to changes in business demands. Infor offers deep industry-specific applications and suites, engineered  for speed, using ground-breaking technology that delivers a rich user experience, and flexible deployment options that give customers  a choice to run their businesses in the cloud, on-premises or both.    Responsibilities:    1. Handling multiple projects with blend of T&M, Fixed bid & L3 support, with varying team sizes of 5 to 15 people, resources  segregated under three projects (two developments and one support).   2. Handling various project management activities like schedule management, task allocations, communication management,  deliveries, managing SLAs along with functional and technical support to team.    3. Engaged in continuous improvement in productivity, resource utilization and quality by introducing/reviewing various  processes and checklists.    4. Performing resource management including resource allocations, utilization, hiring/firing, retention of resource, motivating  resources, conflict management etc.   5. Handling all customer interactions & status meetings  daily, weekly, biweekly & monthly.  6. Creating low-level & high-level architecture of applications   7. Involved in discussion on deploying scalable, highly available & fault-tolerant AWS cloud solutions.  8. Engaged in project requiring reconfiguring & retrofitting existing application (on premises) to cloud (CPQ-MT).  9. Involved in code reviews and optimization, tool creation and automation related activities.  10. Having hands on experience in Java/J2EE, Infor XA, Amazon AWS, IBM AS400 / iSeries & Python.  11. Team mentoring & supports team in resolving domain / technical issues.   12. Done D3-Consumerization course on Artificial Intelligence and Machine Learning to keep skills aligned with latest trends.     Environment: AWS Cloud, Java/J2EE, Eclipse, SVN, EDISON, JIRA & other DevOps tools, Infor XA Framework, RPG IV/Free Format,  DB2/400, IBM AS/400, iSeries, PMSMART, MPP etc.     CEVA Logistics                                                                             CEVA was formed in August 2007 as a result of the merger of TNT Logistics and EGL Eagle Global Logistics.   CEVA is one of the worlds leading logistics companies, providing end-to-end design, implementation and operational capabilities in  freight forwarding, contract logistics, transportation management and distribution management.  CEVA offer every customer a service tailored to their specific needs, built on their formidable experience across a broad range of market  sectors. They have expertise in automotive and tires; technology; consumer and retail; industrial; publishing; energy; aerospace; and  healthcare.          Responsibilities:    1. Team lead & Database Administrator (DBA)  2. System performance monitoring & resolving issues impacting system performance  3. IMPLEMENTER Administrator  4. Mb-software EZRAD WORPERF tool administrator  5. Coding, Testing, code reviews database administration and tuning using iSeries / Java based technologies.      Environment: Core Java, JDBC, JSP, Multithreading, RPG IV/400, CL, DB2 and other system tools viz mb-software, MKS Implementer  etc.     Center of Excellence (COE)                                                      Worked in various tools and projects for developing solution and approaches. Few tasks accomplished are:     1. iSeries modernization - particularly using IBM HATS & WDSCi  2. Created tools viz.  Security Assessment tool for Infra, customized Spool to PDF convertor etc.   3. Worked on various POCs like DDS to DDL conversion, CGI on iSeries with RPG IV, PHP with iSeries etc  4. Did solutioning for various projects, which includes  GSI at risk field, UTi Security Solution, GSI  Globalization/Unicode   conversion etc.     Environment: Core Java, HTML, Java Script, PHP, SQL, RPG 400/IV, DB2, CL etc.     Gordian Knot                                                                              Integrated Transaction Management (ITM) is the core of Banking / Financial solutions provided by the Euronet Worldwide around the  globe. ITM plays a vital role in all the services provided by Euronet Worldwide, which Includes ATM driving, CASHNET (Including Master  Card, VISA and American Express), POS Transactions, Mobile Banking, Net Banking & much more.   ITM is quite modular in nature composed of different Modules which includes Device Control Module (e.g. Network Device Control,  HosttoHost),Transport Control Protocol Module, Transaction Management Module, Security & System Administrator Module, Log  Module etc. to name a few. ITM is one of the most customizable switches available in the market. Developed and maintained on AS400  system, it provides functionalities like authorization, routing, security etc for real-time electronic financial transaction for Banks.    Was engaged in enhancement of ITM 1.4 version to ITM 3.0 version, in which we migrated Host(s) from Old Poland subsystem (1.4) to  new gateway subsystem (3.0).     Responsibilities: Analyzing / Understanding Application Architecture, Technical Designing, Coding, Unit/Regression Testing, System  Monitoring, Task Allocation and Team Handling, file setups & end-to-end testing (i.e. transaction generated from ATM going to Issuer  and vice versa).     Environment: HTML, Java Script, Core Java, JSP, RPG 400/IV, DB2, CL, C/C++.     Lennox                                                                                          Lennox International provides climate control solutions for heating, air conditioning and refrigeration. Two major Modules of Lennox  are STARS and IMR. Along with this, Lennox acquired Service Experts to expand their business into the servicing, repairing, and replacing  HVAC equipment. They have wide range of experience in service and repairs of ACs, Refrigeration units and climate control solution.  Service Experts operates around 200 service centers across US and Canada. The applications on IBM iSeries are used to support the daily  operations of the service centers, including customer data, work orders, job orders, purchase orders, invoicing and financial reporting.    Responsibilities: Analysis, Technical Designing, Coding, Unit/Regression Testing and client interaction.   Environment: Java/J2EE, RPGLE/Free-format, DB2, Aldon.           ", "tokens": [{"text": "HCL Technologies Ltd", "start": 648, "end": 668, "token_start": 112, "token_end": 115, "entityLabel": "company"}, {"text": "2/2009  Present", "start": 671, "end": 686, "token_start": 116, "token_end": 118, "entityLabel": "period"}, {"text": "Euronet Worldwide Ltd", "start": 2080, "end": 2101, "token_start": 363, "token_end": 366, "entityLabel": "company"}, {"text": "4/2008  2/2009", "start": 2104, "end": 2118, "token_start": 367, "token_end": 369, "entityLabel": "period"}, {"text": "Keane India Inc", "start": 2465, "end": 2480, "token_start": 446, "token_end": 449, "entityLabel": "company"}, {"text": "3/2007  4/2008", "start": 2483, "end": 2497, "token_start": 450, "token_end": 452, "entityLabel": "period"}], "relations": [{"child": 450, "head": 446, "relationLabel": "duration"}, {"child": 367, "head": 363, "relationLabel": "duration"}, {"child": 116, "head": 112, "relationLabel": "duration"}]}, {"document": "                          Harsha Vardhan Veeturi Mail  harshavardhanveeturi@gmail.com                                                                          Mobile No +91 9551267821 Linked In www.linkedin.com/in/harshavardhanveeturi    DOB  27th Aug 1993    PROFESSIONAL SUMMARY   6+ years of experience in Machine learning and ETL Technologies. Exposure to work with key global leaders in Finance                   Domain. Currently building a framework that uses AI to classify customer information which is NPPI using Predictive                 Modeling and NLP. Exposure to building data models and applying learning algorithms in both supervised and                semi-supervised learning projects. Solid mathematical knowledge; understanding of machine learning, statistics.  SKILLS AND COMPETENCIES    Technical Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, SQL, Predictive Modeling, RandomForest, Logistic Regression, TensorFlow, Keras, Pandas, NumPy, Seaborn, Matplotlib, Python, IBM Infosphere DataStage, Shell scripting.    Domain Banking. Certifications  Applied Machine Learning - AppliedAI, Neural networks, and deep learning - Coursera   Digital Skill Tag as Python Professional - Infosys.   PROFESSIONAL EXPERIENCE    ML Engineer  Infosys Ltd.  Dec2019Present Leading US Bank (Data Protection Solutions | Machine Learning and Big Data Developer)    Skills - Machine Learning, Deep Learning, Python, Big Data, Hadoop, Hive, Sqoop, Shell Scripting.  Working on an AI and Big Data-based platform use AI to classify the customer-sensitive data that resides on Hadoop                    Distributed File Systems(HDFS).  Exhibited key skills like preprocessing of Data like Cleansing and Sanitizing, exploratory data analysis, and Identifying                 the right metric for the model.  Comprehensively worked on deriving new features from existing features and identifying them, Feature Engineering and                Feature Selection.  Build Natural Language Processing based models for identifying NPPI data present in tables and log files.  Developed an Ensemble Machine learning model for classifying skewed data coming from different sources.  Develop and adapts best practices to design and build medium complexity machine learning systems.  Closely worked with functional  SMEs to understand critical business logic to design.      Lead Developer  Infosys Ltd. Dec2017Dec2019 Leading US Bank (CATT Migration | ETL and Big Data Developer)    Skills - Big data, Hadoop, IBM Infosphere DataStage, Hive, Python, and  Shell Scripting.  The entire banks credit card data resides on a single platform, which has multiple code environments in staging,  cleansing, and transformations. Bank has decided to revamp the entire platform by migrating and by upgrading servers, databases and eliminating the legacy codebase to make Authorized Data Sources (ADS) for entire card data.    Extensively involved in analyzing the functionality of legacy BCDI systems.  Responsible for Data Management and Environment Setup for the critical business process of BCDI.  Built data pipelines to transform ingested data to meet reporting needs and load to Hive tables.  Implemented data validation and integrity checks to ensure data has been loaded into the target systems.  Provided technical advice to junior-level team members when developing solutions.  Appreciated and Awarded Star Award for creating automation tools for analyzing logs and ad hoc activities which helped  in delivering the project fluently.                 mailto:***************@gmail.com http://www.linkedin.com/in/harshavardhanveeturi     Developer  Infosys Ltd.  Jan2015Dec2017 Leading US Bank (SIAI | ETL Developer)    Skills - IBM Infosphere DataStage, DB2, Python, and Shell scripting.  BCDI is the critical Credit Card Management Platforms of Bank which comprises sensitive data of credit card customers.  The Access Management system for the users was not effective; it has to be revalidated and reconfigured with a new set                      of rules.  Responsible for understanding the business and coming up with an effective new set of rules with SMEs.  Responsible for automating the process based on the new set of rules.  Appreciated and awarded Best Debutant award for thorough analysis and creating automation tools.   EDUCATION  Jawaharlal Nehru Technological University(JNTU-K)  June 2010-March 2014   ", "tokens": [{"text": "Infosys Ltd", "start": 1283, "end": 1294, "token_start": 188, "token_end": 190, "entityLabel": "company"}, {"text": "Dec2019Present", "start": 1297, "end": 1311, "token_start": 191, "token_end": 191, "entityLabel": "period"}, {"text": "Infosys Ltd", "start": 2408, "end": 2419, "token_start": 377, "token_end": 379, "entityLabel": "company"}, {"text": "Dec2017Dec2019", "start": 2421, "end": 2435, "token_start": 379, "token_end": 379, "entityLabel": "period"}, {"text": "Infosys Ltd", "start": 3662, "end": 3673, "token_start": 585, "token_end": 587, "entityLabel": "company"}, {"text": "Jan2015Dec2017", "start": 3676, "end": 3690, "token_start": 588, "token_end": 588, "entityLabel": "period"}], "relations": [{"child": 191, "head": 188, "relationLabel": "duration"}, {"child": 379, "head": 377, "relationLabel": "duration"}, {"child": 588, "head": 585, "relationLabel": "duration"}]}]